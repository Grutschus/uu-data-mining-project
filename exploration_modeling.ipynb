{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tillgrutschus/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import nltk\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import PorterStemmer\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from copy import deepcopy\n",
    "\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "import random\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_count</th>\n",
       "      <th>figure_count</th>\n",
       "      <th>author_count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>text</th>\n",
       "      <th>page_imputed</th>\n",
       "      <th>citation_bucket</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doi</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10.1002/adfm.202001307</th>\n",
       "      <td>25.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>Qinghua Zhao, Wanqi Jie, Tao Wang, Andres Cast...</td>\n",
       "      <td>False</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.1002/cphc.200900857</th>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Haifeng Ma, Thomas Brugger, Simon Berner, Yun ...</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.1002/prop.200710532</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>Milovan Vasilic, Marko Vojinovic Interaction o...</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.1007/978-3-030-30493-5_44</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>Itay Mosafi, Eli David, Nathan S. Netanyahu De...</td>\n",
       "      <td>True</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.1007/s00025-018-0843-4</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>Deepshikha and Lalit K. Vashisht Weaving K-fra...</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              page_count  figure_count  author_count  year  \\\n",
       "doi                                                                          \n",
       "10.1002/adfm.202001307              25.0          13.0             5  2020   \n",
       "10.1002/cphc.200900857              15.0           4.0             8  2010   \n",
       "10.1002/prop.200710532               5.0           0.0             2  2015   \n",
       "10.1007/978-3-030-30493-5_44        15.0           0.0             3  2019   \n",
       "10.1007/s00025-018-0843-4           15.0           0.0             2  2018   \n",
       "\n",
       "                              month  day  \\\n",
       "doi                                        \n",
       "10.1002/adfm.202001307            7   30   \n",
       "10.1002/cphc.200900857            2    4   \n",
       "10.1002/prop.200710532            5   20   \n",
       "10.1007/978-3-030-30493-5_44     12    3   \n",
       "10.1007/s00025-018-0843-4         6    8   \n",
       "\n",
       "                                                                           text  \\\n",
       "doi                                                                               \n",
       "10.1002/adfm.202001307        Qinghua Zhao, Wanqi Jie, Tao Wang, Andres Cast...   \n",
       "10.1002/cphc.200900857        Haifeng Ma, Thomas Brugger, Simon Berner, Yun ...   \n",
       "10.1002/prop.200710532        Milovan Vasilic, Marko Vojinovic Interaction o...   \n",
       "10.1007/978-3-030-30493-5_44  Itay Mosafi, Eli David, Nathan S. Netanyahu De...   \n",
       "10.1007/s00025-018-0843-4     Deepshikha and Lalit K. Vashisht Weaving K-fra...   \n",
       "\n",
       "                              page_imputed citation_bucket  \n",
       "doi                                                         \n",
       "10.1002/adfm.202001307               False            high  \n",
       "10.1002/cphc.200900857                True            high  \n",
       "10.1002/prop.200710532               False             low  \n",
       "10.1007/978-3-030-30493-5_44          True             low  \n",
       "10.1007/s00025-018-0843-4             True            high  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the dataset\n",
    "dataset_pd = pd.read_parquet('dataset/dataset_labeled.parquet')\n",
    "dataset_pd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 0.90\n",
      "Test set size: 0.10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TRAIN_RATIO = 0.9\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "# split the dataset into train and test sets\n",
    "train_df, test_df = train_test_split(\n",
    "    dataset_pd,\n",
    "    test_size=TEST_RATIO,\n",
    "    random_state=42,\n",
    "    stratify=dataset_pd[\"citation_bucket\"],\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(train_df) / len(dataset_pd):.2f}\")\n",
    "print(f\"Test set size: {len(test_df) / len(dataset_pd):.2f}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "sns.countplot(x=\"citation_bucket\", data=train_df, ax=ax[0])\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "ax[0].set_title(\"Train set\")\n",
    "sns.countplot(x=\"citation_bucket\", data=test_df, ax=ax[1])\n",
    "ax[1].set_title(\"Test set\")\n",
    "ax[1].set_ylabel(\"\")\n",
    "\n",
    "filtered_train_df = train_df[train_df[\"page_imputed\"] == False]\n",
    "filtered_test_df = test_df[test_df[\"page_imputed\"] == False]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "sns.countplot(x=\"citation_bucket\", data=filtered_train_df, ax=ax[0])\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "ax[0].set_title(\"Train set\")\n",
    "sns.countplot(x=\"citation_bucket\", data=filtered_test_df, ax=ax[1])\n",
    "ax[1].set_title(\"Test set\")\n",
    "ax[1].set_ylabel(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the individual sets\n",
    "train_df.to_parquet(\"dataset/train.parquet\")\n",
    "test_df.to_parquet(\"dataset/test.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Pipeline\n",
    "\n",
    "The following models / algorithms will be tried:\n",
    " \n",
    " **Models**:\n",
    " - SVM\n",
    " - Multinomial Naive Bayes\n",
    "\n",
    "**Vectorizers**:\n",
    "  - Count Vectorizer -> We will not use the CountVectorizer as the SparseMatrix cannot be scaled; we will use the TF-IDF Vectorizer instead\n",
    "  - TF-IDF Vectorizer\n",
    "\n",
    "**Dimensionality Reduction**:\n",
    "  - Stopword removal\n",
    "  - Lemmatization\n",
    "  - Stemming\n",
    "\n",
    "**Scaling:**\n",
    "  - Standard Scaler\n",
    "\n",
    "**Metrics:**\n",
    "  - Accuracy\n",
    "  - Precision\n",
    "  - Recall\n",
    "  - F1 Score\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "- **Ratio attributes**: [\"page_count\", \"figure_count\", \"author_count\"]\n",
    "- **Date attributes**: [\"year\", \"month\", \"day\"]\n",
    "- **Text attributes**: [\"text\"]\n",
    "- **Label**: [\"citation_bucket\"]\n",
    "\n",
    "[Ratio] -> [StandardScaler] -> [Model]\n",
    "\n",
    "Since the three attributes are actually ordinal (not categorical), we treat them as numerical attributes.\n",
    "\n",
    "[Date] -> [StandardScaler] -> [Model]\n",
    "\n",
    "[Text] -> [Vectorizer (w/ stopword removal, lemmatization, stemming)] -> [Model]\n",
    "\n",
    "[Label] -> [OrdinalEncoder] -> [Model]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;model&#x27;, LinearSVC(loss=&#x27;hinge&#x27;, random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;model&#x27;, LinearSVC(loss=&#x27;hinge&#x27;, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(n_jobs=-1,\n",
       "                  transformers=[(&#x27;ratio&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                  &#x27;author_count&#x27;]),\n",
       "                                (&#x27;date&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                (&#x27;text&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                  TfidfVectorizer())]),\n",
       "                                 &#x27;text&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ratio</label><div class=\"sk-toggleable__content\"><pre>[&#x27;page_count&#x27;, &#x27;figure_count&#x27;, &#x27;author_count&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">date</label><div class=\"sk-toggleable__content\"><pre>[&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">text</label><div class=\"sk-toggleable__content\"><pre>text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(loss=&#x27;hinge&#x27;, random_state=42)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[('ratio',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['page_count', 'figure_count',\n",
       "                                                   'author_count']),\n",
       "                                                 ('date',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['year', 'month', 'day']),\n",
       "                                                 ('text',\n",
       "                                                  Pipeline(steps=[('vectorizer',\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  'text')])),\n",
       "                ('model', LinearSVC(loss='hinge', random_state=42))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratio_columns = [\"page_count\", \"figure_count\", \"author_count\"]\n",
    "date_columns = [\"year\", \"month\", \"day\"]\n",
    "text_columns = \"text\"\n",
    "label_columns = [\"citation_bucket\"]\n",
    "\n",
    "# Ratio\n",
    "ratio_pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Date\n",
    "date_pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Text\n",
    "text_pipe = Pipeline(\n",
    "    [\n",
    "        (\"vectorizer\", TfidfVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Column Transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"ratio\", ratio_pipe, ratio_columns),\n",
    "        (\"date\", date_pipe, date_columns),\n",
    "        (\"text\", text_pipe, text_columns),\n",
    "    ], n_jobs=-1\n",
    ")\n",
    "\n",
    "# Total pipeline\n",
    "model = LinearSVC(\n",
    "    loss=\"hinge\",\n",
    "    C=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", model),\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_model(model, folder_path):\n",
    "    # get a list of all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "    # get the largest number in the filenames\n",
    "    max_num = 0\n",
    "    for f in files:\n",
    "        if f.endswith('.pkl'):\n",
    "            try:\n",
    "                num = int(f.split('.')[0])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if num > max_num:\n",
    "                max_num = num\n",
    "    # create a filename for the new model\n",
    "    filename_model = f\"{max_num + 1}.pkl\"\n",
    "    # save the model and parameters\n",
    "    with open(os.path.join(folder_path, filename_model), 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "def load_model(model_number, folder_path):\n",
    "    # create the file path\n",
    "    file_path = os.path.join(folder_path, str(model_number))\n",
    "    # load the model and parameters from the file\n",
    "    with open(file_path + \".pkl\", 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out imputed data\n",
    "\n",
    "We have noticed that the computation time for the full dataset is too long. Thus, we have filtered out the imputed page counts to reduce computational costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set absolute size: 338052\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_parquet(\"dataset/train.parquet\")\n",
    "train_df = train_df[train_df[\"page_imputed\"] == False]\n",
    "\n",
    "print(f\"Train set absolute size: {len(train_df)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, naive training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = pipeline.fit(train_df, train_df[\"citation_bucket\"])\n",
    "save_model(trained, \"models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5118747512272788"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained.score(val_df, val_df[\"citation_bucket\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a custom text preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'sweetheart', 'have', 'show', 'you', 'how', 'shower']\n",
      "['Hey', 'sweetheart', 'have', 'showed', 'you', 'how', 'shower']\n"
     ]
    }
   ],
   "source": [
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "        return [\n",
    "            self.wnl.lemmatize(t)\n",
    "            for t in nltk.regexp_tokenize(doc, token_pattern)\n",
    "        ]\n",
    "\n",
    "\n",
    "class StemmingTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = nltk.stem.PorterStemmer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "        return [\n",
    "            self.wnl.stem(t)\n",
    "            for t in nltk.regexp_tokenize(doc, token_pattern)\n",
    "        ]\n",
    "\n",
    "test_stemmer = StemmingTokenizer()\n",
    "test_lemmatizer = LemmaTokenizer()\n",
    "print(test_stemmer(\"Hey sweetheart, have I showed you how I shower?\"))\n",
    "print(test_lemmatizer(\"Hey sweetheart, have I showed you how I shower?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.3 s  91.7 ns per loop (mean  std. dev. of 7 runs, 10,000 loops each)\n",
      "9.73 s  28.8 ns per loop (mean  std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit test_stemmer(\"Hey sweetheart, have I showed you how I shower?\")\n",
    "%timeit test_lemmatizer(\"Hey sweetheart, have I showed you how I shower?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_tranf_vec\n",
      "Number of features: 156361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sample from the vocabulary: ['karuppath', 'mansuripur', 'naser', 'mandell', 'ippocratis', 'vidaexpert', '66006', 'piotto', 'jianyu', 'valio', 'semianalytical', 'biagi', 'yatian', 'pentti', 'photocentre', 'mu_f', 'hunar', 'roati', 'quentin', 'delbo', 'microphonics', 'cerne', 'paradigm', 'hypocenter', '4408']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trunc_train_df = train_df[:50000]\n",
    "# Let's compare dim reduction models\n",
    "print(\"baseline_tranf_vec\")\n",
    "baseline_tranf_vec = TfidfVectorizer(strip_accents=\"unicode\")\n",
    "baseline_tranf = baseline_tranf_vec.fit_transform(trunc_train_df[\"text\"])\n",
    "print(f\"Number of features: {baseline_tranf.shape[1]}\")\n",
    "display(f\"Sample from the vocabulary: {random.sample(list(baseline_tranf_vec.vocabulary_.keys()), 25)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 156226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sample from the vocabulary: ['m62', 'cib', 'olfactory', 'rijcke', 'b881', 'genetically', 'kegerreis', 'celestial', 'entropic', '273m', 'eva', '15au', 'oabo', 'applic', 'lyophobic', 'tsuji', 'foliation', '5194', 'caines', 'pseudoconvex', 'shanhe', 'mcid', 'host', 'jhep0505', 'contrastive']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Relative dim reduction: 0.00086'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stop word removal\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "stop_word_vec = TfidfVectorizer(strip_accents=\"unicode\", stop_words=stop_words)\n",
    "stop_word_transf = stop_word_vec.fit_transform(trunc_train_df[\"text\"])\n",
    "print(f\"Number of features: {stop_word_transf.shape[1]}\")\n",
    "display(f\"Sample from the vocabulary: {random.sample(list(stop_word_vec.vocabulary_.keys()), 25)}\")\n",
    "display(f\"Relative dim reduction: {1 - (stop_word_transf.shape[1] / baseline_tranf.shape[1]):.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 53726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sample from the vocabulary: ['l5', 'displacement', 'qmw', 'insulated', 'telemetry', 'pioline', 'implementing', 'axp', 'jamiolkowski', 'concealed', 'semihadronic', 'onodera', '4t', 'chong', '150pc', 'colliding', 'anisotropic', 'expansive', 'pentaquarks', 'cleaning', 'haojing', 'neutralization', 'aisi', 'maro', 'radim']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Relative dim reduction: 0.65640'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemma_vec = TfidfVectorizer(strip_accents=\"unicode\", tokenizer=LemmaTokenizer())\n",
    "lemma_transf = lemma_vec.fit_transform(trunc_train_df[\"text\"])\n",
    "print(f\"Number of features: {lemma_transf.shape[1]}\")\n",
    "display(f\"Sample from the vocabulary: {random.sample(list(lemma_vec.vocabulary_.keys()), 25)}\")\n",
    "display(f\"Relative dim reduction: {1 - (lemma_transf.shape[1] / baseline_tranf.shape[1]):.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 134201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sample from the vocabulary: ['rossier', 'feyzabadi', '136393', 'scarpin', 'gsu', 'he1', 'c70', 'gallego', 'ontivero', 'looser', '5e41', 'ahlborn', '084712', 'photodesorb', 'uzundag', '8054', 'slatteri', 'anheier', 'moncelsi', 'peaker', 'polsar', 'janowiecki', '7006', 'h_q', 'leik']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Relative dim reduction: 0.14172'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stemming\n",
    "stem_vec = TfidfVectorizer(strip_accents=\"unicode\", tokenizer=StemmingTokenizer())\n",
    "stem_transf = stem_vec.fit_transform(trunc_train_df[\"text\"])\n",
    "print(f\"Number of features: {stem_transf.shape[1]}\")\n",
    "display(f\"Sample from the vocabulary: {random.sample(list(stem_vec.vocabulary_.keys()), 25)}\")\n",
    "display(f\"Relative dim reduction: {1 - (stem_transf.shape[1] / baseline_tranf.shape[1]):.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tillgrutschus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "param_grid_svc = {\n",
    "    \"model\": [LinearSVC()],\n",
    "    \"model__C\": [0.1, 1.0, 10.0],\n",
    "    \"model__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "param_grid_mnb = {\n",
    "    \"model\": [MultinomialNB()],\n",
    "    \"model__alpha\": [0.1, 1.0, 10.0],\n",
    "}\n",
    "\n",
    "# param_grid_preprocess_ngram = {\n",
    "#     \"preprocessor__text__vectorizer__ngram_range\": [(1, 2)],\n",
    "#     \"preprocessor__text__vectorizer__max_features\": [1000, 10000],\n",
    "#     \"preprocessor__text__vectorizer__max_df\": [0.5, 0.75, 1.0],\n",
    "#     \"preprocessor__text__vectorizer__strip_accents\": [None, \"unicode\"],\n",
    "# }\n",
    "\n",
    "param_grid_preprocess_max_df = {\n",
    "    \"preprocessor__text__vectorizer__max_df\": [0.5, 0.75, 1.0],\n",
    "    \"preprocessor__text__vectorizer__max_features\": [None, 1000, 10000],\n",
    "    \"preprocessor__text__vectorizer__strip_accents\": [None, \"unicode\"],\n",
    "}\n",
    "\n",
    "param_grid_preprocess_tokenizer = {\n",
    "    \"preprocessor__text__vectorizer__tokenizer\": [\n",
    "        None,\n",
    "        LemmaTokenizer(),\n",
    "        StemmingTokenizer(),\n",
    "    ],\n",
    "    \"preprocessor__text__vectorizer__stop_words\": [None, list(stop_words)],\n",
    "}\n",
    "\n",
    "param_grid_scaler_min_max = {\n",
    "    \"preprocessor__ratio__scaler\": [MinMaxScaler()],\n",
    "    \"preprocessor__date__scaler\": [MinMaxScaler()],\n",
    "}\n",
    "\n",
    "param_grid_scaler_std = {\n",
    "    \"preprocessor__ratio__scaler\": [StandardScaler()],\n",
    "    \"preprocessor__date__scaler\": [StandardScaler()],\n",
    "}\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {**param_grid_svc, **param_grid_scaler_min_max, **param_grid_preprocess_max_df},\n",
    "    {**param_grid_mnb, **param_grid_scaler_min_max, **param_grid_preprocess_max_df},\n",
    "    {**param_grid_svc, **param_grid_scaler_std, **param_grid_preprocess_max_df},\n",
    "    {**param_grid_mnb, **param_grid_scaler_std, **param_grid_preprocess_max_df},\n",
    "    {**param_grid_svc, **param_grid_scaler_min_max, **param_grid_preprocess_tokenizer},\n",
    "    {**param_grid_mnb, **param_grid_scaler_min_max, **param_grid_preprocess_tokenizer},\n",
    "    {**param_grid_svc, **param_grid_scaler_std, **param_grid_preprocess_tokenizer},\n",
    "    {**param_grid_mnb, **param_grid_scaler_std, **param_grid_preprocess_tokenizer},\n",
    "]\n",
    "\n",
    "\n",
    "search = GridSearchCV(pipeline, param_grid=param_grid, cv=3, verbose=10, n_jobs=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n",
      "[CV 1/3; 1/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 1/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 1/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 2/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 2/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 2/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 3/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 3/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 1/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.520 total time=  47.7s\n",
      "[CV 2/3; 1/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.522 total time=  48.0s\n",
      "[CV 3/3; 3/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 3/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=  44.0s\n",
      "[CV 2/3; 3/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=  43.4s\n",
      "[CV 3/3; 1/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.521 total time=  49.9s\n",
      "[CV 1/3; 2/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.520 total time=  49.4s\n",
      "[CV 1/3; 4/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 2/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.522 total time=  49.4s\n",
      "[CV 2/3; 4/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 4/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 2/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.521 total time=  49.4s\n",
      "[CV 1/3; 5/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 5/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 5/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 6/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 3/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=  35.2s\n",
      "[CV 2/3; 6/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 4/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=  35.9s\n",
      "[CV 2/3; 4/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=  36.0s\n",
      "[CV 3/3; 6/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 4/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time=  36.1s\n",
      "[CV 1/3; 7/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 5/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time=  37.5s\n",
      "[CV 2/3; 7/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 7/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 5/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.516 total time=  38.6s\n",
      "[CV 3/3; 5/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time=  37.8s\n",
      "[CV 1/3; 8/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 6/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.514 total time=  38.2s\n",
      "[CV 2/3; 8/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 8/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 6/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.516 total time=  38.7s\n",
      "[CV 1/3; 9/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 6/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.515 total time=  39.9s\n",
      "[CV 2/3; 9/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 7/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.520 total time=  42.2s\n",
      "[CV 3/3; 9/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 7/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.521 total time=  41.6s\n",
      "[CV 2/3; 7/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.523 total time=  42.9s\n",
      "[CV 1/3; 10/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 10/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 8/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.520 total time=  42.9s\n",
      "[CV 2/3; 8/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.523 total time=  43.0s\n",
      "[CV 3/3; 10/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 8/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.521 total time=  43.1s\n",
      "[CV 1/3; 11/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 11/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 9/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=  35.0s\n",
      "[CV 3/3; 11/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 9/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=  34.9s\n",
      "[CV 1/3; 12/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 9/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=  35.8s\n",
      "[CV 2/3; 12/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 10/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=  35.8s\n",
      "[CV 3/3; 12/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 10/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=  35.8s\n",
      "[CV 1/3; 13/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 10/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time=  36.0s\n",
      "[CV 2/3; 13/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 11/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time=  36.9s\n",
      "[CV 2/3; 11/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.517 total time=  37.6s\n",
      "[CV 3/3; 13/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 14/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 11/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time=  37.9s\n",
      "[CV 2/3; 14/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 12/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.515 total time=  38.5s\n",
      "[CV 3/3; 14/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 12/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.516 total time=  38.5s\n",
      "[CV 1/3; 15/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 12/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.515 total time=  39.0s\n",
      "[CV 2/3; 15/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 13/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.520 total time=  41.4s\n",
      "[CV 2/3; 13/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.523 total time=  42.3s\n",
      "[CV 3/3; 15/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 16/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 13/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.521 total time=  40.9s\n",
      "[CV 1/3; 14/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.520 total time=  40.5s\n",
      "[CV 2/3; 16/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 16/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 14/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.523 total time=  39.2s\n",
      "[CV 1/3; 17/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 15/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.513 total time=  35.7s\n",
      "[CV 3/3; 14/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.521 total time=  39.2s\n",
      "[CV 2/3; 17/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 17/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 15/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.513 total time=  36.2s\n",
      "[CV 1/3; 18/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 15/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=  36.6s\n",
      "[CV 2/3; 18/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 16/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.513 total time=  36.2s\n",
      "[CV 2/3; 16/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.513 total time=  36.2s\n",
      "[CV 3/3; 18/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 16/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=  35.9s\n",
      "[CV 1/3; 19/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 19/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 17/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time=  38.1s\n",
      "[CV 3/3; 19/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 17/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.517 total time=  38.3s\n",
      "[CV 3/3; 17/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.516 total time=  38.1s\n",
      "[CV 1/3; 20/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 20/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 18/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.515 total time=  38.8s\n",
      "[CV 3/3; 20/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 18/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.517 total time=  39.6s\n",
      "[CV 1/3; 21/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 18/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.516 total time=  39.8s\n",
      "[CV 2/3; 21/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 19/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.493 total time=  44.3s\n",
      "[CV 2/3; 19/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.493 total time=  44.5s\n",
      "[CV 3/3; 21/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 22/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 19/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.492 total time=  45.9s\n",
      "[CV 2/3; 22/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 20/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.493 total time=  45.4s\n",
      "[CV 2/3; 20/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.493 total time=  45.3s\n",
      "[CV 3/3; 22/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 23/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 20/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.492 total time=  45.8s\n",
      "[CV 2/3; 23/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 21/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=  42.0s\n",
      "[CV 3/3; 23/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 21/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time=  41.5s\n",
      "[CV 1/3; 24/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 21/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=  40.6s\n",
      "[CV 2/3; 24/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 22/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=  39.1s\n",
      "[CV 3/3; 24/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 22/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time=  38.7s\n",
      "[CV 1/3; 25/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 22/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time=  38.2s\n",
      "[CV 2/3; 25/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 23/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.477 total time=  40.6s\n",
      "[CV 3/3; 25/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 23/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.479 total time=  41.1s\n",
      "[CV 1/3; 26/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 23/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.478 total time=  40.9s\n",
      "[CV 2/3; 26/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 24/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time=  41.2s\n",
      "[CV 3/3; 26/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 24/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.479 total time=  40.8s\n",
      "[CV 1/3; 27/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 24/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time=  40.4s\n",
      "[CV 2/3; 27/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 25/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.493 total time=  44.5s\n",
      "[CV 3/3; 27/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 25/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.494 total time=  44.2s\n",
      "[CV 1/3; 28/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 25/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.491 total time=  43.7s\n",
      "[CV 2/3; 28/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 26/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.493 total time=  44.6s\n",
      "[CV 3/3; 28/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 26/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.494 total time=  44.6s\n",
      "[CV 1/3; 29/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 27/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=  41.6s\n",
      "[CV 3/3; 26/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.492 total time=  44.6s\n",
      "[CV 2/3; 27/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=  40.8s\n",
      "[CV 2/3; 29/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 29/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 30/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 27/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time=  38.8s\n",
      "[CV 2/3; 30/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 28/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=  38.9s\n",
      "[CV 3/3; 30/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 28/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=  38.7s\n",
      "[CV 1/3; 31/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 28/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time=  39.0s\n",
      "[CV 2/3; 31/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 29/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.478 total time=  40.9s\n",
      "[CV 3/3; 31/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 29/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.479 total time=  40.2s\n",
      "[CV 3/3; 29/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.478 total time=  40.0s\n",
      "[CV 1/3; 32/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 30/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time=  40.6s\n",
      "[CV 2/3; 32/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 32/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 30/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.479 total time=  42.1s\n",
      "[CV 1/3; 33/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 30/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time=  42.2s\n",
      "[CV 2/3; 33/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 31/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.494 total time=  44.9s\n",
      "[CV 3/3; 33/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 31/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.494 total time=  44.5s\n",
      "[CV 1/3; 34/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 31/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.492 total time=  45.5s\n",
      "[CV 2/3; 34/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 32/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.494 total time=  45.3s\n",
      "[CV 2/3; 32/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.494 total time=  45.5s\n",
      "[CV 3/3; 34/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 32/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.492 total time=  45.3s\n",
      "[CV 1/3; 35/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 35/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 33/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=  39.9s\n",
      "[CV 3/3; 35/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 33/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=  40.0s\n",
      "[CV 1/3; 36/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 33/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=  39.9s\n",
      "[CV 2/3; 36/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 34/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=  39.8s\n",
      "[CV 3/3; 36/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 34/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=  39.7s\n",
      "[CV 1/3; 37/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 34/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=  39.8s\n",
      "[CV 2/3; 37/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 35/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.478 total time=  41.0s\n",
      "[CV 2/3; 35/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.480 total time=  41.3s\n",
      "[CV 3/3; 37/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 38/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 35/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.479 total time=  42.8s\n",
      "[CV 2/3; 38/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 36/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.479 total time=  44.3s\n",
      "[CV 3/3; 38/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 36/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.480 total time=  45.0s\n",
      "[CV 1/3; 39/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 36/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time=  44.8s\n",
      "[CV 2/3; 39/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 37/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.500 total time= 1.7min\n",
      "[CV 3/3; 39/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 37/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.501 total time= 1.7min\n",
      "[CV 1/3; 38/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.500 total time= 1.7min\n",
      "[CV 1/3; 40/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 37/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.501 total time= 1.8min\n",
      "[CV 2/3; 40/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 40/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 38/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.501 total time= 1.7min\n",
      "[CV 1/3; 41/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 39/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 1.7min\n",
      "[CV 2/3; 41/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 38/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.501 total time= 1.8min\n",
      "[CV 3/3; 41/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 39/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 1.7min\n",
      "[CV 1/3; 42/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 39/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 1.6min\n",
      "[CV 2/3; 42/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 40/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 1.6min\n",
      "[CV 2/3; 40/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 1.5min\n",
      "[CV 3/3; 42/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 43/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 40/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 1.5min\n",
      "[CV 2/3; 43/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 41/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time= 1.6min\n",
      "[CV 3/3; 43/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 41/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.508 total time= 1.6min\n",
      "[CV 1/3; 44/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 42/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time= 1.6min\n",
      "[CV 3/3; 41/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time= 1.6min\n",
      "[CV 2/3; 44/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 44/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 42/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.508 total time= 1.7min\n",
      "[CV 1/3; 45/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 43/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.500 total time= 1.7min\n",
      "[CV 2/3; 45/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 42/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time= 1.8min\n",
      "[CV 2/3; 43/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.501 total time= 1.7min\n",
      "[CV 3/3; 45/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 46/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 43/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.502 total time= 1.8min\n",
      "[CV 2/3; 46/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 44/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.500 total time= 1.8min\n",
      "[CV 3/3; 46/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 44/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.501 total time= 1.8min\n",
      "[CV 3/3; 44/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.502 total time= 1.8min\n",
      "[CV 1/3; 47/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 47/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 45/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 1.6min\n",
      "[CV 3/3; 47/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 45/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 1.6min\n",
      "[CV 1/3; 48/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 45/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 1.6min\n",
      "[CV 2/3; 48/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 46/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 1.6min\n",
      "[CV 3/3; 48/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 46/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 1.6min\n",
      "[CV 1/3; 49/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 46/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 1.6min\n",
      "[CV 2/3; 49/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 47/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.509 total time= 1.6min\n",
      "[CV 1/3; 47/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time= 1.6min\n",
      "[CV 3/3; 49/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 50/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 47/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time= 1.7min\n",
      "[CV 2/3; 50/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 48/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time= 1.8min\n",
      "[CV 2/3; 48/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.509 total time= 1.7min\n",
      "[CV 3/3; 50/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 51/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 48/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time= 1.7min\n",
      "[CV 2/3; 51/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 49/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.501 total time= 1.8min\n",
      "[CV 3/3; 51/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 49/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.502 total time= 1.8min\n",
      "[CV 1/3; 52/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 50/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.501 total time= 1.8min\n",
      "[CV 2/3; 52/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 49/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.502 total time= 1.9min\n",
      "[CV 3/3; 52/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 50/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.502 total time= 1.8min\n",
      "[CV 1/3; 53/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 51/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 1.7min\n",
      "[CV 2/3; 53/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 50/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.502 total time= 1.8min\n",
      "[CV 2/3; 51/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 1.7min\n",
      "[CV 3/3; 53/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 54/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 51/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 1.7min\n",
      "[CV 2/3; 54/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 52/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 1.7min\n",
      "[CV 3/3; 54/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 52/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 1.7min\n",
      "[CV 1/3; 55/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 52/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 1.7min\n",
      "[CV 2/3; 55/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 53/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time= 1.8min\n",
      "[CV 3/3; 55/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 53/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.509 total time= 1.7min\n",
      "[CV 1/3; 56/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 54/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time= 1.8min\n",
      "[CV 3/3; 53/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time= 1.8min\n",
      "[CV 2/3; 56/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 56/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 54/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.509 total time= 1.8min\n",
      "[CV 1/3; 57/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 54/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time= 1.9min\n",
      "[CV 2/3; 57/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 55/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.483 total time= 2.2min\n",
      "[CV 3/3; 57/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 55/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.485 total time= 2.1min\n",
      "[CV 1/3; 58/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 55/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.484 total time= 2.0min\n",
      "[CV 2/3; 58/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 56/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.483 total time= 2.0min\n",
      "[CV 3/3; 58/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 56/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.485 total time= 2.0min\n",
      "[CV 1/3; 59/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 56/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.484 total time= 2.0min\n",
      "[CV 2/3; 59/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 57/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time= 2.2min\n",
      "[CV 1/3; 57/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 2.3min\n",
      "[CV 3/3; 59/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 60/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 57/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time= 2.2min\n",
      "[CV 2/3; 60/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 58/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 2.2min\n",
      "[CV 3/3; 60/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 58/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time= 2.1min\n",
      "[CV 1/3; 61/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 59/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time= 2.1min\n",
      "[CV 2/3; 59/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 2.0min\n",
      "[CV 2/3; 61/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 58/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time= 2.2min\n",
      "[CV 3/3; 61/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 62/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 59/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time= 2.1min\n",
      "[CV 1/3; 60/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time= 2.1min\n",
      "[CV 2/3; 62/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 62/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 60/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 2.2min\n",
      "[CV 1/3; 63/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 60/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time= 2.1min\n",
      "[CV 2/3; 63/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 61/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.484 total time= 2.1min\n",
      "[CV 3/3; 63/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 61/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.485 total time= 2.1min\n",
      "[CV 1/3; 64/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 61/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.484 total time= 2.1min\n",
      "[CV 2/3; 64/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 62/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.484 total time= 2.1min\n",
      "[CV 3/3; 64/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 62/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.485 total time= 2.0min\n",
      "[CV 1/3; 65/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 62/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.484 total time= 2.0min\n",
      "[CV 2/3; 65/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 63/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time= 2.3min\n",
      "[CV 3/3; 65/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 63/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 2.2min\n",
      "[CV 1/3; 66/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 63/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time= 2.3min\n",
      "[CV 2/3; 66/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 64/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time= 2.2min\n",
      "[CV 2/3; 64/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 2.2min\n",
      "[CV 3/3; 66/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 67/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 64/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time= 2.2min\n",
      "[CV 2/3; 67/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 65/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time= 2.1min\n",
      "[CV 3/3; 67/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 65/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 2.1min\n",
      "[CV 1/3; 68/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 65/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time= 2.2min\n",
      "[CV 2/3; 68/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 66/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time= 2.2min\n",
      "[CV 3/3; 68/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 66/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 2.3min\n",
      "[CV 1/3; 69/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 67/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.485 total time= 2.2min\n",
      "[CV 2/3; 69/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 67/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.486 total time= 2.1min\n",
      "[CV 3/3; 69/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 66/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time= 2.3min\n",
      "[CV 1/3; 70/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 67/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.484 total time= 2.1min\n",
      "[CV 2/3; 70/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 68/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.485 total time= 2.1min\n",
      "[CV 3/3; 70/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 68/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.486 total time= 2.0min\n",
      "[CV 1/3; 71/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 68/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.484 total time= 2.0min\n",
      "[CV 2/3; 71/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 69/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time= 2.3min\n",
      "[CV 3/3; 71/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 69/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 2.3min\n",
      "[CV 1/3; 72/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 69/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 2.3min\n",
      "[CV 1/3; 70/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time= 2.3min\n",
      "[CV 2/3; 72/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 72/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 70/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 2.3min\n",
      "[CV 1/3; 73/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 70/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 2.3min\n",
      "[CV 2/3; 73/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 71/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time= 2.1min\n",
      "[CV 3/3; 73/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 71/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time= 2.1min\n",
      "[CV 1/3; 74/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 71/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time= 2.3min\n",
      "[CV 2/3; 74/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 72/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time= 2.3min\n",
      "[CV 3/3; 72/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time= 2.3min\n",
      "[CV 3/3; 74/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 75/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 72/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time= 2.4min\n",
      "[CV 2/3; 75/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 73/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time= 9.1min\n",
      "[CV 3/3; 75/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 73/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 9.0min\n",
      "[CV 1/3; 76/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 73/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time= 8.7min\n",
      "[CV 2/3; 76/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 74/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.471 total time= 8.9min\n",
      "[CV 3/3; 76/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 74/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 9.0min\n",
      "[CV 1/3; 77/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 74/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.471 total time= 9.1min\n",
      "[CV 2/3; 77/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 75/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=10.0min\n",
      "[CV 3/3; 77/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 75/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=10.0min\n",
      "[CV 1/3; 78/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 75/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 9.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 78/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 76/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 9.5min\n",
      "[CV 2/3; 76/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 9.5min\n",
      "[CV 3/3; 78/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 79/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 76/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 9.5min\n",
      "[CV 2/3; 79/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 77/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.503 total time=10.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 79/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 77/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=10.2min\n",
      "[CV 1/3; 80/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 77/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.504 total time=10.4min\n",
      "[CV 1/3; 78/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=10.3min\n",
      "[CV 2/3; 80/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 80/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 79/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time= 9.0min\n",
      "[CV 1/3; 81/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 79/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 8.9min\n",
      "[CV 2/3; 81/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 78/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=10.5min\n",
      "[CV 3/3; 81/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 78/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.504 total time=10.6min\n",
      "[CV 1/3; 82/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 79/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time= 8.8min\n",
      "[CV 2/3; 82/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 80/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.471 total time= 9.1min\n",
      "[CV 3/3; 82/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 80/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 8.7min\n",
      "[CV 1/3; 83/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 80/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.471 total time= 9.0min\n",
      "[CV 2/3; 83/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 81/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 9.7min\n",
      "[CV 2/3; 81/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 9.8min\n",
      "[CV 3/3; 83/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 84/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 81/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 9.8min\n",
      "[CV 2/3; 84/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 82/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 9.7min\n",
      "[CV 3/3; 84/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 82/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 9.8min\n",
      "[CV 1/3; 85/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 82/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 9.8min\n",
      "[CV 2/3; 85/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 83/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.503 total time=10.0min\n",
      "[CV 3/3; 85/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 83/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=10.1min\n",
      "[CV 1/3; 86/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 83/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.503 total time=10.7min\n",
      "[CV 2/3; 86/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 84/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=10.7min\n",
      "[CV 3/3; 86/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 85/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.472 total time= 9.7min\n",
      "[CV 1/3; 87/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 84/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=10.8min\n",
      "[CV 2/3; 87/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 85/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time= 9.7min\n",
      "[CV 3/3; 84/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=10.8min\n",
      "[CV 3/3; 87/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 88/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 85/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.472 total time= 9.3min\n",
      "[CV 2/3; 88/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 86/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.472 total time= 9.5min\n",
      "[CV 3/3; 88/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 86/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.472 total time= 9.0min\n",
      "[CV 1/3; 89/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 86/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time= 9.2min\n",
      "[CV 2/3; 89/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 87/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=10.6min\n",
      "[CV 3/3; 89/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 87/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.513 total time=10.6min\n",
      "[CV 1/3; 90/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 87/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=10.6min\n",
      "[CV 1/3; 88/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=10.6min\n",
      "[CV 2/3; 90/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 90/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 88/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.513 total time=10.6min\n",
      "[CV 1/3; 91/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 88/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time=10.7min\n",
      "[CV 2/3; 91/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 89/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.503 total time=10.9min\n",
      "[CV 3/3; 91/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 89/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=10.9min\n",
      "[CV 1/3; 92/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 91/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time= 9.3min\n",
      "[CV 2/3; 92/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 91/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time= 8.9min\n",
      "[CV 3/3; 92/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 89/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.503 total time=11.0min\n",
      "[CV 1/3; 93/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 90/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=11.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 93/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 90/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=11.0min\n",
      "[CV 3/3; 90/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=11.0min\n",
      "[CV 3/3; 93/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 94/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 91/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time= 9.4min\n",
      "[CV 2/3; 94/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 92/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time= 9.4min\n",
      "[CV 3/3; 94/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 92/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time= 8.5min\n",
      "[CV 1/3; 95/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 92/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time= 8.4min\n",
      "[CV 2/3; 95/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 93/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.463 total time=10.3min\n",
      "[CV 3/3; 95/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 93/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=10.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 96/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 93/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=10.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 96/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 94/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=10.4min\n",
      "[CV 3/3; 96/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 94/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=10.2min\n",
      "[CV 1/3; 97/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 94/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time=10.2min\n",
      "[CV 2/3; 97/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 95/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.464 total time=10.8min\n",
      "[CV 3/3; 97/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 95/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.466 total time=10.9min\n",
      "[CV 1/3; 98/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 95/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.464 total time=10.9min\n",
      "[CV 2/3; 98/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 96/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.464 total time=11.0min\n",
      "[CV 2/3; 96/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time=10.8min\n",
      "[CV 3/3; 98/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 99/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 96/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time=11.2min\n",
      "[CV 2/3; 99/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 97/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time= 9.3min\n",
      "[CV 3/3; 99/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 97/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time= 9.8min\n",
      "[CV 1/3; 100/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 97/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time= 9.0min\n",
      "[CV 2/3; 100/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 98/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time= 9.0min\n",
      "[CV 3/3; 100/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 98/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time= 9.0min\n",
      "[CV 1/3; 101/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 98/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time= 9.3min\n",
      "[CV 2/3; 101/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 99/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=10.7min\n",
      "[CV 3/3; 101/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 99/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time=10.8min\n",
      "[CV 1/3; 102/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 99/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time=10.5min\n",
      "[CV 2/3; 102/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 100/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=10.5min\n",
      "[CV 3/3; 102/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 100/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=10.5min\n",
      "[CV 1/3; 103/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 100/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time=10.6min\n",
      "[CV 2/3; 103/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 101/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.464 total time=10.9min\n",
      "[CV 3/3; 103/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 101/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=11.1min\n",
      "[CV 1/3; 104/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 101/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.464 total time=11.2min\n",
      "[CV 2/3; 104/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 102/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.464 total time=11.3min\n",
      "[CV 3/3; 104/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 103/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.466 total time= 9.6min\n",
      "[CV 1/3; 105/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 102/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time=11.5min\n",
      "[CV 2/3; 105/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 103/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.466 total time= 9.9min\n",
      "[CV 3/3; 105/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 102/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.464 total time=11.8min\n",
      "[CV 1/3; 106/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 103/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.466 total time=10.0min\n",
      "[CV 2/3; 106/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 104/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time= 9.8min\n",
      "[CV 3/3; 106/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 104/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time=10.0min\n",
      "[CV 1/3; 107/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 104/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time= 9.7min\n",
      "[CV 2/3; 107/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 105/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=11.4min\n",
      "[CV 3/3; 107/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 105/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=11.5min\n",
      "[CV 1/3; 108/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 105/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time=11.3min\n",
      "[CV 2/3; 108/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 106/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=11.0min\n",
      "[CV 3/3; 108/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 106/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=11.0min\n",
      "[CV 1/3; 109/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 106/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=11.1min\n",
      "[CV 2/3; 109/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 109/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.451 total time=  34.5s\n",
      "[CV 3/3; 109/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 109/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.450 total time=  33.6s\n",
      "[CV 1/3; 110/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 109/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.450 total time=  30.5s\n",
      "[CV 2/3; 110/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 110/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.451 total time=  37.6s\n",
      "[CV 3/3; 110/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 110/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.450 total time=  36.4s\n",
      "[CV 1/3; 111/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 107/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.464 total time=11.0min\n",
      "[CV 2/3; 111/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 110/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.450 total time=  31.3s\n",
      "[CV 1/3; 111/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  28.4s\n",
      "[CV 3/3; 111/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 112/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 107/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=10.9min\n",
      "[CV 2/3; 112/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 111/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.443 total time=  29.2s\n",
      "[CV 3/3; 112/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 111/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.444 total time=  29.8s\n",
      "[CV 1/3; 112/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  29.5s\n",
      "[CV 1/3; 113/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 113/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 112/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.443 total time=  34.0s\n",
      "[CV 3/3; 113/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 112/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.444 total time=  32.3s\n",
      "[CV 1/3; 114/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 113/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.437 total time=  33.4s\n",
      "[CV 2/3; 113/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.435 total time=  33.2s\n",
      "[CV 2/3; 114/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 114/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 113/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.435 total time=  29.1s\n",
      "[CV 1/3; 115/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 114/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.437 total time=  29.5s\n",
      "[CV 2/3; 115/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 114/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.435 total time=  29.4s\n",
      "[CV 3/3; 114/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.435 total time=  30.0s\n",
      "[CV 3/3; 115/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 116/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 115/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.451 total time=  29.6s\n",
      "[CV 2/3; 116/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 115/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.450 total time=  30.4s\n",
      "[CV 3/3; 116/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 115/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.450 total time=  33.1s\n",
      "[CV 1/3; 116/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.451 total time=  33.1s\n",
      "[CV 1/3; 117/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 117/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 116/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.450 total time=  34.0s\n",
      "[CV 3/3; 117/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 116/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.450 total time=  32.5s\n",
      "[CV 1/3; 118/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 117/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  28.2s\n",
      "[CV 2/3; 118/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 117/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.444 total time=  27.0s\n",
      "[CV 3/3; 118/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 117/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.444 total time=  27.6s\n",
      "[CV 1/3; 119/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 118/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  27.9s\n",
      "[CV 2/3; 119/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 118/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.444 total time=  27.8s\n",
      "[CV 3/3; 119/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 118/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.444 total time=  28.1s\n",
      "[CV 1/3; 120/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 119/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.437 total time=  29.1s\n",
      "[CV 2/3; 120/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 119/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.435 total time=  29.1s\n",
      "[CV 3/3; 120/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 119/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.435 total time=  29.4s\n",
      "[CV 1/3; 121/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 120/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.437 total time=  29.3s\n",
      "[CV 2/3; 121/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 120/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.435 total time=  34.3s\n",
      "[CV 3/3; 121/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 120/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.435 total time=  34.1s\n",
      "[CV 1/3; 121/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.452 total time=  34.8s\n",
      "[CV 1/3; 122/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 121/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.450 total time=  34.2s\n",
      "[CV 2/3; 122/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 122/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 121/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.450 total time=  28.1s\n",
      "[CV 1/3; 123/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 122/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.452 total time=  27.7s\n",
      "[CV 2/3; 123/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 122/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.450 total time=  27.7s\n",
      "[CV 3/3; 122/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.450 total time=  27.7s\n",
      "[CV 3/3; 123/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 124/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 123/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.448 total time=  28.8s\n",
      "[CV 2/3; 124/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 107/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.464 total time=10.8min\n",
      "[CV 2/3; 123/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  29.7s\n",
      "[CV 3/3; 123/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.447 total time=  28.9s\n",
      "[CV 3/3; 124/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 124/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.448 total time=  29.3s\n",
      "[CV 1/3; 125/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 125/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 125/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 124/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  30.8s\n",
      "[CV 1/3; 126/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 124/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.447 total time=  32.1s\n",
      "[CV 1/3; 125/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.438 total time=  32.2s\n",
      "[CV 2/3; 126/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 125/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.436 total time=  32.7s\n",
      "[CV 3/3; 126/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 108/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.464 total time=10.8min\n",
      "[CV 3/3; 125/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.435 total time=  31.9s\n",
      "[CV 1/3; 127/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 127/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 127/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 126/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.438 total time=  29.4s\n",
      "[CV 2/3; 108/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time=10.9min\n",
      "[CV 1/3; 128/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 128/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 108/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.464 total time=10.7min\n",
      "[CV 2/3; 126/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.436 total time=  29.6s\n",
      "[CV 3/3; 126/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.435 total time=  29.3s\n",
      "[CV 3/3; 128/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 129/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 127/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.448 total time=  30.2s\n",
      "[CV 2/3; 129/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 127/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.447 total time=  30.6s\n",
      "[CV 3/3; 127/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.448 total time=  30.0s\n",
      "[CV 3/3; 129/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 130/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 130/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 128/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.448 total time=  29.5s\n",
      "[CV 3/3; 130/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 128/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.447 total time=  30.0s\n",
      "[CV 1/3; 131/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 129/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  32.9s\n",
      "[CV 3/3; 128/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.448 total time=  34.2s\n",
      "[CV 2/3; 131/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 129/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.443 total time=  32.9s\n",
      "[CV 3/3; 129/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.444 total time=  33.1s\n",
      "[CV 1/3; 130/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  31.4s\n",
      "[CV 3/3; 131/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 132/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 130/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.443 total time=  32.0s\n",
      "[CV 2/3; 132/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 132/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 133/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 130/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.444 total time=  32.0s\n",
      "[CV 2/3; 133/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 131/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.437 total time=  32.6s\n",
      "[CV 3/3; 133/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 131/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.435 total time=  28.2s\n",
      "[CV 3/3; 131/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.435 total time=  27.9s\n",
      "[CV 1/3; 134/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 132/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.437 total time=  28.0s\n",
      "[CV 2/3; 134/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 132/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.435 total time=  27.4s\n",
      "[CV 3/3; 134/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 132/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.435 total time=  27.5s\n",
      "[CV 1/3; 135/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 133/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.448 total time=  27.7s\n",
      "[CV 2/3; 135/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 135/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 133/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.447 total time=  26.4s\n",
      "[CV 1/3; 136/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 133/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.448 total time=  26.6s\n",
      "[CV 2/3; 136/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 134/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.448 total time=  26.1s\n",
      "[CV 2/3; 134/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.447 total time=  25.8s\n",
      "[CV 3/3; 136/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 135/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  24.5s\n",
      "[CV 3/3; 134/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.448 total time=  25.8s\n",
      "[CV 1/3; 137/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 135/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.444 total time=  24.4s\n",
      "[CV 2/3; 137/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 137/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 135/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.444 total time=  24.7s\n",
      "[CV 1/3; 138/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 138/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 136/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  23.9s\n",
      "[CV 3/3; 138/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 136/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.444 total time=  24.0s\n",
      "[CV 1/3; 139/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 136/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.444 total time=  24.3s\n",
      "[CV 1/3; 137/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.437 total time=  24.8s\n",
      "[CV 2/3; 137/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.436 total time=  24.8s\n",
      "[CV 2/3; 139/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 137/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.435 total time=  25.2s\n",
      "[CV 3/3; 139/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 138/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.437 total time=  24.6s\n",
      "[CV 1/3; 140/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 138/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.436 total time=  24.6s\n",
      "[CV 2/3; 140/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 140/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 141/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 138/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.435 total time=  24.7s\n",
      "[CV 2/3; 141/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 139/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.448 total time=  23.9s\n",
      "[CV 3/3; 141/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 139/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  24.5s\n",
      "[CV 3/3; 139/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.447 total time=  24.6s\n",
      "[CV 1/3; 142/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 140/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.448 total time=  24.7s\n",
      "[CV 2/3; 142/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 142/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 140/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  25.5s\n",
      "[CV 1/3; 143/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 141/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.448 total time=  24.0s\n",
      "[CV 3/3; 140/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.447 total time=  25.8s\n",
      "[CV 2/3; 143/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 143/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 141/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  24.3s\n",
      "[CV 3/3; 141/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.447 total time=  23.7s\n",
      "[CV 1/3; 144/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 144/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 142/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.448 total time=  23.5s\n",
      "[CV 2/3; 142/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  23.8s\n",
      "[CV 3/3; 144/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 142/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.447 total time=  23.7s\n",
      "[CV 1/3; 145/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 143/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.437 total time=  24.2s\n",
      "[CV 2/3; 145/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 143/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.436 total time=  23.7s\n",
      "[CV 3/3; 145/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 143/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.436 total time=  23.9s\n",
      "[CV 1/3; 146/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 146/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 144/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.437 total time=  24.6s\n",
      "[CV 2/3; 144/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.436 total time=  24.0s\n",
      "[CV 3/3; 146/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 147/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 144/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.436 total time=  25.1s\n",
      "[CV 2/3; 147/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 145/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.408 total time=  26.1s\n",
      "[CV 2/3; 145/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.408 total time=  25.7s\n",
      "[CV 3/3; 147/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 148/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 145/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.408 total time=  26.3s\n",
      "[CV 1/3; 146/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.408 total time=  26.6s\n",
      "[CV 2/3; 148/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 146/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.408 total time=  26.2s\n",
      "[CV 3/3; 148/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 149/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 146/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.408 total time=  25.7s\n",
      "[CV 1/3; 147/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  24.7s\n",
      "[CV 2/3; 149/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 149/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 147/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.443 total time=  24.3s\n",
      "[CV 1/3; 150/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 147/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.445 total time=  24.2s\n",
      "[CV 1/3; 148/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  24.3s\n",
      "[CV 2/3; 150/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 150/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 148/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.443 total time=  24.2s\n",
      "[CV 3/3; 148/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.445 total time=  23.7s\n",
      "[CV 1/3; 151/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 151/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 149/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.441 total time=  24.9s\n",
      "[CV 3/3; 151/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 149/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.439 total time=  24.8s\n",
      "[CV 3/3; 149/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.440 total time=  24.5s\n",
      "[CV 1/3; 152/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 152/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 150/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.441 total time=  25.7s\n",
      "[CV 3/3; 152/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 150/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.439 total time=  25.4s\n",
      "[CV 3/3; 150/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.440 total time=  25.1s\n",
      "[CV 1/3; 153/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 153/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 151/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.408 total time=  25.7s\n",
      "[CV 2/3; 151/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.408 total time=  25.8s\n",
      "[CV 3/3; 153/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 154/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 151/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.408 total time=  26.0s\n",
      "[CV 2/3; 154/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 152/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.408 total time=  26.0s\n",
      "[CV 2/3; 152/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.408 total time=  26.2s\n",
      "[CV 3/3; 154/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 155/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 152/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.408 total time=  26.5s\n",
      "[CV 1/3; 153/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  25.3s\n",
      "[CV 2/3; 155/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 153/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.444 total time=  25.0s\n",
      "[CV 3/3; 155/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 153/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.445 total time=  25.2s\n",
      "[CV 1/3; 156/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 154/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  24.7s\n",
      "[CV 2/3; 156/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 156/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 154/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.444 total time=  25.0s\n",
      "[CV 3/3; 154/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.445 total time=  24.9s\n",
      "[CV 1/3; 157/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 155/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.441 total time=  25.0s\n",
      "[CV 2/3; 157/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 157/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 155/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.439 total time=  25.4s\n",
      "[CV 3/3; 155/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.440 total time=  25.1s\n",
      "[CV 1/3; 158/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 158/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 156/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.441 total time=  25.7s\n",
      "[CV 2/3; 156/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.439 total time=  25.9s\n",
      "[CV 3/3; 158/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 156/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.440 total time=  26.3s\n",
      "[CV 1/3; 159/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 159/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 157/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.408 total time=  26.4s\n",
      "[CV 2/3; 157/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.408 total time=  26.4s\n",
      "[CV 3/3; 159/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 157/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.408 total time=  27.1s\n",
      "[CV 1/3; 160/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 160/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 158/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.408 total time=  28.3s\n",
      "[CV 2/3; 158/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.408 total time=  28.0s\n",
      "[CV 3/3; 160/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 158/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.408 total time=  27.9s\n",
      "[CV 1/3; 159/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.449 total time=  25.9s\n",
      "[CV 1/3; 161/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 161/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 159/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  25.9s\n",
      "[CV 3/3; 161/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 162/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 159/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.447 total time=  25.5s\n",
      "[CV 1/3; 160/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.449 total time=  25.3s\n",
      "[CV 2/3; 162/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 160/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  24.6s\n",
      "[CV 3/3; 162/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 163/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 160/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.447 total time=  24.8s\n",
      "[CV 2/3; 163/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 161/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.442 total time=  24.6s\n",
      "[CV 2/3; 161/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.440 total time=  24.5s\n",
      "[CV 3/3; 163/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 161/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.441 total time=  24.5s\n",
      "[CV 1/3; 164/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 162/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.442 total time=  25.2s\n",
      "[CV 2/3; 164/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 164/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 162/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.440 total time=  25.8s\n",
      "[CV 1/3; 165/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 162/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.441 total time=  26.3s\n",
      "[CV 2/3; 165/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 165/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 6.3min\n",
      "[CV 3/3; 165/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 163/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.520 total time= 7.5min\n",
      "[CV 1/3; 166/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 165/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 7.2min\n",
      "[CV 2/3; 166/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 164/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.522 total time= 7.5min\n",
      "[CV 3/3; 166/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 164/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.520 total time= 7.7min\n",
      "[CV 1/3; 167/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 164/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.521 total time= 8.3min\n",
      "[CV 2/3; 167/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 163/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.521 total time= 9.0min\n",
      "[CV 3/3; 167/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 163/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.522 total time= 9.8min\n",
      "[CV 1/3; 168/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 165/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 6.8min\n",
      "[CV 2/3; 168/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 166/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 5.9min\n",
      "[CV 2/3; 166/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 6.1min\n",
      "[CV 3/3; 168/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 169/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 166/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 6.4min\n",
      "[CV 2/3; 169/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 167/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time= 8.0min\n",
      "[CV 3/3; 169/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 168/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.514 total time= 6.4min\n",
      "[CV 1/3; 170/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 167/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time= 7.2min\n",
      "[CV 2/3; 170/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 167/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.516 total time= 8.3min\n",
      "[CV 3/3; 170/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 168/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.516 total time= 7.1min\n",
      "[CV 1/3; 171/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 169/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.520 total time= 7.9min\n",
      "[CV 2/3; 171/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 168/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.515 total time= 8.7min\n",
      "[CV 3/3; 171/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 169/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.523 total time= 8.7min\n",
      "[CV 1/3; 172/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 169/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.521 total time= 7.9min\n",
      "[CV 2/3; 172/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 170/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.521 total time= 7.1min\n",
      "[CV 3/3; 172/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 170/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.520 total time= 7.8min\n",
      "[CV 1/3; 173/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 170/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.523 total time= 7.9min\n",
      "[CV 2/3; 173/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 171/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 6.2min\n",
      "[CV 3/3; 173/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 171/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 6.2min\n",
      "[CV 1/3; 174/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 172/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 6.1min\n",
      "[CV 2/3; 174/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 172/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 6.3min\n",
      "[CV 3/3; 174/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 171/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 7.8min\n",
      "[CV 1/3; 175/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 173/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time= 6.3min\n",
      "[CV 2/3; 175/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 172/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 7.1min\n",
      "[CV 3/3; 175/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 173/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.517 total time= 7.6min\n",
      "[CV 1/3; 176/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 173/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time= 7.7min\n",
      "[CV 2/3; 176/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 174/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.515 total time= 7.1min\n",
      "[CV 3/3; 176/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 174/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.516 total time= 7.6min\n",
      "[CV 1/3; 177/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 175/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.520 total time= 8.1min\n",
      "[CV 2/3; 177/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 174/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.515 total time= 8.5min\n",
      "[CV 3/3; 177/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 176/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.520 total time= 8.6min\n",
      "[CV 1/3; 178/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 175/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.521 total time=10.1min\n",
      "[CV 2/3; 178/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 175/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.523 total time=11.5min\n",
      "[CV 3/3; 178/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 176/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.523 total time= 7.6min\n",
      "[CV 1/3; 179/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 176/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.521 total time= 8.4min\n",
      "[CV 2/3; 179/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 177/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.513 total time= 7.8min\n",
      "[CV 3/3; 179/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 177/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.513 total time= 7.2min\n",
      "[CV 1/3; 180/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 178/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.513 total time= 6.7min\n",
      "[CV 2/3; 180/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 177/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 9.2min\n",
      "[CV 3/3; 180/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 178/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.513 total time= 7.0min\n",
      "[CV 1/3; 181/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 179/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time= 7.6min\n",
      "[CV 2/3; 181/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 178/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 8.6min\n",
      "[CV 3/3; 181/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 179/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.517 total time= 8.0min\n",
      "[CV 1/3; 182/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 179/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.516 total time= 8.8min\n",
      "[CV 2/3; 182/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 180/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.515 total time= 7.5min\n",
      "[CV 3/3; 182/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 180/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.518 total time= 8.5min\n",
      "[CV 1/3; 183/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 180/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.516 total time= 8.1min\n",
      "[CV 2/3; 183/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 181/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.493 total time= 8.3min\n",
      "[CV 3/3; 183/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 181/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.493 total time= 8.7min\n",
      "[CV 1/3; 184/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 182/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.493 total time= 8.3min\n",
      "[CV 2/3; 184/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 181/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.492 total time=10.2min\n",
      "[CV 3/3; 184/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 182/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.492 total time= 8.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 185/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 182/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.493 total time= 8.7min\n",
      "[CV 2/3; 185/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 183/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time= 7.3min\n",
      "[CV 3/3; 185/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 183/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time= 7.5min\n",
      "[CV 1/3; 186/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 183/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time= 7.6min\n",
      "[CV 2/3; 186/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 184/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time= 6.5min\n",
      "[CV 3/3; 186/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 184/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time= 7.3min\n",
      "[CV 1/3; 187/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 184/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time= 7.1min\n",
      "[CV 2/3; 187/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 185/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.477 total time= 7.6min\n",
      "[CV 3/3; 187/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 185/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.478 total time= 7.2min\n",
      "[CV 1/3; 188/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 185/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.479 total time= 9.2min\n",
      "[CV 2/3; 188/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 186/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time= 8.0min\n",
      "[CV 3/3; 188/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 186/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.479 total time= 7.8min\n",
      "[CV 1/3; 189/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 186/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time= 8.2min\n",
      "[CV 2/3; 189/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 187/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.493 total time= 8.7min\n",
      "[CV 3/3; 189/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 187/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.494 total time=10.6min\n",
      "[CV 1/3; 190/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 188/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.494 total time= 8.0min\n",
      "[CV 1/3; 188/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.493 total time= 8.5min\n",
      "[CV 2/3; 190/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 190/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 187/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.492 total time=10.0min\n",
      "[CV 1/3; 191/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 189/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time= 7.9min\n",
      "[CV 2/3; 191/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 188/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.492 total time= 9.1min\n",
      "[CV 3/3; 191/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 189/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 7.9min\n",
      "[CV 1/3; 192/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 189/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time= 8.1min\n",
      "[CV 2/3; 192/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 190/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time= 6.7min\n",
      "[CV 3/3; 192/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 190/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time= 6.7min\n",
      "[CV 1/3; 193/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 190/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 7.0min\n",
      "[CV 2/3; 193/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 191/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.479 total time= 7.2min\n",
      "[CV 1/3; 191/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.478 total time= 8.0min\n",
      "[CV 3/3; 193/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 194/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 191/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.478 total time= 7.8min\n",
      "[CV 2/3; 194/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 192/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time= 7.9min\n",
      "[CV 3/3; 194/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 192/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.479 total time= 9.0min\n",
      "[CV 1/3; 195/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 193/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.494 total time= 9.3min\n",
      "[CV 2/3; 195/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 192/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time=10.0min\n",
      "[CV 3/3; 195/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 193/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.494 total time=10.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 196/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 194/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.494 total time= 9.5min\n",
      "[CV 2/3; 196/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 193/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.492 total time=10.9min\n",
      "[CV 3/3; 196/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 194/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.492 total time= 9.1min\n",
      "[CV 1/3; 197/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 194/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.494 total time=10.6min\n",
      "[CV 2/3; 197/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 195/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time= 7.9min\n",
      "[CV 3/3; 197/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 195/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 7.9min\n",
      "[CV 1/3; 198/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 195/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 7.9min\n",
      "[CV 2/3; 198/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 196/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time= 7.7min\n",
      "[CV 3/3; 198/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 196/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 8.5min\n",
      "[CV 1/3; 199/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 196/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 8.6min\n",
      "[CV 2/3; 199/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 197/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.480 total time= 7.7min\n",
      "[CV 3/3; 199/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 197/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.479 total time= 9.4min\n",
      "[CV 1/3; 200/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 197/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.479 total time= 8.7min\n",
      "[CV 2/3; 200/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 198/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.479 total time= 7.6min\n",
      "[CV 3/3; 200/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 198/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.480 total time= 8.7min\n",
      "[CV 1/3; 201/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 198/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time= 9.4min\n",
      "[CV 2/3; 201/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 199/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.500 total time=11.5min\n",
      "[CV 3/3; 201/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 199/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.501 total time=11.5min\n",
      "[CV 1/3; 202/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 199/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.501 total time=11.5min\n",
      "[CV 2/3; 202/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 200/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.500 total time=11.4min\n",
      "[CV 3/3; 202/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 200/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.501 total time=11.3min\n",
      "[CV 1/3; 203/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 200/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.501 total time=11.3min\n",
      "[CV 2/3; 203/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 201/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=10.0min\n",
      "[CV 3/3; 203/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 201/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 9.9min\n",
      "[CV 1/3; 204/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 201/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 9.8min\n",
      "[CV 2/3; 204/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 202/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 9.7min\n",
      "[CV 3/3; 204/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 202/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 9.7min\n",
      "[CV 1/3; 205/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 202/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 9.7min\n",
      "[CV 2/3; 205/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 203/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=10.5min\n",
      "[CV 3/3; 205/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 203/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.508 total time=10.6min\n",
      "[CV 1/3; 206/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 203/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=10.6min\n",
      "[CV 2/3; 206/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 204/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=10.6min\n",
      "[CV 3/3; 206/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 204/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.508 total time=10.9min\n",
      "[CV 1/3; 207/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 204/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=10.9min\n",
      "[CV 2/3; 207/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 205/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.500 total time=11.6min\n",
      "[CV 3/3; 207/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 205/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.501 total time=11.6min\n",
      "[CV 1/3; 208/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 205/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.502 total time=11.7min\n",
      "[CV 2/3; 208/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 206/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.500 total time=11.6min\n",
      "[CV 3/3; 208/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 206/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.501 total time=11.6min\n",
      "[CV 1/3; 209/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 206/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.502 total time=11.5min\n",
      "[CV 2/3; 209/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 207/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=10.2min\n",
      "[CV 3/3; 209/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 207/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=10.2min\n",
      "[CV 1/3; 210/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 207/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=10.1min\n",
      "[CV 2/3; 210/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 208/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=10.1min\n",
      "[CV 3/3; 210/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 208/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=10.1min\n",
      "[CV 1/3; 211/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 208/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time=10.1min\n",
      "[CV 2/3; 211/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 209/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=10.5min\n",
      "[CV 3/3; 211/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 209/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.509 total time=10.7min\n",
      "[CV 1/3; 212/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 209/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=10.7min\n",
      "[CV 2/3; 212/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 210/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=10.8min\n",
      "[CV 3/3; 212/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 210/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.509 total time=11.0min\n",
      "[CV 1/3; 213/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 210/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=11.0min\n",
      "[CV 2/3; 213/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 211/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.501 total time=12.1min\n",
      "[CV 3/3; 213/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 211/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.502 total time=12.2min\n",
      "[CV 1/3; 214/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 211/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.502 total time=12.2min\n",
      "[CV 2/3; 214/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 212/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.501 total time=12.0min\n",
      "[CV 3/3; 214/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 212/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.502 total time=12.0min\n",
      "[CV 1/3; 215/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 213/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=10.9min\n",
      "[CV 3/3; 212/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.502 total time=11.9min\n",
      "[CV 2/3; 215/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 215/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 213/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=10.8min\n",
      "[CV 1/3; 216/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 213/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=10.7min\n",
      "[CV 2/3; 216/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 214/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=10.6min\n",
      "[CV 3/3; 216/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 214/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=10.6min\n",
      "[CV 1/3; 217/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 214/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=10.6min\n",
      "[CV 2/3; 217/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 215/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=10.8min\n",
      "[CV 3/3; 217/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 215/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.509 total time=10.9min\n",
      "[CV 3/3; 215/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=10.9min\n",
      "[CV 1/3; 218/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 218/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 216/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=10.9min\n",
      "[CV 3/3; 218/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 216/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.509 total time=11.1min\n",
      "[CV 1/3; 219/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 216/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=11.1min\n",
      "[CV 2/3; 219/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 217/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.483 total time=11.4min\n",
      "[CV 3/3; 219/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 217/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.485 total time=11.5min\n",
      "[CV 1/3; 220/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 217/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.484 total time=11.4min\n",
      "[CV 2/3; 220/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 218/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.484 total time=11.4min\n",
      "[CV 3/3; 220/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 218/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.485 total time=11.4min\n",
      "[CV 1/3; 221/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 218/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.484 total time=11.4min\n",
      "[CV 2/3; 221/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 219/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=10.2min\n",
      "[CV 3/3; 221/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 219/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time=10.1min\n",
      "[CV 1/3; 222/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 219/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=10.1min\n",
      "[CV 2/3; 222/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 220/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=10.1min\n",
      "[CV 3/3; 222/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 220/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time=10.0min\n",
      "[CV 1/3; 223/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 220/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time=10.0min\n",
      "[CV 2/3; 223/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 221/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=10.8min\n",
      "[CV 3/3; 223/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 221/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=10.8min\n",
      "[CV 1/3; 224/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 221/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=10.8min\n",
      "[CV 2/3; 224/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 222/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time=10.9min\n",
      "[CV 3/3; 224/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 222/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=11.0min\n",
      "[CV 1/3; 225/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 222/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time=11.1min\n",
      "[CV 2/3; 225/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 223/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.484 total time=11.6min\n",
      "[CV 3/3; 225/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 223/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.485 total time=11.6min\n",
      "[CV 1/3; 226/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 223/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.484 total time=11.6min\n",
      "[CV 2/3; 226/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 224/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.484 total time=11.5min\n",
      "[CV 3/3; 226/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 224/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.485 total time=11.6min\n",
      "[CV 1/3; 227/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 224/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.484 total time=11.5min\n",
      "[CV 2/3; 227/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 225/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=10.6min\n",
      "[CV 3/3; 227/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 225/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=10.6min\n",
      "[CV 1/3; 228/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 225/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time=10.4min\n",
      "[CV 2/3; 228/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 226/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=10.4min\n",
      "[CV 3/3; 228/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 226/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=10.4min\n",
      "[CV 1/3; 229/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 226/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time=10.4min\n",
      "[CV 2/3; 229/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 227/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=10.9min\n",
      "[CV 3/3; 229/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 227/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=10.9min\n",
      "[CV 3/3; 227/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=10.9min\n",
      "[CV 1/3; 230/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 230/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 228/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time=11.0min\n",
      "[CV 3/3; 230/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 228/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=11.1min\n",
      "[CV 1/3; 231/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 228/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time=11.2min\n",
      "[CV 2/3; 231/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 229/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.485 total time=12.1min\n",
      "[CV 3/3; 231/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 229/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.486 total time=12.1min\n",
      "[CV 1/3; 232/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 229/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.485 total time=12.1min\n",
      "[CV 2/3; 232/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 230/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.485 total time=12.0min\n",
      "[CV 3/3; 232/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 230/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.486 total time=12.1min\n",
      "[CV 1/3; 233/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 230/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.485 total time=12.0min\n",
      "[CV 2/3; 233/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 231/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=11.2min\n",
      "[CV 3/3; 233/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 231/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=11.0min\n",
      "[CV 1/3; 234/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 231/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=11.0min\n",
      "[CV 2/3; 234/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 232/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=11.0min\n",
      "[CV 3/3; 234/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 232/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=11.0min\n",
      "[CV 1/3; 235/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 232/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=11.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 235/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 233/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time=11.1min\n",
      "[CV 3/3; 235/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 233/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=11.3min\n",
      "[CV 1/3; 236/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 233/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=11.2min\n",
      "[CV 2/3; 236/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 234/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time=11.3min\n",
      "[CV 3/3; 236/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 234/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=11.5min\n",
      "[CV 1/3; 237/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 234/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time=11.6min\n",
      "[CV 2/3; 237/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 235/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time=11.2min\n",
      "[CV 3/3; 237/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 235/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=11.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 238/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 235/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time=11.2min\n",
      "[CV 2/3; 238/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 236/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.471 total time=11.1min\n",
      "[CV 3/3; 238/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 236/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=11.0min\n",
      "[CV 1/3; 239/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 236/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.471 total time=10.9min\n",
      "[CV 2/3; 239/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 237/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=10.7min\n",
      "[CV 3/3; 239/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 237/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=10.7min\n",
      "[CV 1/3; 240/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 237/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=10.5min\n",
      "[CV 2/3; 240/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 238/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time=10.5min\n",
      "[CV 3/3; 240/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 238/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=10.5min\n",
      "[CV 1/3; 241/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 238/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.510 total time=10.5min\n",
      "[CV 2/3; 241/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 239/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.503 total time=11.6min\n",
      "[CV 3/3; 241/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 239/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.505 total time=11.6min\n",
      "[CV 1/3; 242/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 239/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.503 total time=11.7min\n",
      "[CV 2/3; 242/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 240/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=11.7min\n",
      "[CV 3/3; 242/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 240/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=11.9min\n",
      "[CV 1/3; 243/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 241/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time=11.4min\n",
      "[CV 2/3; 243/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 240/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=11.9min\n",
      "[CV 3/3; 243/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 241/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=11.4min\n",
      "[CV 1/3; 244/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 241/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time=11.4min\n",
      "[CV 2/3; 244/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 242/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.471 total time=11.3min\n",
      "[CV 3/3; 244/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 242/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=11.3min\n",
      "[CV 1/3; 245/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 242/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.471 total time=11.2min\n",
      "[CV 2/3; 245/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 243/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.513 total time=11.0min\n",
      "[CV 3/3; 245/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 243/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=11.0min\n",
      "[CV 1/3; 246/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 243/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=11.0min\n",
      "[CV 2/3; 246/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 244/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=11.0min\n",
      "[CV 3/3; 246/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 244/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.513 total time=10.9min\n",
      "[CV 1/3; 247/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 244/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time=10.9min\n",
      "[CV 2/3; 247/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 245/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.503 total time=11.5min\n",
      "[CV 3/3; 247/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 245/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=11.6min\n",
      "[CV 1/3; 248/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 245/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.504 total time=11.7min\n",
      "[CV 2/3; 248/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 246/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=11.8min\n",
      "[CV 3/3; 248/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 246/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=11.9min\n",
      "[CV 1/3; 249/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 246/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=12.0min\n",
      "[CV 2/3; 249/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 247/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.472 total time=11.7min\n",
      "[CV 3/3; 249/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 247/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=11.8min\n",
      "[CV 1/3; 250/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 247/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.472 total time=11.7min\n",
      "[CV 2/3; 250/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 248/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.472 total time=11.6min\n",
      "[CV 3/3; 250/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 248/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.471 total time=11.6min\n",
      "[CV 1/3; 251/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 248/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.472 total time=11.4min\n",
      "[CV 2/3; 251/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 249/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=11.6min\n",
      "[CV 3/3; 251/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 249/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=11.6min\n",
      "[CV 1/3; 252/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 249/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=11.4min\n",
      "[CV 2/3; 252/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 250/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=11.4min\n",
      "[CV 3/3; 252/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 250/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.513 total time=11.4min\n",
      "[CV 1/3; 253/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 250/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time=11.4min\n",
      "[CV 2/3; 253/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 251/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.503 total time=11.9min\n",
      "[CV 3/3; 253/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 251/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.505 total time=11.8min\n",
      "[CV 1/3; 254/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 251/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.504 total time=12.0min\n",
      "[CV 2/3; 254/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 252/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=12.1min\n",
      "[CV 3/3; 254/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 252/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=12.2min\n",
      "[CV 1/3; 255/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 252/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=12.3min\n",
      "[CV 2/3; 255/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 253/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time=11.2min\n",
      "[CV 3/3; 255/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 253/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time=11.2min\n",
      "[CV 1/3; 256/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 253/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time=11.1min\n",
      "[CV 2/3; 256/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 254/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time=11.1min\n",
      "[CV 3/3; 256/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 254/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time=11.0min\n",
      "[CV 1/3; 257/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 254/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time=11.0min\n",
      "[CV 2/3; 257/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 255/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.473 total time=10.8min\n",
      "[CV 3/3; 257/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 255/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.477 total time=10.7min\n",
      "[CV 1/3; 258/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 255/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.477 total time=10.6min\n",
      "[CV 2/3; 258/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 256/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.473 total time=10.7min\n",
      "[CV 3/3; 258/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 256/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.477 total time=10.6min\n",
      "[CV 1/3; 259/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 256/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.477 total time=10.6min\n",
      "[CV 2/3; 259/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 257/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=11.8min\n",
      "[CV 3/3; 259/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 257/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=11.9min\n",
      "[CV 1/3; 260/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 257/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time=12.0min\n",
      "[CV 2/3; 260/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 258/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.464 total time=12.0min\n",
      "[CV 3/3; 260/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 258/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=12.1min\n",
      "[CV 1/3; 261/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 258/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=12.2min\n",
      "[CV 2/3; 261/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 259/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time=11.3min\n",
      "[CV 3/3; 261/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 259/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.466 total time=11.3min\n",
      "[CV 1/3; 262/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 259/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time=11.3min\n",
      "[CV 2/3; 262/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 260/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time=11.2min\n",
      "[CV 3/3; 262/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 260/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time=11.2min\n",
      "[CV 1/3; 263/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 260/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time=11.0min\n",
      "[CV 2/3; 263/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 261/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.480 total time=11.2min\n",
      "[CV 3/3; 263/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 261/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.480 total time=11.1min\n",
      "[CV 1/3; 264/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 261/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=11.0min\n",
      "[CV 2/3; 264/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 262/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.456 total time=11.0min\n",
      "[CV 3/3; 264/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 262/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.473 total time=11.0min\n",
      "[CV 1/3; 265/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 262/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.479 total time=11.0min\n",
      "[CV 2/3; 265/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 263/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=11.8min\n",
      "[CV 3/3; 265/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 263/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time=11.9min\n",
      "[CV 1/3; 266/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 263/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time=12.0min\n",
      "[CV 2/3; 266/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 264/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.464 total time=12.1min\n",
      "[CV 3/3; 266/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 264/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time=12.3min\n",
      "[CV 1/3; 267/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 264/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time=12.3min\n",
      "[CV 2/3; 267/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 265/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.466 total time=11.8min\n",
      "[CV 3/3; 267/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 265/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.466 total time=11.8min\n",
      "[CV 1/3; 268/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 265/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.466 total time=11.8min\n",
      "[CV 2/3; 268/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 266/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time=11.8min\n",
      "[CV 3/3; 268/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 266/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time=11.7min\n",
      "[CV 1/3; 269/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 266/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time=11.6min\n",
      "[CV 2/3; 269/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 267/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.479 total time=11.8min\n",
      "[CV 3/3; 269/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 267/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.480 total time=11.7min\n",
      "[CV 1/3; 270/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 267/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.479 total time=11.8min\n",
      "[CV 2/3; 270/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 268/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.481 total time=11.7min\n",
      "[CV 3/3; 270/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 268/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.479 total time=11.7min\n",
      "[CV 1/3; 271/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 271/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  20.2s\n",
      "[CV 2/3; 271/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 271/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  19.3s\n",
      "[CV 3/3; 271/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 268/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.463 total time=11.7min\n",
      "[CV 3/3; 271/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  20.2s\n",
      "[CV 1/3; 272/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 272/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 272/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  21.0s\n",
      "[CV 3/3; 272/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 272/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  20.7s\n",
      "[CV 1/3; 273/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 272/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  20.0s\n",
      "[CV 1/3; 273/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.6s\n",
      "[CV 2/3; 273/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 273/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 273/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.6s\n",
      "[CV 1/3; 274/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 273/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.1s\n",
      "[CV 2/3; 274/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 274/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.4s\n",
      "[CV 3/3; 274/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 274/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.0s\n",
      "[CV 1/3; 275/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 274/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.0s\n",
      "[CV 2/3; 275/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 275/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.8s\n",
      "[CV 3/3; 275/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 269/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.466 total time=12.3min\n",
      "[CV 1/3; 276/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 275/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  20.5s\n",
      "[CV 3/3; 275/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  20.0s\n",
      "[CV 2/3; 276/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 276/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 276/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  19.8s\n",
      "[CV 1/3; 277/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 276/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  20.4s\n",
      "[CV 3/3; 276/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  19.2s\n",
      "[CV 2/3; 277/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 277/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 277/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  20.2s\n",
      "[CV 2/3; 269/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=12.2min\n",
      "[CV 1/3; 278/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 278/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 277/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  20.8s\n",
      "[CV 3/3; 277/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  22.0s\n",
      "[CV 3/3; 278/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 279/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 278/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  21.4s\n",
      "[CV 2/3; 278/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  21.0s\n",
      "[CV 2/3; 279/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 279/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 278/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  21.1s\n",
      "[CV 1/3; 279/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  20.1s\n",
      "[CV 1/3; 280/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 280/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 279/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.4s\n",
      "[CV 3/3; 279/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.5s\n",
      "[CV 3/3; 280/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 280/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.7s\n",
      "[CV 1/3; 281/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 280/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.3s\n",
      "[CV 2/3; 281/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 281/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 280/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.5s\n",
      "[CV 1/3; 282/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 281/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  19.0s\n",
      "[CV 2/3; 281/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  19.3s\n",
      "[CV 2/3; 282/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 282/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 281/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  19.3s\n",
      "[CV 1/3; 283/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 282/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.4s\n",
      "[CV 2/3; 282/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.3s\n",
      "[CV 2/3; 283/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 282/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.8s\n",
      "[CV 3/3; 283/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 284/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 283/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  19.2s\n",
      "[CV 2/3; 284/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 283/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  20.2s\n",
      "[CV 3/3; 283/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  19.7s\n",
      "[CV 3/3; 284/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 284/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  21.0s\n",
      "[CV 1/3; 285/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 284/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  20.2s\n",
      "[CV 2/3; 285/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 285/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 269/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.462 total time=12.3min\n",
      "[CV 3/3; 284/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  19.5s\n",
      "[CV 1/3; 285/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.8s\n",
      "[CV 1/3; 286/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 285/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.0s\n",
      "[CV 2/3; 286/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 285/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.8s\n",
      "[CV 3/3; 286/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 287/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 287/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 286/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.8s\n",
      "[CV 2/3; 286/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.2s\n",
      "[CV 3/3; 286/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.9s\n",
      "[CV 3/3; 287/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 288/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 287/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.0s\n",
      "[CV 2/3; 288/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 270/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=12.3min\n",
      "[CV 2/3; 287/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.1s\n",
      "[CV 3/3; 288/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 289/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 289/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 287/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.9s\n",
      "[CV 1/3; 288/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.8s\n",
      "[CV 3/3; 289/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 288/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.8s\n",
      "[CV 3/3; 288/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  19.2s\n",
      "[CV 1/3; 290/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 289/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  19.0s\n",
      "[CV 2/3; 290/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 290/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 289/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.7s\n",
      "[CV 1/3; 291/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 291/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 289/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.9s\n",
      "[CV 3/3; 291/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 290/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.8s\n",
      "[CV 2/3; 290/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  19.1s\n",
      "[CV 1/3; 292/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 290/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  19.5s\n",
      "[CV 1/3; 291/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.8s\n",
      "[CV 2/3; 292/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 291/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.5s\n",
      "[CV 3/3; 292/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 293/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 293/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 291/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.6s\n",
      "[CV 1/3; 292/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.7s\n",
      "[CV 3/3; 293/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 292/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.1s\n",
      "[CV 1/3; 294/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 292/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.9s\n",
      "[CV 2/3; 294/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 293/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.6s\n",
      "[CV 3/3; 294/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 293/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.8s\n",
      "[CV 1/3; 295/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 295/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 293/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 294/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.0s\n",
      "[CV 3/3; 295/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 294/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.0s\n",
      "[CV 1/3; 296/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 294/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.1s\n",
      "[CV 2/3; 296/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 295/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  19.2s\n",
      "[CV 3/3; 296/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 295/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.9s\n",
      "[CV 1/3; 297/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 297/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 270/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=12.1min\n",
      "[CV 3/3; 297/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 295/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.6s\n",
      "[CV 1/3; 298/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 296/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  19.0s\n",
      "[CV 2/3; 296/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.3s\n",
      "[CV 2/3; 298/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 298/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 296/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  19.3s\n",
      "[CV 1/3; 297/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.2s\n",
      "[CV 1/3; 299/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 297/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.2s\n",
      "[CV 2/3; 299/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 299/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 297/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.9s\n",
      "[CV 1/3; 300/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 298/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.1s\n",
      "[CV 2/3; 300/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 298/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.3s\n",
      "[CV 3/3; 298/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.6s\n",
      "[CV 3/3; 300/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 301/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 299/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.1s\n",
      "[CV 2/3; 301/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 299/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.5s\n",
      "[CV 3/3; 299/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.7s\n",
      "[CV 3/3; 301/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 302/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 300/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.8s\n",
      "[CV 2/3; 302/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 300/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 300/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.8s\n",
      "[CV 3/3; 302/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 301/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.8s\n",
      "[CV 1/3; 303/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 301/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.4s\n",
      "[CV 2/3; 303/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 303/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 301/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.7s\n",
      "[CV 1/3; 302/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.4s\n",
      "[CV 1/3; 304/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 270/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=11.9min\n",
      "[CV 2/3; 304/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 304/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 302/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.9s\n",
      "[CV 1/3; 305/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 302/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.6s\n",
      "[CV 1/3; 303/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.4s\n",
      "[CV 2/3; 305/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 303/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.6s\n",
      "[CV 3/3; 305/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 303/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.3s\n",
      "[CV 1/3; 306/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 306/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 304/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.0s\n",
      "[CV 2/3; 304/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.6s\n",
      "[CV 3/3; 306/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 304/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.2s\n",
      "[CV 1/3; 307/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 305/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.9s\n",
      "[CV 2/3; 307/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 307/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 305/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.0s\n",
      "[CV 3/3; 305/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.6s\n",
      "[CV 1/3; 308/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 306/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.1s\n",
      "[CV 2/3; 308/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 306/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.5s\n",
      "[CV 3/3; 308/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 309/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 306/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.6s\n",
      "[CV 1/3; 307/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.2s\n",
      "[CV 2/3; 309/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 307/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.7s\n",
      "[CV 3/3; 309/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 307/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.6s\n",
      "[CV 1/3; 310/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 310/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 308/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.5s\n",
      "[CV 3/3; 310/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 308/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.7s\n",
      "[CV 3/3; 308/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.7s\n",
      "[CV 1/3; 311/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 309/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.7s\n",
      "[CV 2/3; 311/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 311/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 309/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.3s\n",
      "[CV 3/3; 309/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  15.8s\n",
      "[CV 1/3; 312/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 310/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.0s\n",
      "[CV 2/3; 312/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 312/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 310/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.4s\n",
      "[CV 1/3; 313/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 310/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  15.7s\n",
      "[CV 1/3; 311/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  15.9s\n",
      "[CV 2/3; 313/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 311/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.5s\n",
      "[CV 3/3; 313/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 311/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.5s\n",
      "[CV 1/3; 314/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 314/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 312/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.9s\n",
      "[CV 2/3; 312/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.4s\n",
      "[CV 3/3; 314/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 312/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.7s\n",
      "[CV 1/3; 315/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 313/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.7s\n",
      "[CV 2/3; 315/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 315/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 313/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.9s\n",
      "[CV 3/3; 313/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.6s\n",
      "[CV 1/3; 316/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 314/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.0s\n",
      "[CV 2/3; 316/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 314/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.3s\n",
      "[CV 3/3; 316/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 317/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 314/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.3s\n",
      "[CV 1/3; 315/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.1s\n",
      "[CV 2/3; 315/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.9s\n",
      "[CV 2/3; 317/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 315/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.2s\n",
      "[CV 3/3; 317/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 318/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 318/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 316/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.5s\n",
      "[CV 2/3; 316/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.1s\n",
      "[CV 3/3; 318/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 316/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  15.9s\n",
      "[CV 1/3; 319/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 319/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 317/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  15.7s\n",
      "[CV 3/3; 319/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 317/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  15.8s\n",
      "[CV 1/3; 320/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 317/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.7s\n",
      "[CV 2/3; 320/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 318/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.9s\n",
      "[CV 2/3; 318/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.2s\n",
      "[CV 3/3; 320/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 321/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 318/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.9s\n",
      "[CV 1/3; 319/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.7s\n",
      "[CV 2/3; 321/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 321/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 319/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.3s\n",
      "[CV 1/3; 322/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 319/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.2s\n",
      "[CV 2/3; 322/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 320/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.2s\n",
      "[CV 3/3; 322/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 320/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.1s\n",
      "[CV 1/3; 323/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 320/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.1s\n",
      "[CV 1/3; 321/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.7s\n",
      "[CV 2/3; 323/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 323/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 321/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.9s\n",
      "[CV 3/3; 321/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.9s\n",
      "[CV 1/3; 324/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 322/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.6s\n",
      "[CV 2/3; 324/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 322/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.1s\n",
      "[CV 3/3; 324/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 325/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 322/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.4s\n",
      "[CV 1/3; 323/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.3s\n",
      "[CV 2/3; 325/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 325/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 323/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.5s\n",
      "[CV 3/3; 323/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.2s\n",
      "[CV 1/3; 326/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x12f0e2290>\n",
      "[CV 1/3; 326/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x12f0e2290>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 326/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1394594d0>\n",
      "[CV 2/3; 326/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1394594d0>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 326/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x12d66ff10>\n",
      "[CV 3/3; 326/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x12d66ff10>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 327/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x139063f10>\n",
      "[CV 1/3; 324/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  20.0s\n",
      "[CV 2/3; 327/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x33473bf50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 324/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  20.3s\n",
      "[CV 3/3; 327/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x168044890>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 324/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  21.0s\n",
      "[CV 1/3; 328/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 328/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 325/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.520 total time=  43.3s\n",
      "[CV 3/3; 328/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 325/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.523 total time=  44.5s\n",
      "[CV 3/3; 325/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.521 total time=  42.8s\n",
      "[CV 1/3; 329/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16eaa4910>\n",
      "[CV 1/3; 329/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16eaa4910>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 329/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1523e9550>\n",
      "[CV 2/3; 329/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1523e9550>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 329/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x168bf4f90>\n",
      "[CV 3/3; 329/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x168bf4f90>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 330/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x139946450>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 330/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b5a62990>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 328/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.520 total time=  37.5s\n",
      "[CV 3/3; 330/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x12a2d9750>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 328/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.522 total time=  36.7s\n",
      "[CV 1/3; 331/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 328/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.521 total time=  35.6s\n",
      "[CV 2/3; 331/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 331/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.494 total time=  35.6s\n",
      "[CV 3/3; 331/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 331/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.494 total time=  35.0s\n",
      "[CV 1/3; 332/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c8f6e390>\n",
      "[CV 1/3; 332/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c8f6e390>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 332/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16f0964d0>\n",
      "[CV 2/3; 332/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16f0964d0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 332/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2ac1a2cd0>\n",
      "[CV 3/3; 332/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2ac1a2cd0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 333/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x30c3dd790>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 331/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.492 total time=  35.4s\n",
      "[CV 2/3; 333/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10a316210>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 327/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x139063f10>;, score=0.522 total time=10.1min\n",
      "[CV 3/3; 327/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x168044890>;, score=0.521 total time=10.0min\n",
      "[CV 3/3; 333/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x282c4d610>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 334/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 327/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x33473bf50>;, score=0.522 total time=10.1min\n",
      "[CV 2/3; 334/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 330/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b5a62990>;, score=0.522 total time=10.0min\n",
      "[CV 1/3; 330/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x139946450>;, score=0.521 total time=10.0min\n",
      "[CV 3/3; 334/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 334/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.493 total time=  34.1s\n",
      "[CV 1/3; 335/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2ae855010>\n",
      "[CV 1/3; 335/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2ae855010>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 335/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1669a0e10>\n",
      "[CV 2/3; 335/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1669a0e10>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 335/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2ae877f10>\n",
      "[CV 3/3; 335/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2ae877f10>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 336/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16a5a1710>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 330/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x12a2d9750>;, score=0.519 total time=10.1min\n",
      "[CV 2/3; 336/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2ae555210>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 336/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x14e2c2110>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 334/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.492 total time=  36.0s\n",
      "[CV 1/3; 337/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 334/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.491 total time=  36.0s\n",
      "[CV 2/3; 337/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 333/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x30c3dd790>;, score=0.493 total time= 9.9min\n",
      "[CV 2/3; 333/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10a316210>;, score=0.493 total time= 9.9min\n",
      "[CV 3/3; 337/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 338/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17761a2d0>\n",
      "[CV 1/3; 338/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17761a2d0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 338/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177618790>\n",
      "[CV 2/3; 338/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177618790>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 338/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177312c90>\n",
      "[CV 3/3; 338/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177312c90>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 339/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1775f3f10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 337/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.501 total time= 1.3min\n",
      "[CV 2/3; 339/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x285879f50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 337/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.502 total time= 1.3min\n",
      "[CV 3/3; 339/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16c09dad0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 337/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.502 total time= 1.2min\n",
      "[CV 1/3; 340/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 340/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.499 total time= 1.1min\n",
      "[CV 2/3; 340/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 340/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.501 total time= 1.1min\n",
      "[CV 3/3; 340/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 340/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.500 total time= 1.1min\n",
      "[CV 1/3; 341/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x176c9dcd0>\n",
      "[CV 1/3; 341/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x176c9dcd0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 341/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17ce14e10>\n",
      "[CV 2/3; 341/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17ce14e10>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 341/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x176a82290>\n",
      "[CV 3/3; 341/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x176a82290>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 342/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x176b15090>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 333/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x282c4d610>;, score=0.493 total time=10.1min\n",
      "[CV 2/3; 342/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a522ec90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 336/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16a5a1710>;, score=0.492 total time=10.0min\n",
      "[CV 3/3; 342/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x291c75190>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 336/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2ae555210>;, score=0.492 total time=10.1min\n",
      "[CV 1/3; 343/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 336/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x14e2c2110>;, score=0.492 total time=10.2min\n",
      "[CV 2/3; 343/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 343/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.485 total time= 1.5min\n",
      "[CV 3/3; 343/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 343/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.486 total time= 1.5min\n",
      "[CV 1/3; 344/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x295c6e3d0>\n",
      "[CV 1/3; 344/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x295c6e3d0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 344/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28eecead0>\n",
      "[CV 2/3; 344/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28eecead0>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 344/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28eece210>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 344/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28eece210>;, score=nan total time=   0.2s\n",
      "[CV 1/3; 345/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x30e3bf310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 339/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1775f3f10>;, score=0.506 total time=10.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 345/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10a366ed0>\n",
      "[CV 2/3; 339/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x285879f50>;, score=0.505 total time=10.9min\n",
      "[CV 3/3; 345/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15cf7ff10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 339/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16c09dad0>;, score=0.504 total time=10.8min\n",
      "[CV 1/3; 346/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 343/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.484 total time= 1.5min\n",
      "[CV 2/3; 346/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 346/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.483 total time= 1.4min\n",
      "[CV 3/3; 346/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 346/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.484 total time= 1.4min\n",
      "[CV 1/3; 347/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x292ace790>\n",
      "[CV 1/3; 347/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x292ace790>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 347/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x292add010>\n",
      "[CV 2/3; 347/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x292add010>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 347/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2928167d0>\n",
      "[CV 3/3; 347/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2928167d0>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 348/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x292808850>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 346/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.483 total time= 1.3min\n",
      "[CV 2/3; 348/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x169575610>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 342/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x176b15090>;, score=0.504 total time=10.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 348/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1137c9e50>\n",
      "[CV 2/3; 342/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a522ec90>;, score=0.503 total time=10.7min\n",
      "[CV 1/3; 349/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 342/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x291c75190>;, score=0.503 total time=10.7min\n",
      "[CV 2/3; 349/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 345/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x30e3bf310>;, score=0.490 total time=11.0min\n",
      "[CV 3/3; 349/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 345/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10a366ed0>;, score=0.490 total time=11.1min\n",
      "[CV 1/3; 350/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177374950>\n",
      "[CV 1/3; 350/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177374950>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 350/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177376b10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 350/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177376b10>;, score=nan total time=   0.3s\n",
      "[CV 3/3; 350/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177953f10>\n",
      "[CV 3/3; 350/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177953f10>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 351/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x177268a10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 345/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15cf7ff10>;, score=0.491 total time=11.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 351/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x12e0d1f50>\n",
      "[CV 1/3; 348/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x292808850>;, score=0.489 total time=11.2min\n",
      "[CV 3/3; 351/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2ae017f10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 348/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x169575610>;, score=0.488 total time=11.2min\n",
      "[CV 1/3; 352/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 349/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.472 total time= 7.2min\n",
      "[CV 2/3; 352/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 348/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1137c9e50>;, score=0.489 total time=11.2min\n",
      "[CV 3/3; 352/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 349/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.470 total time= 7.7min\n",
      "[CV 1/3; 353/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d74ef50>\n",
      "[CV 1/3; 353/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d74ef50>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 353/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d74ee10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 353/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d74ee10>;, score=nan total time=   0.3s\n",
      "[CV 3/3; 353/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10da2c090>\n",
      "[CV 3/3; 353/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10da2c090>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 354/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10da3fa90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 349/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.472 total time= 7.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 354/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x28cf03b90>\n",
      "[CV 1/3; 352/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.470 total time= 7.2min\n",
      "[CV 3/3; 354/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15f3c7990>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 352/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.469 total time= 7.5min\n",
      "[CV 1/3; 355/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 352/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.470 total time= 7.3min\n",
      "[CV 2/3; 355/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 351/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x12e0d1f50>;, score=0.475 total time=17.8min\n",
      "[CV 3/3; 355/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 351/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x177268a10>;, score=0.475 total time=18.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 356/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15af8f610>\n",
      "[CV 1/3; 356/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15af8f610>;, score=nan total time=   0.1s\n",
      "[CV 2/3; 356/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15af8d050>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 356/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15af8d050>;, score=nan total time=   0.9s\n",
      "[CV 3/3; 356/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15af276d0>\n",
      "[CV 3/3; 356/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15af276d0>;, score=nan total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 357/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16002edd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 355/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.466 total time= 8.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 357/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x108f19a10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 355/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.466 total time= 8.4min\n",
      "[CV 3/3; 357/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d1d6e3d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 351/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2ae017f10>;, score=0.475 total time=18.8min\n",
      "[CV 1/3; 358/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 354/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10da3fa90>;, score=0.473 total time=17.4min\n",
      "[CV 2/3; 358/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 354/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x28cf03b90>;, score=0.475 total time=17.3min\n",
      "[CV 3/3; 358/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 355/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.466 total time= 7.4min\n",
      "[CV 1/3; 359/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28124ce90>\n",
      "[CV 1/3; 359/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28124ce90>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 359/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28124f850>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 359/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28124f850>;, score=nan total time=   0.3s\n",
      "[CV 3/3; 359/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16de6cb10>\n",
      "[CV 3/3; 359/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16de6cb10>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 360/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x28123ff10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 354/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15f3c7990>;, score=0.473 total time=17.2min\n",
      "[CV 2/3; 360/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x168ff51d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 358/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.465 total time= 7.2min\n",
      "[CV 3/3; 360/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10ad5ead0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 358/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.464 total time= 7.4min\n",
      "[CV 1/3; 361/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 361/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.452 total time=  27.6s\n",
      "[CV 2/3; 361/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 361/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.450 total time=  27.8s\n",
      "[CV 3/3; 361/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 361/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.450 total time=  27.5s\n",
      "[CV 1/3; 362/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15a984690>\n",
      "[CV 1/3; 362/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15a984690>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 362/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15b237f10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 362/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15b237f10>;, score=nan total time=   0.3s\n",
      "[CV 3/3; 362/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15a1c0450>\n",
      "[CV 3/3; 362/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15a1c0450>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 363/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15a9c9c10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 358/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.465 total time= 7.5min\n",
      "[CV 2/3; 363/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2923a1150>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 357/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16002edd0>;, score=0.471 total time=17.8min\n",
      "[CV 3/3; 363/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x17784f010>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 357/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x108f19a10>;, score=0.471 total time=17.4min\n",
      "[CV 1/3; 364/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 357/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d1d6e3d0>;, score=0.472 total time=17.4min\n",
      "[CV 2/3; 364/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 364/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.451 total time=  27.2s\n",
      "[CV 3/3; 364/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 364/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.449 total time=  27.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 365/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1158e8890>\n",
      "[CV 1/3; 365/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1158e8890>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 365/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1158eabd0>\n",
      "[CV 2/3; 365/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1158eabd0>;, score=nan total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 365/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x115bb7010>\n",
      "[CV 3/3; 365/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x115bb7010>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 366/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1158eff10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 364/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.450 total time=  28.8s\n",
      "[CV 2/3; 366/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10748f290>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 363/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15a9c9c10>;, score=0.458 total time=10.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 366/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c58e7d0>\n",
      "[CV 1/3; 360/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x28123ff10>;, score=0.469 total time=16.1min\n",
      "[CV 1/3; 367/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 363/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2923a1150>;, score=0.455 total time= 9.9min\n",
      "[CV 2/3; 367/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 367/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.448 total time=  21.1s\n",
      "[CV 2/3; 367/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.446 total time=  21.2s\n",
      "[CV 3/3; 367/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 368/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c9da1290>\n",
      "[CV 1/3; 368/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c9da1290>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 368/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2aa2edbd0>\n",
      "[CV 2/3; 368/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2aa2edbd0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 368/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2aa2ed2d0>\n",
      "[CV 3/3; 368/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2aa2ed2d0>;, score=nan total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 369/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x30f006cd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 367/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.447 total time=  23.4s\n",
      "[CV 2/3; 369/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a80b4f90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 360/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x168ff51d0>;, score=0.470 total time=15.7min\n",
      "[CV 3/3; 369/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1090a1c90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 360/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10ad5ead0>;, score=0.470 total time=16.1min\n",
      "[CV 1/3; 370/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 370/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.449 total time=  22.0s\n",
      "[CV 2/3; 370/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 370/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.447 total time=  20.9s\n",
      "[CV 3/3; 370/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 370/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.448 total time=  20.6s\n",
      "[CV 1/3; 371/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c548fad0>\n",
      "[CV 1/3; 371/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c548fad0>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 371/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x29d7c7c90>\n",
      "[CV 2/3; 371/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x29d7c7c90>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 371/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x29d7c7390>\n",
      "[CV 3/3; 371/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x29d7c7390>;, score=nan total time=   0.1s\n",
      "[CV 1/3; 372/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d42fba50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 363/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x17784f010>;, score=0.455 total time= 9.9min\n",
      "[CV 2/3; 372/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b64a3990>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 366/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1158eff10>;, score=0.457 total time= 9.7min\n",
      "[CV 3/3; 372/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a825db50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 366/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10748f290>;, score=0.455 total time= 9.7min\n",
      "[CV 1/3; 373/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 373/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.408 total time=  21.0s\n",
      "[CV 2/3; 373/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 373/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.408 total time=  21.4s\n",
      "[CV 3/3; 373/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 373/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.408 total time=  20.4s\n",
      "[CV 1/3; 374/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d04ee90>\n",
      "[CV 1/3; 374/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d04ee90>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 374/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d44de50>\n",
      "[CV 2/3; 374/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d44de50>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 374/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10cfa3f10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 374/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10cfa3f10>;, score=nan total time=   0.2s\n",
      "[CV 1/3; 375/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c8abf10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 366/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c58e7d0>;, score=0.454 total time= 9.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 375/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10da61390>\n",
      "[CV 1/3; 369/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x30f006cd0>;, score=0.426 total time= 9.7min\n",
      "[CV 2/3; 369/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a80b4f90>;, score=0.427 total time= 9.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 375/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x148a64ed0>\n",
      "[CV 1/3; 376/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 376/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.408 total time=  21.5s\n",
      "[CV 2/3; 376/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 376/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.408 total time=  21.3s\n",
      "[CV 3/3; 376/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 369/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1090a1c90>;, score=0.426 total time= 9.6min\n",
      "[CV 1/3; 377/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2d48a3dd0>\n",
      "[CV 1/3; 377/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2d48a3dd0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 377/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2f00cff90>\n",
      "[CV 2/3; 377/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2f00cff90>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 377/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2efad0ad0>\n",
      "[CV 3/3; 377/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2efad0ad0>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 376/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.408 total time=  23.1s\n",
      "[CV 1/3; 378/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x3104fa090>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 378/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x17ba97690>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 372/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d42fba50>;, score=0.431 total time= 9.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 378/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11a5a3b10>\n",
      "[CV 2/3; 372/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b64a3990>;, score=0.431 total time= 9.7min\n",
      "[CV 1/3; 379/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 372/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a825db50>;, score=0.431 total time= 9.7min\n",
      "[CV 2/3; 379/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 375/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c8abf10>;, score=0.408 total time= 9.7min\n",
      "[CV 3/3; 379/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 375/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10da61390>;, score=0.408 total time= 9.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 380/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10cb21e50>\n",
      "[CV 1/3; 380/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10cb21e50>;, score=nan total time=   0.1s\n",
      "[CV 2/3; 380/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c7c6350>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 380/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c7c6350>;, score=nan total time=   0.3s\n",
      "[CV 3/3; 380/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c34a390>\n",
      "[CV 3/3; 380/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c34a390>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 381/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c33f2d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 375/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x148a64ed0>;, score=0.408 total time= 9.9min\n",
      "[CV 2/3; 381/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d426e010>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 379/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.520 total time= 6.1min\n",
      "[CV 3/3; 381/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11a415110>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 378/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x3104fa090>;, score=0.408 total time= 9.8min\n",
      "[CV 2/3; 378/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x17ba97690>;, score=0.408 total time= 9.8min\n",
      "[CV 1/3; 382/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 382/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 379/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.523 total time= 6.7min\n",
      "[CV 3/3; 382/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 378/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11a5a3b10>;, score=0.408 total time=10.0min\n",
      "[CV 1/3; 383/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2b6996990>\n",
      "[CV 1/3; 383/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2b6996990>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 383/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2b6995010>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 383/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2b6995010>;, score=nan total time=   0.3s\n",
      "[CV 3/3; 383/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x29d7e0fd0>\n",
      "[CV 3/3; 383/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x29d7e0fd0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 384/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x29d724650>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 379/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.521 total time= 7.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 384/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1234d6f50>\n",
      "[CV 1/3; 382/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.520 total time= 6.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 384/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16efa4850>\n",
      "[CV 2/3; 382/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.522 total time= 6.9min\n",
      "[CV 1/3; 385/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 382/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.521 total time= 7.6min\n",
      "[CV 2/3; 385/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 385/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.494 total time= 7.7min\n",
      "[CV 3/3; 385/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 381/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c33f2d0>;, score=0.522 total time=16.9min\n",
      "[CV 1/3; 386/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15f921e50>\n",
      "[CV 1/3; 386/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15f921e50>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 386/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15b27be10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 386/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15b27be10>;, score=nan total time=   0.9s\n",
      "[CV 3/3; 386/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15a761050>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 386/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15a761050>;, score=nan total time=   1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 387/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x159d0ff10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 385/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.494 total time= 8.1min\n",
      "[CV 2/3; 387/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d0e869d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 381/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11a415110>;, score=0.521 total time=19.7min\n",
      "[CV 3/3; 387/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x12ad32110>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 381/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d426e010>;, score=0.522 total time=20.4min\n",
      "[CV 1/3; 388/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 384/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x29d724650>;, score=0.521 total time=17.0min\n",
      "[CV 2/3; 388/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 384/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1234d6f50>;, score=0.522 total time=17.7min\n",
      "[CV 3/3; 388/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 385/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.492 total time= 8.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 389/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c253990>\n",
      "[CV 1/3; 389/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c253990>;, score=nan total time=   0.1s\n",
      "[CV 2/3; 389/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c250950>\n",
      "[CV 2/3; 389/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c250950>;, score=nan total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 389/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c542850>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 389/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c542850>;, score=nan total time=   0.5s\n",
      "[CV 1/3; 390/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c546ad0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 384/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16efa4850>;, score=0.519 total time=17.8min\n",
      "[CV 2/3; 390/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15c72cf10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 388/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.492 total time= 8.9min\n",
      "[CV 3/3; 390/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1187687d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 388/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.492 total time= 8.9min\n",
      "[CV 1/3; 391/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 388/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.491 total time= 9.2min\n",
      "[CV 2/3; 391/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 387/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x159d0ff10>;, score=0.493 total time=18.3min\n",
      "[CV 3/3; 391/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 387/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d0e869d0>;, score=0.493 total time=20.4min\n",
      "[CV 1/3; 392/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x118a51ed0>\n",
      "[CV 1/3; 392/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x118a51ed0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 392/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x118a563d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 392/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x118a563d0>;, score=nan total time=   0.5s\n",
      "[CV 3/3; 392/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x11873cbd0>\n",
      "[CV 3/3; 392/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x11873cbd0>;, score=nan total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 393/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11872c6d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 387/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x12ad32110>;, score=0.493 total time=20.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 393/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11eeee350>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 391/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.501 total time=11.2min\n",
      "[CV 3/3; 393/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x285eebe10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 390/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c546ad0>;, score=0.493 total time=17.9min\n",
      "[CV 1/3; 394/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 391/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.502 total time=11.0min\n",
      "[CV 2/3; 394/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 390/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15c72cf10>;, score=0.492 total time=18.1min\n",
      "[CV 3/3; 394/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 391/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.502 total time=10.7min\n",
      "[CV 1/3; 395/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10edb7cd0>\n",
      "[CV 1/3; 395/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10edb7cd0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 395/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10ed1ff10>\n",
      "[CV 2/3; 395/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10ed1ff10>;, score=nan total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 395/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10985f7d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 395/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10985f7d0>;, score=nan total time=   0.3s\n",
      "[CV 1/3; 396/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10ed07f10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 390/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1187687d0>;, score=0.492 total time=17.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 396/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1181dc250>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 394/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.499 total time= 9.9min\n",
      "[CV 3/3; 396/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x17496ebd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 394/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.501 total time=10.0min\n",
      "[CV 1/3; 397/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 394/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.500 total time=10.1min\n",
      "[CV 2/3; 397/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 393/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11872c6d0>;, score=0.506 total time=21.5min\n",
      "[CV 3/3; 397/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 393/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11eeee350>;, score=0.505 total time=21.7min\n",
      "[CV 1/3; 398/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x11ecb6090>\n",
      "[CV 1/3; 398/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x11ecb6090>;, score=nan total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 398/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2a833a990>\n",
      "[CV 2/3; 398/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2a833a990>;, score=nan total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 398/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1284aa5d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 398/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1284aa5d0>;, score=nan total time=   0.5s\n",
      "[CV 1/3; 399/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a7fbd2d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 393/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x285eebe10>;, score=0.504 total time=21.8min\n",
      "[CV 2/3; 399/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x14a0f4210>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 397/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.485 total time=11.6min\n",
      "[CV 3/3; 399/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10d3e9590>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 397/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.486 total time=11.6min\n",
      "[CV 1/3; 400/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 396/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10ed07f10>;, score=0.504 total time=20.8min\n",
      "[CV 2/3; 400/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 396/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1181dc250>;, score=0.503 total time=21.0min\n",
      "[CV 3/3; 400/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 397/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.485 total time=10.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 401/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c399050>\n",
      "[CV 1/3; 401/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c399050>;, score=nan total time=   0.1s\n",
      "[CV 2/3; 401/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c398610>\n",
      "[CV 2/3; 401/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c398610>;, score=nan total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 401/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c0ea790>\n",
      "[CV 3/3; 401/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c0ea790>;, score=nan total time=   0.1s\n",
      "[CV 1/3; 402/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c0c9d90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 396/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x17496ebd0>;, score=0.503 total time=20.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 402/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x171f17f10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 400/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.483 total time=10.1min\n",
      "[CV 3/3; 402/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2aa8bea90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 400/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.484 total time=10.3min\n",
      "[CV 1/3; 403/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 400/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.483 total time=10.5min\n",
      "[CV 2/3; 403/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 399/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a7fbd2d0>;, score=0.490 total time=22.1min\n",
      "[CV 3/3; 403/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 399/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x14a0f4210>;, score=0.490 total time=22.2min\n",
      "[CV 1/3; 404/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16b72ff10>\n",
      "[CV 1/3; 404/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16b72ff10>;, score=nan total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 404/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x169c25ad0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 404/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x169c25ad0>;, score=nan total time=   0.7s\n",
      "[CV 3/3; 404/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16b1a6010>\n",
      "[CV 3/3; 404/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16b1a6010>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 405/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16b1b4c50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 399/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10d3e9590>;, score=0.491 total time=22.4min\n",
      "[CV 2/3; 405/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10cdcce10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 403/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.472 total time=11.6min\n",
      "[CV 3/3; 405/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15a6b8090>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 403/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.471 total time=11.5min\n",
      "[CV 1/3; 406/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 402/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c0c9d90>;, score=0.489 total time=21.3min\n",
      "[CV 2/3; 406/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 402/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x171f17f10>;, score=0.488 total time=21.4min\n",
      "[CV 3/3; 406/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 403/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.472 total time=10.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 407/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17ec14210>\n",
      "[CV 1/3; 407/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17ec14210>;, score=nan total time=   0.1s\n",
      "[CV 2/3; 407/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17ec37f10>\n",
      "[CV 2/3; 407/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17ec37f10>;, score=nan total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 407/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17e934a10>\n",
      "[CV 3/3; 407/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17e934a10>;, score=nan total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 408/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x17e929c10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 402/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2aa8bea90>;, score=0.489 total time=20.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 408/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2abf92b50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 406/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.470 total time= 9.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 408/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2916c2810>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 406/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.469 total time=10.2min\n",
      "[CV 1/3; 409/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 406/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.470 total time=10.3min\n",
      "[CV 2/3; 409/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 405/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16b1b4c50>;, score=0.475 total time=21.4min\n",
      "[CV 3/3; 409/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 405/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10cdcce10>;, score=0.475 total time=21.6min\n",
      "[CV 1/3; 410/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x123005e50>\n",
      "[CV 1/3; 410/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x123005e50>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 410/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d64bf10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 410/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d64bf10>;, score=nan total time=   0.7s\n",
      "[CV 3/3; 410/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x13304ab50>\n",
      "[CV 3/3; 410/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x13304ab50>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 411/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1226f1650>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 405/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15a6b8090>;, score=0.475 total time=21.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 411/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15dd19e10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 409/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.466 total time=11.2min\n",
      "[CV 3/3; 411/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d0082410>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 409/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.466 total time=11.1min\n",
      "[CV 1/3; 412/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 408/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x17e929c10>;, score=0.473 total time=20.9min\n",
      "[CV 2/3; 412/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 408/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2abf92b50>;, score=0.474 total time=20.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 412/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 409/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.466 total time=11.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 413/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c53aa10>\n",
      "[CV 1/3; 413/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c53aa10>;, score=nan total time=   0.1s\n",
      "[CV 2/3; 413/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c53ee50>\n",
      "[CV 2/3; 413/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c53ee50>;, score=nan total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 413/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c287f10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 413/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c287f10>;, score=nan total time=   0.4s\n",
      "[CV 1/3; 414/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c812490>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 408/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2916c2810>;, score=0.473 total time=20.5min\n",
      "[CV 2/3; 414/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1663af310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 412/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.465 total time=10.4min\n",
      "[CV 3/3; 414/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10b854b50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 412/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.464 total time=10.7min\n",
      "[CV 1/3; 415/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 412/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.465 total time=10.7min\n",
      "[CV 2/3; 415/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 415/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  20.2s\n",
      "[CV 3/3; 415/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 415/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  19.6s\n",
      "[CV 1/3; 416/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2aa450c90>\n",
      "[CV 1/3; 416/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2aa450c90>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 416/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x29cb97e90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 416/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x29cb97e90>;, score=nan total time=   0.2s\n",
      "[CV 3/3; 416/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2d030f610>\n",
      "[CV 3/3; 416/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2d030f610>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 415/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  21.5s\n",
      "[CV 1/3; 417/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b9f52010>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 417/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2aaf45150>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 411/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1226f1650>;, score=0.471 total time=22.4min\n",
      "[CV 3/3; 417/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1227e9290>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 411/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15dd19e10>;, score=0.471 total time=22.4min\n",
      "[CV 1/3; 418/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 418/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  19.2s\n",
      "[CV 2/3; 418/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 418/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  19.0s\n",
      "[CV 3/3; 418/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 418/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  18.5s\n",
      "[CV 1/3; 419/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10bfb1d10>\n",
      "[CV 1/3; 419/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10bfb1d10>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 419/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10bfb1010>\n",
      "[CV 2/3; 419/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10bfb1010>;, score=nan total time=   0.1s\n",
      "[CV 3/3; 419/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10bf57f10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 419/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10bf57f10>;, score=nan total time=   0.2s\n",
      "[CV 1/3; 420/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c3a4bd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 417/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b9f52010>;, score=nan total time= 7.2min\n",
      "[CV 2/3; 417/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2aaf45150>;, score=nan total time= 7.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 420/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1380abbd0>\n",
      "[CV 3/3; 420/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d54a3c10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 411/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d0082410>;, score=0.472 total time=22.0min\n",
      "[CV 1/3; 421/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 421/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  18.0s\n",
      "[CV 2/3; 421/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 421/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  18.0s\n",
      "[CV 3/3; 421/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 421/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  16.1s\n",
      "[CV 1/3; 422/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x119ab9210>\n",
      "[CV 1/3; 422/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x119ab9210>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 422/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x11990a5d0>\n",
      "[CV 2/3; 422/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x11990a5d0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 422/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x119909010>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 422/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x119909010>;, score=nan total time=   0.3s\n",
      "[CV 1/3; 423/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1199d7d50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 417/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1227e9290>;, score=nan total time= 7.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 423/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16e10bf10>\n",
      "[CV 1/3; 414/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c812490>;, score=0.469 total time=20.7min\n",
      "[CV 3/3; 423/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10bfa58d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 414/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1663af310>;, score=0.470 total time=20.1min\n",
      "[CV 1/3; 424/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 424/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  16.0s\n",
      "[CV 1/3; 420/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c3a4bd0>;, score=nan total time= 7.0min\n",
      "[CV 2/3; 424/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 424/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 424/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  16.6s\n",
      "[CV 3/3; 424/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  16.4s\n",
      "[CV 1/3; 425/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16dc3f8d0>\n",
      "[CV 1/3; 425/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16dc3f8d0>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 425/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c6816d0>\n",
      "[CV 2/3; 425/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c6816d0>;, score=nan total time=   0.0s\n",
      "[CV 3/3; 425/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2d528fbd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 425/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2d528fbd0>;, score=nan total time=   0.4s\n",
      "[CV 1/3; 426/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c667f10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 426/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2cda0b990>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 420/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1380abbd0>;, score=nan total time= 6.8min\n",
      "[CV 3/3; 420/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d54a3c10>;, score=nan total time= 6.8min\n",
      "[CV 3/3; 426/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x122ac7810>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 427/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 427/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  14.4s\n",
      "[CV 2/3; 427/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 427/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  14.6s\n",
      "[CV 3/3; 427/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 427/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  15.1s\n",
      "[CV 1/3; 428/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1202e53d0>\n",
      "[CV 1/3; 428/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1202e53d0>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 428/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1202e6050>\n",
      "[CV 2/3; 428/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1202e6050>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 428/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x12081d790>\n",
      "[CV 3/3; 428/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x12081d790>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 429/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x120826010>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 423/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1199d7d50>;, score=nan total time= 6.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 429/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x13a4d9650>\n",
      "[CV 3/3; 414/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10b854b50>;, score=0.470 total time=19.0min\n",
      "[CV 3/3; 429/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10be6db50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 423/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16e10bf10>;, score=nan total time= 6.8min\n",
      "[CV 1/3; 430/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 430/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  15.4s\n",
      "[CV 3/3; 423/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10bfa58d0>;, score=nan total time= 6.7min\n",
      "[CV 2/3; 430/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 430/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 430/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  15.5s\n",
      "[CV 3/3; 430/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  15.1s\n",
      "[CV 1/3; 431/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1221dde50>\n",
      "[CV 1/3; 431/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1221dde50>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 431/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x148f75350>\n",
      "[CV 2/3; 431/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x148f75350>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 431/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x122408e50>\n",
      "[CV 3/3; 431/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x122408e50>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 432/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10d29f050>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 432/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10cf27dd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 426/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c667f10>;, score=nan total time= 6.7min\n",
      "[CV 3/3; 432/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10b82e2d0>\n",
      "[CV 2/3; 426/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2cda0b990>;, score=nan total time= 6.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 426/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x122ac7810>;, score=nan total time= 6.6min\n",
      "[CV 1/3; 429/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x120826010>;, score=nan total time= 6.6min\n",
      "[CV 2/3; 429/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x13a4d9650>;, score=nan total time= 6.6min\n",
      "[CV 3/3; 429/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10be6db50>;, score=nan total time= 6.5min\n",
      "[CV 1/3; 432/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10d29f050>;, score=nan total time= 6.5min\n",
      "[CV 2/3; 432/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10cf27dd0>;, score=nan total time= 6.5min\n",
      "[CV 3/3; 432/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10b82e2d0>;, score=nan total time= 6.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "306 fits failed out of a total of 1296.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "198 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/naive_bayes.py\", line 776, in fit\n",
      "    self._count(X, Y)\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/naive_bayes.py\", line 898, in _count\n",
      "    check_non_negative(X, \"MultinomialNB (input X)\")\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1418, in check_non_negative\n",
      "    raise ValueError(\"Negative values in data passed to %s\" % whom)\n",
      "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "108 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/corpus/util.py\", line 84, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{zip_name}\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/data.py\", line 583, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mwordnet\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('wordnet')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/Users/tillgrutschus/nltk_data'\n",
      "    - '/opt/homebrew/anaconda3/envs/uu-data-mining/nltk_data'\n",
      "    - '/opt/homebrew/anaconda3/envs/uu-data-mining/share/nltk_data'\n",
      "    - '/opt/homebrew/anaconda3/envs/uu-data-mining/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/pipeline.py\", line 401, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/pipeline.py\", line 359, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py\", line 727, in fit_transform\n",
      "    result = self._fit_transform(X, y, _fit_transform_one)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py\", line 658, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/joblib/parallel.py\", line 1098, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/joblib/parallel.py\", line 975, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/multiprocessing/pool.py\", line 774, in get\n",
      "    raise self._value\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/pipeline.py\", line 445, in fit_transform\n",
      "    return last_step.fit_transform(Xt, y, **fit_params_last_step)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\", line 2133, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\", line 1388, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\", line 1275, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "                   ^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\", line 113, in _analyze\n",
      "    doc = tokenizer(doc)\n",
      "          ^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/_p/1z6zm9vn1h909hckkyp2mjhc0000gn/T/ipykernel_55720/2406683290.py\", line 41, in __call__\n",
      "  File \"/var/folders/_p/1z6zm9vn1h909hckkyp2mjhc0000gn/T/ipykernel_55720/2406683290.py\", line 42, in <listcomp>\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/stem/wordnet.py\", line 45, in lemmatize\n",
      "    lemmas = wn._morphy(word, pos)\n",
      "             ^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/corpus/util.py\", line 121, in __getattr__\n",
      "    self.__load()\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/corpus/util.py\", line 86, in __load\n",
      "    raise e\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/corpus/util.py\", line 81, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{self.__name}\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/data.py\", line 583, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mwordnet\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('wordnet')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/Users/tillgrutschus/nltk_data'\n",
      "    - '/opt/homebrew/anaconda3/envs/uu-data-mining/nltk_data'\n",
      "    - '/opt/homebrew/anaconda3/envs/uu-data-mining/share/nltk_data'\n",
      "    - '/opt/homebrew/anaconda3/envs/uu-data-mining/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.52108847 0.52109735 0.51151894 0.5115219  0.51523434 0.51508644\n",
      " 0.52116242 0.5211713  0.51183842 0.51183842 0.51537337 0.51533492\n",
      " 0.52143161 0.52142866 0.51240046 0.51240046 0.51599458 0.51604487\n",
      " 0.49269047 0.49268752 0.46821199 0.46821199 0.47829624 0.47829328\n",
      " 0.49270526 0.49271414 0.4687977  0.4687977  0.47831695 0.47835836\n",
      " 0.49329393 0.49329393 0.46926508 0.46926804 0.47891449 0.4789204\n",
      " 0.50078094 0.50075728 0.51119059 0.51119059 0.50661141 0.50656112\n",
      " 0.5008194  0.50081644 0.51169051 0.51169051 0.50673565 0.50679185\n",
      " 0.50166838 0.50166838 0.51204253 0.51204253 0.50706696 0.50705809\n",
      " 0.48389005 0.48389597 0.46794576 0.46794576 0.46742217 0.46748429\n",
      " 0.48411487 0.484106   0.4688894  0.46888644 0.46776531 0.46776236\n",
      " 0.48487511 0.4848899  0.46926804 0.46926804 0.46822087 0.46820016\n",
      " 0.47034184 0.47037142 0.5111758  0.5111758  0.50413546 0.50419758\n",
      " 0.47048975 0.47047496 0.51160177 0.51159289 0.50415025 0.504168\n",
      " 0.4714541  0.4714541  0.51212831 0.51213127 0.50408221 0.50408517\n",
      " 0.46504976 0.46507046 0.46641049 0.46865867 0.46476578 0.46493735\n",
      " 0.46525682 0.46525387 0.46903435 0.46873558 0.46501426 0.46479536\n",
      " 0.46582478 0.46582774 0.46945145 0.46989516 0.46512075 0.46449363\n",
      " 0.45008164 0.45008164 0.44450558 0.44450558 0.43562233 0.43562529\n",
      " 0.45017335 0.45017335 0.44483985 0.44483985 0.43564895 0.43564895\n",
      " 0.45079751 0.45079751 0.44678038 0.44678038 0.43629087 0.4363027\n",
      " 0.44772994 0.44772994 0.4444967  0.4444967  0.43565191 0.43564304\n",
      " 0.44757611 0.44757907 0.44483985 0.44483985 0.43573474 0.43577319\n",
      " 0.44709985 0.44710873 0.44673009 0.44673009 0.43644173 0.43643286\n",
      " 0.40787216 0.40787216 0.44474815 0.44474815 0.44009501 0.44009797\n",
      " 0.40787216 0.40787216 0.44505875 0.44505875 0.44025771 0.44024884\n",
      " 0.4078692  0.4078692  0.44710281 0.44710281 0.44114515 0.44116881\n",
      " 0.52107368 0.5210796  0.51152485 0.51152485 0.51528759 0.51513672\n",
      " 0.52122455 0.52124229 0.511868   0.511868   0.51535563 0.51534379\n",
      " 0.52144345 0.52144345 0.51240638 0.51240638 0.51599458 0.51605374\n",
      " 0.49270231 0.49270526 0.46820312 0.46821791 0.47828736 0.47832878\n",
      " 0.49277034 0.49277922 0.46883024 0.46881841 0.47834948 0.47836132\n",
      " 0.49332647 0.49334718 0.46927987 0.46927987 0.47892632 0.47893815\n",
      " 0.50078686 0.50080165 0.51115213 0.51119354 0.50664691 0.50655816\n",
      " 0.50081644 0.50081053 0.51172896 0.51169347 0.50679481 0.5067534\n",
      " 0.50161514 0.50160922 0.51206027 0.51207802 0.50712612 0.50704034\n",
      " 0.48392851 0.48399063 0.46818241 0.46817058 0.4674695  0.46761741\n",
      " 0.48413853 0.48413558 0.46903435 0.46902843 0.46782152 0.46777715\n",
      " 0.48490173 0.4848899  0.46934791 0.46940116 0.46822087 0.46828594\n",
      " 0.47057849 0.47043946 0.51127046 0.51094802 0.50373611 0.50379232\n",
      " 0.47062286 0.47059624 0.51162543 0.51181771 0.50407334 0.50404967\n",
      " 0.4714748  0.47147184 0.51131483 0.51193603 0.50401713 0.50419166\n",
      " 0.46524499 0.46534853 0.47578183 0.47550968 0.46941299 0.46729793\n",
      " 0.46533374 0.4654491  0.47631134 0.46949582 0.46888349 0.46655544\n",
      " 0.46602002 0.46606735 0.47927834 0.47416965 0.46586028 0.46985671\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.52143457        nan 0.52155881 0.52082224        nan 0.52068025\n",
      " 0.49329097        nan 0.49276147 0.49167288        nan 0.49203377\n",
      " 0.50166838        nan 0.50511756 0.50004141        nan 0.50338114\n",
      " 0.48487215        nan 0.49003408 0.48343154        nan 0.48875617\n",
      " 0.47145705        nan 0.47481157 0.4699632         nan 0.47344787\n",
      " 0.46582774        nan 0.4713476  0.46454392        nan 0.46972655\n",
      " 0.45079751        nan 0.45600085 0.45001952        nan 0.45544768\n",
      " 0.44709985        nan 0.42642552 0.44783939        nan 0.43104315\n",
      " 0.4078692         nan 0.4078692  0.40788991        nan 0.4078692\n",
      " 0.52144345        nan 0.52161502 0.52087549        nan 0.52071575\n",
      " 0.49332647        nan 0.49284134 0.49165513        nan 0.49210772\n",
      " 0.50161218        nan 0.50511164 0.50008579        nan 0.50336339\n",
      " 0.48489581        nan 0.49000745 0.48340492        nan 0.48876208\n",
      " 0.47169666        nan 0.47471987 0.4699632         nan 0.47344196\n",
      " 0.4659017         nan 0.47146593 0.46468886        nan 0.4698005\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Warning - Takes ages!\n",
    "search.fit(train_df, train_df[\"citation_bucket\"])\n",
    "save_model(search, \"models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = load_model(\"gridsearch\", \"models\")\n",
    "cv_results = pd.DataFrame(search.cv_results_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the results of the Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_model</th>\n",
       "      <th>param_model__C</th>\n",
       "      <th>param_model__class_weight</th>\n",
       "      <th>param_preprocessor__date__scaler</th>\n",
       "      <th>param_preprocessor__ratio__scaler</th>\n",
       "      <th>param_preprocessor__text__vectorizer__max_df</th>\n",
       "      <th>...</th>\n",
       "      <th>param_model__alpha</th>\n",
       "      <th>param_preprocessor__text__vectorizer__stop_words</th>\n",
       "      <th>param_preprocessor__text__vectorizer__tokenizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>402.458091</td>\n",
       "      <td>1.176289</td>\n",
       "      <td>194.640154</td>\n",
       "      <td>0.884686</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.StemmingTokenizer object at 0x2b87d8...</td>\n",
       "      <td>{'model': MultinomialNB(), 'model__alpha': 0.1...</td>\n",
       "      <td>0.457776</td>\n",
       "      <td>0.455096</td>\n",
       "      <td>0.455131</td>\n",
       "      <td>0.456001</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "362     402.458091      1.176289       194.640154        0.884686   \n",
       "\n",
       "         param_model param_model__C param_model__class_weight  \\\n",
       "362  MultinomialNB()            NaN                       NaN   \n",
       "\n",
       "    param_preprocessor__date__scaler param_preprocessor__ratio__scaler  \\\n",
       "362                   MinMaxScaler()                    MinMaxScaler()   \n",
       "\n",
       "    param_preprocessor__text__vectorizer__max_df  ... param_model__alpha  \\\n",
       "362                                          NaN  ...                0.1   \n",
       "\n",
       "    param_preprocessor__text__vectorizer__stop_words  \\\n",
       "362                                             None   \n",
       "\n",
       "       param_preprocessor__text__vectorizer__tokenizer  \\\n",
       "362  <__main__.StemmingTokenizer object at 0x2b87d8...   \n",
       "\n",
       "                                                params split0_test_score  \\\n",
       "362  {'model': MultinomialNB(), 'model__alpha': 0.1...          0.457776   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "362          0.455096           0.455131         0.456001        0.001255   \n",
       "\n",
       "     rank_test_score  \n",
       "362              265  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(cv_results[cv_results[\"param_model\"].astype(str).str.contains(\"MultinomialNB\")].sort_values(\"rank_test_score\").head(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> We use the LinearSVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_model</th>\n",
       "      <th>param_model__C</th>\n",
       "      <th>param_model__class_weight</th>\n",
       "      <th>param_preprocessor__date__scaler</th>\n",
       "      <th>param_preprocessor__ratio__scaler</th>\n",
       "      <th>param_preprocessor__text__vectorizer__max_df</th>\n",
       "      <th>...</th>\n",
       "      <th>param_model__alpha</th>\n",
       "      <th>param_preprocessor__text__vectorizer__stop_words</th>\n",
       "      <th>param_preprocessor__text__vectorizer__tokenizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38.837949</td>\n",
       "      <td>1.054323</td>\n",
       "      <td>9.715375</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.519879</td>\n",
       "      <td>0.522301</td>\n",
       "      <td>0.521086</td>\n",
       "      <td>0.521088</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.734405</td>\n",
       "      <td>0.121402</td>\n",
       "      <td>9.690159</td>\n",
       "      <td>0.122804</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.519879</td>\n",
       "      <td>0.522328</td>\n",
       "      <td>0.521086</td>\n",
       "      <td>0.521097</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.384088</td>\n",
       "      <td>4.302849</td>\n",
       "      <td>8.487338</td>\n",
       "      <td>0.322813</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.511874</td>\n",
       "      <td>0.511528</td>\n",
       "      <td>0.511155</td>\n",
       "      <td>0.511519</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.294612</td>\n",
       "      <td>0.266356</td>\n",
       "      <td>8.703293</td>\n",
       "      <td>0.170315</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.511883</td>\n",
       "      <td>0.511528</td>\n",
       "      <td>0.511155</td>\n",
       "      <td>0.511522</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.202874</td>\n",
       "      <td>0.178631</td>\n",
       "      <td>8.765668</td>\n",
       "      <td>0.313946</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.514714</td>\n",
       "      <td>0.516063</td>\n",
       "      <td>0.514927</td>\n",
       "      <td>0.515234</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.308460</td>\n",
       "      <td>0.251459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.LemmaTokenizer object at 0x1664f9210&gt;</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1113.252318</td>\n",
       "      <td>7.008182</td>\n",
       "      <td>223.910202</td>\n",
       "      <td>6.115537</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.StemmingTokenizer object at 0x2b87d8...</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>0.470812</td>\n",
       "      <td>0.471194</td>\n",
       "      <td>0.472392</td>\n",
       "      <td>0.471466</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>626.791677</td>\n",
       "      <td>8.300176</td>\n",
       "      <td>10.630937</td>\n",
       "      <td>0.491366</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[doing, shouldn, you'd, do, yourself, before, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>0.465159</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>0.464991</td>\n",
       "      <td>0.464689</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>0.374433</td>\n",
       "      <td>0.191894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[doing, shouldn, you'd, do, yourself, before, ...</td>\n",
       "      <td>&lt;__main__.LemmaTokenizer object at 0x1664f9210&gt;</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>983.944981</td>\n",
       "      <td>38.647208</td>\n",
       "      <td>212.710723</td>\n",
       "      <td>4.590152</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[doing, shouldn, you'd, do, yourself, before, ...</td>\n",
       "      <td>&lt;__main__.StemmingTokenizer object at 0x2b87d8...</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>0.469366</td>\n",
       "      <td>0.469774</td>\n",
       "      <td>0.470262</td>\n",
       "      <td>0.469801</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        38.837949      1.054323         9.715375        0.103300   \n",
       "1        39.734405      0.121402         9.690159        0.122804   \n",
       "2        32.384088      4.302849         8.487338        0.322813   \n",
       "3        27.294612      0.266356         8.703293        0.170315   \n",
       "4        29.202874      0.178631         8.765668        0.313946   \n",
       "..             ...           ...              ...             ...   \n",
       "409       0.308460      0.251459         0.000000        0.000000   \n",
       "410    1113.252318      7.008182       223.910202        6.115537   \n",
       "411     626.791677      8.300176        10.630937        0.491366   \n",
       "412       0.374433      0.191894         0.000000        0.000000   \n",
       "413     983.944981     38.647208       212.710723        4.590152   \n",
       "\n",
       "          param_model param_model__C param_model__class_weight  \\\n",
       "0    LinearSVC(C=0.1)            0.1                      None   \n",
       "1    LinearSVC(C=0.1)            0.1                      None   \n",
       "2    LinearSVC(C=0.1)            0.1                      None   \n",
       "3    LinearSVC(C=0.1)            0.1                      None   \n",
       "4    LinearSVC(C=0.1)            0.1                      None   \n",
       "..                ...            ...                       ...   \n",
       "409  LinearSVC(C=0.1)           10.0                  balanced   \n",
       "410  LinearSVC(C=0.1)           10.0                  balanced   \n",
       "411  LinearSVC(C=0.1)           10.0                  balanced   \n",
       "412  LinearSVC(C=0.1)           10.0                  balanced   \n",
       "413  LinearSVC(C=0.1)           10.0                  balanced   \n",
       "\n",
       "    param_preprocessor__date__scaler param_preprocessor__ratio__scaler  \\\n",
       "0                     MinMaxScaler()                    MinMaxScaler()   \n",
       "1                     MinMaxScaler()                    MinMaxScaler()   \n",
       "2                     MinMaxScaler()                    MinMaxScaler()   \n",
       "3                     MinMaxScaler()                    MinMaxScaler()   \n",
       "4                     MinMaxScaler()                    MinMaxScaler()   \n",
       "..                               ...                               ...   \n",
       "409                 StandardScaler()                  StandardScaler()   \n",
       "410                 StandardScaler()                  StandardScaler()   \n",
       "411                 StandardScaler()                  StandardScaler()   \n",
       "412                 StandardScaler()                  StandardScaler()   \n",
       "413                 StandardScaler()                  StandardScaler()   \n",
       "\n",
       "    param_preprocessor__text__vectorizer__max_df  ... param_model__alpha  \\\n",
       "0                                            0.5  ...                NaN   \n",
       "1                                            0.5  ...                NaN   \n",
       "2                                            0.5  ...                NaN   \n",
       "3                                            0.5  ...                NaN   \n",
       "4                                            0.5  ...                NaN   \n",
       "..                                           ...  ...                ...   \n",
       "409                                          NaN  ...                NaN   \n",
       "410                                          NaN  ...                NaN   \n",
       "411                                          NaN  ...                NaN   \n",
       "412                                          NaN  ...                NaN   \n",
       "413                                          NaN  ...                NaN   \n",
       "\n",
       "      param_preprocessor__text__vectorizer__stop_words  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "409                                               None   \n",
       "410                                               None   \n",
       "411  [doing, shouldn, you'd, do, yourself, before, ...   \n",
       "412  [doing, shouldn, you'd, do, yourself, before, ...   \n",
       "413  [doing, shouldn, you'd, do, yourself, before, ...   \n",
       "\n",
       "       param_preprocessor__text__vectorizer__tokenizer  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "409    <__main__.LemmaTokenizer object at 0x1664f9210>   \n",
       "410  <__main__.StemmingTokenizer object at 0x2b87d8...   \n",
       "411                                               None   \n",
       "412    <__main__.LemmaTokenizer object at 0x1664f9210>   \n",
       "413  <__main__.StemmingTokenizer object at 0x2b87d8...   \n",
       "\n",
       "                                                params split0_test_score  \\\n",
       "0    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.519879   \n",
       "1    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.519879   \n",
       "2    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.511874   \n",
       "3    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.511883   \n",
       "4    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.514714   \n",
       "..                                                 ...               ...   \n",
       "409  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...               NaN   \n",
       "410  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...          0.470812   \n",
       "411  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...          0.465159   \n",
       "412  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...               NaN   \n",
       "413  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...          0.469366   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0            0.522301           0.521086         0.521088        0.000989   \n",
       "1            0.522328           0.521086         0.521097        0.001000   \n",
       "2            0.511528           0.511155         0.511519        0.000294   \n",
       "3            0.511528           0.511155         0.511522        0.000297   \n",
       "4            0.516063           0.514927         0.515234        0.000592   \n",
       "..                ...                ...              ...             ...   \n",
       "409               NaN                NaN              NaN             NaN   \n",
       "410          0.471194           0.472392         0.471466        0.000673   \n",
       "411          0.463917           0.464991         0.464689        0.000550   \n",
       "412               NaN                NaN              NaN             NaN   \n",
       "413          0.469774           0.470262         0.469801        0.000366   \n",
       "\n",
       "     rank_test_score  \n",
       "0                 14  \n",
       "1                 13  \n",
       "2                 59  \n",
       "3                 58  \n",
       "4                 30  \n",
       "..               ...  \n",
       "409              331  \n",
       "410              177  \n",
       "411              262  \n",
       "412              331  \n",
       "413              194  \n",
       "\n",
       "[288 rows x 22 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = cv_results[cv_results[\"param_model\"].astype(str).str.contains(\"LinearSVC\")].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models have not converged. We drop them from consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_model</th>\n",
       "      <th>param_model__C</th>\n",
       "      <th>param_model__class_weight</th>\n",
       "      <th>param_preprocessor__date__scaler</th>\n",
       "      <th>param_preprocessor__ratio__scaler</th>\n",
       "      <th>param_preprocessor__text__vectorizer__max_df</th>\n",
       "      <th>...</th>\n",
       "      <th>param_model__alpha</th>\n",
       "      <th>param_preprocessor__text__vectorizer__stop_words</th>\n",
       "      <th>param_preprocessor__text__vectorizer__tokenizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38.837949</td>\n",
       "      <td>1.054323</td>\n",
       "      <td>9.715375</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.519879</td>\n",
       "      <td>0.522301</td>\n",
       "      <td>0.521086</td>\n",
       "      <td>0.521088</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.734405</td>\n",
       "      <td>0.121402</td>\n",
       "      <td>9.690159</td>\n",
       "      <td>0.122804</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.519879</td>\n",
       "      <td>0.522328</td>\n",
       "      <td>0.521086</td>\n",
       "      <td>0.521097</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.384088</td>\n",
       "      <td>4.302849</td>\n",
       "      <td>8.487338</td>\n",
       "      <td>0.322813</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.511874</td>\n",
       "      <td>0.511528</td>\n",
       "      <td>0.511155</td>\n",
       "      <td>0.511519</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.294612</td>\n",
       "      <td>0.266356</td>\n",
       "      <td>8.703293</td>\n",
       "      <td>0.170315</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.511883</td>\n",
       "      <td>0.511528</td>\n",
       "      <td>0.511155</td>\n",
       "      <td>0.511522</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.202874</td>\n",
       "      <td>0.178631</td>\n",
       "      <td>8.765668</td>\n",
       "      <td>0.313946</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.514714</td>\n",
       "      <td>0.516063</td>\n",
       "      <td>0.514927</td>\n",
       "      <td>0.515234</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1015.551673</td>\n",
       "      <td>9.235389</td>\n",
       "      <td>224.987650</td>\n",
       "      <td>3.563917</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>None</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[doing, shouldn, you'd, do, yourself, before, ...</td>\n",
       "      <td>&lt;__main__.StemmingTokenizer object at 0x2b87d8...</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>0.473155</td>\n",
       "      <td>0.474371</td>\n",
       "      <td>0.472800</td>\n",
       "      <td>0.473442</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>656.491602</td>\n",
       "      <td>5.376175</td>\n",
       "      <td>10.435120</td>\n",
       "      <td>0.922083</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>0.465701</td>\n",
       "      <td>0.466073</td>\n",
       "      <td>0.465931</td>\n",
       "      <td>0.465902</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1113.252318</td>\n",
       "      <td>7.008182</td>\n",
       "      <td>223.910202</td>\n",
       "      <td>6.115537</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.StemmingTokenizer object at 0x2b87d8...</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>0.470812</td>\n",
       "      <td>0.471194</td>\n",
       "      <td>0.472392</td>\n",
       "      <td>0.471466</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>626.791677</td>\n",
       "      <td>8.300176</td>\n",
       "      <td>10.630937</td>\n",
       "      <td>0.491366</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[doing, shouldn, you'd, do, yourself, before, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>0.465159</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>0.464991</td>\n",
       "      <td>0.464689</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>983.944981</td>\n",
       "      <td>38.647208</td>\n",
       "      <td>212.710723</td>\n",
       "      <td>4.590152</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[doing, shouldn, you'd, do, yourself, before, ...</td>\n",
       "      <td>&lt;__main__.StemmingTokenizer object at 0x2b87d8...</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>0.469366</td>\n",
       "      <td>0.469774</td>\n",
       "      <td>0.470262</td>\n",
       "      <td>0.469801</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        38.837949      1.054323         9.715375        0.103300   \n",
       "1        39.734405      0.121402         9.690159        0.122804   \n",
       "2        32.384088      4.302849         8.487338        0.322813   \n",
       "3        27.294612      0.266356         8.703293        0.170315   \n",
       "4        29.202874      0.178631         8.765668        0.313946   \n",
       "..             ...           ...              ...             ...   \n",
       "407    1015.551673      9.235389       224.987650        3.563917   \n",
       "408     656.491602      5.376175        10.435120        0.922083   \n",
       "410    1113.252318      7.008182       223.910202        6.115537   \n",
       "411     626.791677      8.300176        10.630937        0.491366   \n",
       "413     983.944981     38.647208       212.710723        4.590152   \n",
       "\n",
       "          param_model param_model__C param_model__class_weight  \\\n",
       "0    LinearSVC(C=0.1)            0.1                      None   \n",
       "1    LinearSVC(C=0.1)            0.1                      None   \n",
       "2    LinearSVC(C=0.1)            0.1                      None   \n",
       "3    LinearSVC(C=0.1)            0.1                      None   \n",
       "4    LinearSVC(C=0.1)            0.1                      None   \n",
       "..                ...            ...                       ...   \n",
       "407  LinearSVC(C=0.1)           10.0                      None   \n",
       "408  LinearSVC(C=0.1)           10.0                  balanced   \n",
       "410  LinearSVC(C=0.1)           10.0                  balanced   \n",
       "411  LinearSVC(C=0.1)           10.0                  balanced   \n",
       "413  LinearSVC(C=0.1)           10.0                  balanced   \n",
       "\n",
       "    param_preprocessor__date__scaler param_preprocessor__ratio__scaler  \\\n",
       "0                     MinMaxScaler()                    MinMaxScaler()   \n",
       "1                     MinMaxScaler()                    MinMaxScaler()   \n",
       "2                     MinMaxScaler()                    MinMaxScaler()   \n",
       "3                     MinMaxScaler()                    MinMaxScaler()   \n",
       "4                     MinMaxScaler()                    MinMaxScaler()   \n",
       "..                               ...                               ...   \n",
       "407                 StandardScaler()                  StandardScaler()   \n",
       "408                 StandardScaler()                  StandardScaler()   \n",
       "410                 StandardScaler()                  StandardScaler()   \n",
       "411                 StandardScaler()                  StandardScaler()   \n",
       "413                 StandardScaler()                  StandardScaler()   \n",
       "\n",
       "    param_preprocessor__text__vectorizer__max_df  ... param_model__alpha  \\\n",
       "0                                            0.5  ...                NaN   \n",
       "1                                            0.5  ...                NaN   \n",
       "2                                            0.5  ...                NaN   \n",
       "3                                            0.5  ...                NaN   \n",
       "4                                            0.5  ...                NaN   \n",
       "..                                           ...  ...                ...   \n",
       "407                                          NaN  ...                NaN   \n",
       "408                                          NaN  ...                NaN   \n",
       "410                                          NaN  ...                NaN   \n",
       "411                                          NaN  ...                NaN   \n",
       "413                                          NaN  ...                NaN   \n",
       "\n",
       "      param_preprocessor__text__vectorizer__stop_words  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "407  [doing, shouldn, you'd, do, yourself, before, ...   \n",
       "408                                               None   \n",
       "410                                               None   \n",
       "411  [doing, shouldn, you'd, do, yourself, before, ...   \n",
       "413  [doing, shouldn, you'd, do, yourself, before, ...   \n",
       "\n",
       "       param_preprocessor__text__vectorizer__tokenizer  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "407  <__main__.StemmingTokenizer object at 0x2b87d8...   \n",
       "408                                               None   \n",
       "410  <__main__.StemmingTokenizer object at 0x2b87d8...   \n",
       "411                                               None   \n",
       "413  <__main__.StemmingTokenizer object at 0x2b87d8...   \n",
       "\n",
       "                                                params split0_test_score  \\\n",
       "0    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.519879   \n",
       "1    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.519879   \n",
       "2    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.511874   \n",
       "3    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.511883   \n",
       "4    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.514714   \n",
       "..                                                 ...               ...   \n",
       "407  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...          0.473155   \n",
       "408  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...          0.465701   \n",
       "410  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...          0.470812   \n",
       "411  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...          0.465159   \n",
       "413  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...          0.469366   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0            0.522301           0.521086         0.521088        0.000989   \n",
       "1            0.522328           0.521086         0.521097        0.001000   \n",
       "2            0.511528           0.511155         0.511519        0.000294   \n",
       "3            0.511528           0.511155         0.511522        0.000297   \n",
       "4            0.516063           0.514927         0.515234        0.000592   \n",
       "..                ...                ...              ...             ...   \n",
       "407          0.474371           0.472800         0.473442        0.000673   \n",
       "408          0.466073           0.465931         0.465902        0.000154   \n",
       "410          0.471194           0.472392         0.471466        0.000673   \n",
       "411          0.463917           0.464991         0.464689        0.000550   \n",
       "413          0.469774           0.470262         0.469801        0.000366   \n",
       "\n",
       "     rank_test_score  \n",
       "0                 14  \n",
       "1                 13  \n",
       "2                 59  \n",
       "3                 58  \n",
       "4                 30  \n",
       "..               ...  \n",
       "407              173  \n",
       "408              244  \n",
       "410              177  \n",
       "411              262  \n",
       "413              194  \n",
       "\n",
       "[264 rows x 22 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = cv_results[cv_results[\"mean_test_score\"].notna()].copy()\n",
    "cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_model</th>\n",
       "      <th>param_model__C</th>\n",
       "      <th>param_model__class_weight</th>\n",
       "      <th>param_preprocessor__date__scaler</th>\n",
       "      <th>param_preprocessor__ratio__scaler</th>\n",
       "      <th>param_preprocessor__text__vectorizer__max_df</th>\n",
       "      <th>param_preprocessor__text__vectorizer__max_features</th>\n",
       "      <th>param_preprocessor__text__vectorizer__strip_accents</th>\n",
       "      <th>param_model__alpha</th>\n",
       "      <th>param_preprocessor__text__vectorizer__stop_words</th>\n",
       "      <th>param_preprocessor__text__vectorizer__tokenizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.StemmingTokenizer object at 0x2b87d8...</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.521591</td>\n",
       "      <td>0.522346</td>\n",
       "      <td>0.520908</td>\n",
       "      <td>0.521615</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.StemmingTokenizer object at 0x2b87d8...</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.521511</td>\n",
       "      <td>0.522363</td>\n",
       "      <td>0.520802</td>\n",
       "      <td>0.521559</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.520367</td>\n",
       "      <td>0.522523</td>\n",
       "      <td>0.521440</td>\n",
       "      <td>0.521443</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.520367</td>\n",
       "      <td>0.522523</td>\n",
       "      <td>0.521440</td>\n",
       "      <td>0.521443</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>unicode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.520358</td>\n",
       "      <td>0.522514</td>\n",
       "      <td>0.521458</td>\n",
       "      <td>0.521443</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.520322</td>\n",
       "      <td>0.522559</td>\n",
       "      <td>0.521423</td>\n",
       "      <td>0.521435</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.520322</td>\n",
       "      <td>0.522559</td>\n",
       "      <td>0.521414</td>\n",
       "      <td>0.521432</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>unicode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.520313</td>\n",
       "      <td>0.522550</td>\n",
       "      <td>0.521423</td>\n",
       "      <td>0.521429</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>0.75</td>\n",
       "      <td>None</td>\n",
       "      <td>unicode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.519719</td>\n",
       "      <td>0.522843</td>\n",
       "      <td>0.521165</td>\n",
       "      <td>0.521242</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>0.75</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.519710</td>\n",
       "      <td>0.522843</td>\n",
       "      <td>0.521121</td>\n",
       "      <td>0.521225</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          param_model param_model__C param_model__class_weight  \\\n",
       "380  LinearSVC(C=0.1)            0.1                      None   \n",
       "326  LinearSVC(C=0.1)            0.1                      None   \n",
       "378  LinearSVC(C=0.1)            0.1                      None   \n",
       "174  LinearSVC(C=0.1)            0.1                      None   \n",
       "175  LinearSVC(C=0.1)            0.1                      None   \n",
       "324  LinearSVC(C=0.1)            0.1                      None   \n",
       "12   LinearSVC(C=0.1)            0.1                      None   \n",
       "13   LinearSVC(C=0.1)            0.1                      None   \n",
       "169  LinearSVC(C=0.1)            0.1                      None   \n",
       "168  LinearSVC(C=0.1)            0.1                      None   \n",
       "\n",
       "    param_preprocessor__date__scaler param_preprocessor__ratio__scaler  \\\n",
       "380                 StandardScaler()                  StandardScaler()   \n",
       "326                   MinMaxScaler()                    MinMaxScaler()   \n",
       "378                 StandardScaler()                  StandardScaler()   \n",
       "174                 StandardScaler()                  StandardScaler()   \n",
       "175                 StandardScaler()                  StandardScaler()   \n",
       "324                   MinMaxScaler()                    MinMaxScaler()   \n",
       "12                    MinMaxScaler()                    MinMaxScaler()   \n",
       "13                    MinMaxScaler()                    MinMaxScaler()   \n",
       "169                 StandardScaler()                  StandardScaler()   \n",
       "168                 StandardScaler()                  StandardScaler()   \n",
       "\n",
       "    param_preprocessor__text__vectorizer__max_df  \\\n",
       "380                                          NaN   \n",
       "326                                          NaN   \n",
       "378                                          NaN   \n",
       "174                                          1.0   \n",
       "175                                          1.0   \n",
       "324                                          NaN   \n",
       "12                                           1.0   \n",
       "13                                           1.0   \n",
       "169                                         0.75   \n",
       "168                                         0.75   \n",
       "\n",
       "    param_preprocessor__text__vectorizer__max_features  \\\n",
       "380                                                NaN   \n",
       "326                                                NaN   \n",
       "378                                                NaN   \n",
       "174                                               None   \n",
       "175                                               None   \n",
       "324                                                NaN   \n",
       "12                                                None   \n",
       "13                                                None   \n",
       "169                                               None   \n",
       "168                                               None   \n",
       "\n",
       "    param_preprocessor__text__vectorizer__strip_accents param_model__alpha  \\\n",
       "380                                                NaN                 NaN   \n",
       "326                                                NaN                 NaN   \n",
       "378                                                NaN                 NaN   \n",
       "174                                               None                 NaN   \n",
       "175                                            unicode                 NaN   \n",
       "324                                                NaN                 NaN   \n",
       "12                                                None                 NaN   \n",
       "13                                             unicode                 NaN   \n",
       "169                                            unicode                 NaN   \n",
       "168                                               None                 NaN   \n",
       "\n",
       "    param_preprocessor__text__vectorizer__stop_words  \\\n",
       "380                                             None   \n",
       "326                                             None   \n",
       "378                                             None   \n",
       "174                                              NaN   \n",
       "175                                              NaN   \n",
       "324                                             None   \n",
       "12                                               NaN   \n",
       "13                                               NaN   \n",
       "169                                              NaN   \n",
       "168                                              NaN   \n",
       "\n",
       "       param_preprocessor__text__vectorizer__tokenizer  \\\n",
       "380  <__main__.StemmingTokenizer object at 0x2b87d8...   \n",
       "326  <__main__.StemmingTokenizer object at 0x2b87d8...   \n",
       "378                                               None   \n",
       "174                                                NaN   \n",
       "175                                                NaN   \n",
       "324                                               None   \n",
       "12                                                 NaN   \n",
       "13                                                 NaN   \n",
       "169                                                NaN   \n",
       "168                                                NaN   \n",
       "\n",
       "                                                params  split0_test_score  \\\n",
       "380  {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.521591   \n",
       "326  {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.521511   \n",
       "378  {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.520367   \n",
       "174  {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.520367   \n",
       "175  {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.520358   \n",
       "324  {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.520322   \n",
       "12   {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.520322   \n",
       "13   {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.520313   \n",
       "169  {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.519719   \n",
       "168  {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.519710   \n",
       "\n",
       "     split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "380           0.522346           0.520908         0.521615        0.000587   \n",
       "326           0.522363           0.520802         0.521559        0.000639   \n",
       "378           0.522523           0.521440         0.521443        0.000880   \n",
       "174           0.522523           0.521440         0.521443        0.000880   \n",
       "175           0.522514           0.521458         0.521443        0.000880   \n",
       "324           0.522559           0.521423         0.521435        0.000913   \n",
       "12            0.522559           0.521414         0.521432        0.000913   \n",
       "13            0.522550           0.521423         0.521429        0.000913   \n",
       "169           0.522843           0.521165         0.521242        0.001276   \n",
       "168           0.522843           0.521121         0.521225        0.001281   \n",
       "\n",
       "     rank_test_score  \n",
       "380                1  \n",
       "326                2  \n",
       "378                3  \n",
       "174                3  \n",
       "175                5  \n",
       "324                6  \n",
       "12                 7  \n",
       "13                 8  \n",
       "169                9  \n",
       "168               10  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the 10 best models\n",
    "cv_results.sort_values(\"rank_test_score\").head(10).drop([\"mean_fit_time\", \"std_fit_time\", \"mean_score_time\", \"std_score_time\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- The best model is the LinearSVC model with the TF-IDF Vectorizer and the following parameters:\n",
    "    - C = 0.1\n",
    "    - No class weights\n",
    "    - MinMaxScaler vs. StandardScaler does not matter\n",
    "    - max_df should be 1.0\n",
    "    - stopword removal does not help much\n",
    "    - strip_accents does matter much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about Kernels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Size of train set: 304246'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Size of validation set: 33806'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;kernel&#x27;,\n",
       "                 RBFSampler(gamma=0.1, n_components=1000, random_state=42)),\n",
       "                (&#x27;model&#x27;, LinearSVC(C=0.01))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-79\" type=\"checkbox\" ><label for=\"sk-estimator-id-79\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;kernel&#x27;,\n",
       "                 RBFSampler(gamma=0.1, n_components=1000, random_state=42)),\n",
       "                (&#x27;model&#x27;, LinearSVC(C=0.01))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-80\" type=\"checkbox\" ><label for=\"sk-estimator-id-80\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(n_jobs=-1,\n",
       "                  transformers=[(&#x27;ratio&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                  &#x27;author_count&#x27;]),\n",
       "                                (&#x27;date&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                (&#x27;text&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                  TfidfVectorizer())]),\n",
       "                                 &#x27;text&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-81\" type=\"checkbox\" ><label for=\"sk-estimator-id-81\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ratio</label><div class=\"sk-toggleable__content\"><pre>[&#x27;page_count&#x27;, &#x27;figure_count&#x27;, &#x27;author_count&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-82\" type=\"checkbox\" ><label for=\"sk-estimator-id-82\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-83\" type=\"checkbox\" ><label for=\"sk-estimator-id-83\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">date</label><div class=\"sk-toggleable__content\"><pre>[&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-84\" type=\"checkbox\" ><label for=\"sk-estimator-id-84\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-85\" type=\"checkbox\" ><label for=\"sk-estimator-id-85\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">text</label><div class=\"sk-toggleable__content\"><pre>text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-86\" type=\"checkbox\" ><label for=\"sk-estimator-id-86\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-87\" type=\"checkbox\" ><label for=\"sk-estimator-id-87\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RBFSampler</label><div class=\"sk-toggleable__content\"><pre>RBFSampler(gamma=0.1, n_components=1000, random_state=42)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-88\" type=\"checkbox\" ><label for=\"sk-estimator-id-88\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=0.01)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[('ratio',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['page_count', 'figure_count',\n",
       "                                                   'author_count']),\n",
       "                                                 ('date',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['year', 'month', 'day']),\n",
       "                                                 ('text',\n",
       "                                                  Pipeline(steps=[('vectorizer',\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  'text')])),\n",
       "                ('kernel',\n",
       "                 RBFSampler(gamma=0.1, n_components=1000, random_state=42)),\n",
       "                ('model', LinearSVC(C=0.01))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_pipeline = deepcopy(pipeline)\n",
    "kernel_pipeline.steps.insert(1, (\"kernel\", RBFSampler(gamma=0.1, random_state=42, n_components=1000)))\n",
    "kernel_pipeline.steps[-1] = (\"model\", LinearSVC(C=0.01))\n",
    "\n",
    "train_df_trunc, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"citation_bucket\"],\n",
    ")\n",
    "display(f\"Size of train set: {len(train_df_trunc)}\")\n",
    "display(f\"Size of validation set: {len(val_df)}\")\n",
    "kernel_pipeline.fit(train_df_trunc, train_df_trunc[\"citation_bucket\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train score: 0.5014396245143732'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Validation score: 0.49952671123469206'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f\"Train score: {kernel_pipeline.score(train_df_trunc, train_df_trunc['citation_bucket'])}\")\n",
    "display(f\"Validation score: {kernel_pipeline.score(val_df, val_df['citation_bucket'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Grid Search!\n",
    "\n",
    "We observe relative poor performance in the previous grid search.\n",
    "Since we only tried using a Linear model, we will try using a non-linear model (SVM with RBF kernel) to see if we can get better results.\n",
    "Regarding the preprocessing, we realize that the preprocessing operations have limited influence. \n",
    "Additionally, the last GridSearch took over 14 hours to run. We will try to reduce the number of parameters to search over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# First we construct a new pipeline and apply what we've learned\n",
    "\n",
    "data_prep_pipeline = pipeline.named_steps[\"preprocessor\"]\n",
    "# Transform the training data to save some resources!\n",
    "train_df_transformed = data_prep_pipeline.fit_transform(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df_transformed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit und Beruf/Uppsala/Data Mining/uu-data-mining-project/exploration_modeling.ipynb Cell 34\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m kernel_pipeline \u001b[39m=\u001b[39m Pipeline([\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mkernel\u001b[39m\u001b[39m\"\u001b[39m, RBFSampler(gamma\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, n_components\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, LinearSVC())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m ])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m display(train_df_transformed\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m display(kernel_pipeline)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m param_distributions \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mkernel__gamma\u001b[39m\u001b[39m\"\u001b[39m: uniform(\u001b[39m0.01\u001b[39m, \u001b[39m10\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mkernel__n_components\u001b[39m\u001b[39m\"\u001b[39m: randint(\u001b[39m100\u001b[39m, \u001b[39m1000\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmodel__C\u001b[39m\u001b[39m\"\u001b[39m: uniform(\u001b[39m0.01\u001b[39m, \u001b[39m10\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df_transformed' is not defined"
     ]
    }
   ],
   "source": [
    "kernel_pipeline = Pipeline([\n",
    "    (\"kernel\", RBFSampler(gamma=10, n_components=1000)),\n",
    "    (\"model\", LinearSVC())\n",
    "])\n",
    "\n",
    "display(train_df_transformed.shape)\n",
    "display(kernel_pipeline)\n",
    "\n",
    "param_distributions = {\n",
    "    \"kernel__gamma\": uniform(0.01, 10),\n",
    "    \"kernel__n_components\": randint(100, 1000),\n",
    "    \"model__C\": uniform(0.01, 10)\n",
    "}\n",
    "\n",
    "param_grid_kernel = {\n",
    "    \"kernel__gamma\": [0.01, 0.1, 1.0, 10.0],\n",
    "    \"kernel__n_components\": [100, 250, 1000],\n",
    "    \"model__C\": [0.01, 0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "# kernel_search = HalvingRandomSearchCV(kernel_pipeline, param_distributions=param_distributions, cv=3, verbose=10, n_jobs=2)\n",
    "kernel_search = HalvingGridSearchCV(kernel_pipeline, param_grid=param_grid_kernel, cv=3, verbose=10, n_jobs=6)\n",
    "kernel_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 4\n",
      "n_required_iterations: 4\n",
      "n_possible_iterations: 4\n",
      "min_resources_: 12520\n",
      "max_resources_: 338052\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 48\n",
      "n_resources: 12520\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "[CV 1/3; 1/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=0.01\n",
      "[CV 2/3; 1/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=0.01\n",
      "[CV 3/3; 1/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=0.01\n",
      "[CV 1/3; 2/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=0.1.\n",
      "[CV 2/3; 2/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=0.1.\n",
      "[CV 3/3; 2/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=0.1.\n",
      "[CV 1/3; 1/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=0.01;, score=(train=0.418, test=0.407) total time=   0.9s\n",
      "[CV 1/3; 3/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=1.0.\n",
      "[CV 2/3; 1/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=0.01;, score=(train=0.410, test=0.409) total time=   0.9s\n",
      "[CV 3/3; 1/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=0.01;, score=(train=0.411, test=0.393) total time=   0.9s\n",
      "[CV 2/3; 3/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=1.0.\n",
      "[CV 3/3; 3/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=1.0.\n",
      "[CV 1/3; 2/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=0.1;, score=(train=0.461, test=0.448) total time=   1.0s\n",
      "[CV 2/3; 2/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=0.1;, score=(train=0.452, test=0.445) total time=   1.0s\n",
      "[CV 1/3; 4/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=10.0\n",
      "[CV 3/3; 2/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=0.1;, score=(train=0.463, test=0.439) total time=   1.0s\n",
      "[CV 2/3; 4/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=10.0\n",
      "[CV 3/3; 4/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=10.0\n",
      "[CV 1/3; 3/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=1.0;, score=(train=0.470, test=0.459) total time=   1.5s\n",
      "[CV 1/3; 5/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=0.01\n",
      "[CV 2/3; 3/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=1.0;, score=(train=0.479, test=0.454) total time=   1.5s\n",
      "[CV 2/3; 5/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=0.01\n",
      "[CV 3/3; 3/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=1.0;, score=(train=0.488, test=0.439) total time=   1.6s\n",
      "[CV 3/3; 5/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=0.01\n",
      "[CV 1/3; 5/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=0.01;, score=(train=0.418, test=0.407) total time=   2.5s\n",
      "[CV 1/3; 6/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=0.1.\n",
      "[CV 2/3; 5/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=0.01;, score=(train=0.410, test=0.409) total time=   2.5s\n",
      "[CV 2/3; 6/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=0.1.\n",
      "[CV 3/3; 5/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=0.01;, score=(train=0.411, test=0.393) total time=   2.5s\n",
      "[CV 3/3; 6/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=0.1.\n",
      "[CV 1/3; 6/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=0.1;, score=(train=0.472, test=0.453) total time=   2.4s\n",
      "[CV 1/3; 7/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=1.0.\n",
      "[CV 2/3; 6/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=0.1;, score=(train=0.472, test=0.464) total time=   2.4s\n",
      "[CV 2/3; 7/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=1.0.\n",
      "[CV 3/3; 6/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=0.1;, score=(train=0.489, test=0.454) total time=   2.5s\n",
      "[CV 3/3; 7/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=1.0.\n",
      "[CV 2/3; 4/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=10.0;, score=(train=0.483, test=0.450) total time=   7.1s\n",
      "[CV 1/3; 8/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=10.0\n",
      "[CV 1/3; 4/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=10.0;, score=(train=0.487, test=0.449) total time=   7.5s\n",
      "[CV 2/3; 8/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=10.0\n",
      "[CV 3/3; 4/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=10.0;, score=(train=0.486, test=0.447) total time=   7.4s\n",
      "[CV 3/3; 8/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=10.0\n",
      "[CV 1/3; 7/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=1.0;, score=(train=0.505, test=0.461) total time=   3.6s\n",
      "[CV 1/3; 9/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.01\n",
      "[CV 2/3; 7/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=1.0;, score=(train=0.508, test=0.468) total time=   4.0s\n",
      "[CV 2/3; 9/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.01\n",
      "[CV 3/3; 7/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=1.0;, score=(train=0.509, test=0.456) total time=   4.3s\n",
      "[CV 3/3; 9/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.01\n",
      "[CV 1/3; 8/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=10.0;, score=(train=0.515, test=0.450) total time=  19.8s\n",
      "[CV 2/3; 8/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=10.0;, score=(train=0.511, test=0.447) total time=  19.6s\n",
      "[CV 3/3; 8/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=10.0;, score=(train=0.509, test=0.437) total time=  19.5s\n",
      "[CV 1/3; 10/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1\n",
      "[CV 2/3; 10/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1\n",
      "[CV 3/3; 10/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1\n",
      "[CV 1/3; 9/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.01;, score=(train=0.418, test=0.407) total time= 1.1min\n",
      "[CV 1/3; 11/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0\n",
      "[CV 2/3; 9/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.01;, score=(train=0.410, test=0.409) total time= 1.1min\n",
      "[CV 2/3; 11/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0\n",
      "[CV 3/3; 9/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.01;, score=(train=0.411, test=0.393) total time= 1.1min\n",
      "[CV 3/3; 11/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0\n",
      "[CV 1/3; 10/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1;, score=(train=0.475, test=0.453) total time= 1.1min\n",
      "[CV 1/3; 12/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0\n",
      "[CV 2/3; 10/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1;, score=(train=0.492, test=0.474) total time= 1.1min\n",
      "[CV 3/3; 10/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1;, score=(train=0.492, test=0.457) total time= 1.1min\n",
      "[CV 2/3; 12/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0\n",
      "[CV 3/3; 12/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0\n",
      "[CV 1/3; 11/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0;, score=(train=0.548, test=0.473) total time= 1.2min\n",
      "[CV 2/3; 11/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0;, score=(train=0.545, test=0.483) total time= 1.2min\n",
      "[CV 1/3; 13/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=0.01\n",
      "[CV 2/3; 13/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=0.01\n",
      "[CV 3/3; 11/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0;, score=(train=0.550, test=0.468) total time= 1.2min\n",
      "[CV 3/3; 13/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=0.01\n",
      "[CV 1/3; 13/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=0.01;, score=(train=0.456, test=0.438) total time=   1.0s\n",
      "[CV 1/3; 14/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=0.1.\n",
      "[CV 2/3; 13/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=0.01;, score=(train=0.454, test=0.444) total time=   1.0s\n",
      "[CV 2/3; 14/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=0.1.\n",
      "[CV 3/3; 13/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=0.01;, score=(train=0.457, test=0.432) total time=   1.0s\n",
      "[CV 3/3; 14/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=0.1.\n",
      "[CV 1/3; 14/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=0.1;, score=(train=0.483, test=0.454) total time=   1.0s\n",
      "[CV 1/3; 15/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=1.0.\n",
      "[CV 2/3; 14/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=0.1;, score=(train=0.479, test=0.457) total time=   1.0s\n",
      "[CV 2/3; 15/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=1.0.\n",
      "[CV 3/3; 14/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=0.1;, score=(train=0.475, test=0.434) total time=   1.0s\n",
      "[CV 3/3; 15/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=1.0.\n",
      "[CV 1/3; 15/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=1.0;, score=(train=0.466, test=0.437) total time=   1.6s\n",
      "[CV 1/3; 16/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=10.0\n",
      "[CV 2/3; 15/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=1.0;, score=(train=0.472, test=0.453) total time=   1.7s\n",
      "[CV 2/3; 16/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=10.0\n",
      "[CV 3/3; 15/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=1.0;, score=(train=0.482, test=0.436) total time=   1.7s\n",
      "[CV 3/3; 16/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=10.0\n",
      "[CV 3/3; 16/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=10.0;, score=(train=0.490, test=0.449) total time=   8.3s\n",
      "[CV 1/3; 17/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.01\n",
      "[CV 1/3; 16/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=10.0;, score=(train=0.477, test=0.454) total time=   9.6s\n",
      "[CV 2/3; 17/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.01\n",
      "[CV 2/3; 16/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=10.0;, score=(train=0.475, test=0.451) total time=   9.5s\n",
      "[CV 3/3; 17/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.01\n",
      "[CV 1/3; 17/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.01;, score=(train=0.469, test=0.451) total time=   2.5s\n",
      "[CV 1/3; 18/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.1.\n",
      "[CV 2/3; 17/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.01;, score=(train=0.476, test=0.466) total time=   2.5s\n",
      "[CV 2/3; 18/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.1.\n",
      "[CV 3/3; 17/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.01;, score=(train=0.475, test=0.446) total time=   2.5s\n",
      "[CV 3/3; 18/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.1.\n",
      "[CV 1/3; 18/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.1;, score=(train=0.512, test=0.466) total time=   2.4s\n",
      "[CV 1/3; 19/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=1.0.\n",
      "[CV 2/3; 18/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.1;, score=(train=0.502, test=0.474) total time=   2.4s\n",
      "[CV 2/3; 19/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=1.0.\n",
      "[CV 3/3; 18/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.1;, score=(train=0.510, test=0.454) total time=   2.5s\n",
      "[CV 3/3; 19/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=1.0.\n",
      "[CV 1/3; 19/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=1.0;, score=(train=0.513, test=0.455) total time=   3.3s\n",
      "[CV 1/3; 20/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=10.0\n",
      "[CV 2/3; 19/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=1.0;, score=(train=0.510, test=0.457) total time=   3.4s\n",
      "[CV 2/3; 20/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=10.0\n",
      "[CV 3/3; 19/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=1.0;, score=(train=0.512, test=0.443) total time=   3.4s\n",
      "[CV 3/3; 20/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=10.0\n",
      "[CV 3/3; 12/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0;, score=(train=0.574, test=0.449) total time= 1.5min\n",
      "[CV 2/3; 12/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0;, score=(train=0.577, test=0.449) total time= 1.5min\n",
      "[CV 1/3; 21/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01\n",
      "[CV 2/3; 21/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01\n",
      "[CV 1/3; 12/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0;, score=(train=0.577, test=0.451) total time= 1.5min\n",
      "[CV 3/3; 21/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01\n",
      "[CV 1/3; 20/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=10.0;, score=(train=0.507, test=0.453) total time=  20.9s\n",
      "[CV 1/3; 22/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1\n",
      "[CV 3/3; 20/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=10.0;, score=(train=0.513, test=0.446) total time=  20.5s\n",
      "[CV 2/3; 22/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1\n",
      "[CV 2/3; 20/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=10.0;, score=(train=0.505, test=0.448) total time=  21.6s\n",
      "[CV 3/3; 22/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1\n",
      "[CV 1/3; 21/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01;, score=(train=0.474, test=0.451) total time=  12.7s\n",
      "[CV 1/3; 23/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0\n",
      "[CV 2/3; 21/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01;, score=(train=0.479, test=0.474) total time=  44.5s\n",
      "[CV 2/3; 23/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0\n",
      "[CV 3/3; 21/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01;, score=(train=0.496, test=0.460) total time=  53.4s\n",
      "[CV 3/3; 23/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0\n",
      "[CV 1/3; 22/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1;, score=(train=0.547, test=0.475) total time= 1.1min\n",
      "[CV 1/3; 24/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=10.0\n",
      "[CV 2/3; 22/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1;, score=(train=0.539, test=0.485) total time= 1.1min\n",
      "[CV 3/3; 22/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1;, score=(train=0.547, test=0.472) total time= 1.2min\n",
      "[CV 2/3; 24/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=10.0\n",
      "[CV 3/3; 24/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=10.0\n",
      "[CV 1/3; 23/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0;, score=(train=0.592, test=0.452) total time= 1.3min\n",
      "[CV 1/3; 25/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=0.01\n",
      "[CV 1/3; 25/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=0.01;, score=(train=0.446, test=0.441) total time=   2.9s\n",
      "[CV 2/3; 25/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=0.01\n",
      "[CV 2/3; 25/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=0.01;, score=(train=0.441, test=0.421) total time=   1.6s\n",
      "[CV 3/3; 25/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=0.01\n",
      "[CV 3/3; 25/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=0.01;, score=(train=0.455, test=0.426) total time=   1.0s\n",
      "[CV 1/3; 26/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=0.1.\n",
      "[CV 1/3; 26/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=0.1;, score=(train=0.454, test=0.433) total time=   1.0s\n",
      "[CV 2/3; 26/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=0.1.\n",
      "[CV 2/3; 26/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=0.1;, score=(train=0.444, test=0.416) total time=   1.2s\n",
      "[CV 3/3; 26/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=0.1.\n",
      "[CV 3/3; 26/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=0.1;, score=(train=0.455, test=0.413) total time=   1.0s\n",
      "[CV 1/3; 27/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=1.0.\n",
      "[CV 1/3; 27/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=1.0;, score=(train=0.452, test=0.423) total time=   1.7s\n",
      "[CV 2/3; 27/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=1.0.\n",
      "[CV 2/3; 27/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=1.0;, score=(train=0.447, test=0.412) total time=   1.5s\n",
      "[CV 3/3; 27/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=1.0.\n",
      "[CV 3/3; 27/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=1.0;, score=(train=0.453, test=0.411) total time=   1.6s\n",
      "[CV 1/3; 28/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=10.0\n",
      "[CV 1/3; 28/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=10.0;, score=(train=0.449, test=0.412) total time=   8.8s\n",
      "[CV 2/3; 28/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=10.0\n",
      "[CV 2/3; 23/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0;, score=(train=0.581, test=0.463) total time= 1.1min\n",
      "[CV 3/3; 28/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=10.0\n",
      "[CV 3/3; 23/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0;, score=(train=0.584, test=0.453) total time= 1.1min\n",
      "[CV 1/3; 29/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=0.01\n",
      "[CV 2/3; 28/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=10.0;, score=(train=0.446, test=0.416) total time=  10.8s\n",
      "[CV 2/3; 29/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=0.01\n",
      "[CV 3/3; 28/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=10.0;, score=(train=0.457, test=0.393) total time=  10.7s\n",
      "[CV 3/3; 29/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=0.01\n",
      "[CV 2/3; 29/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=0.01;, score=(train=0.465, test=0.444) total time=   3.9s\n",
      "[CV 1/3; 30/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=0.1.\n",
      "[CV 1/3; 29/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=0.01;, score=(train=0.474, test=0.439) total time=   4.6s\n",
      "[CV 2/3; 30/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=0.1.\n",
      "[CV 3/3; 29/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=0.01;, score=(train=0.480, test=0.447) total time=   2.9s\n",
      "[CV 3/3; 30/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=0.1.\n",
      "[CV 1/3; 30/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=0.1;, score=(train=0.497, test=0.431) total time=   2.9s\n",
      "[CV 2/3; 30/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=0.1;, score=(train=0.489, test=0.433) total time=   2.8s\n",
      "[CV 1/3; 31/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=1.0.\n",
      "[CV 2/3; 31/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=1.0.\n",
      "[CV 3/3; 30/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=0.1;, score=(train=0.490, test=0.419) total time=   2.6s\n",
      "[CV 3/3; 31/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=1.0.\n",
      "[CV 1/3; 31/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=1.0;, score=(train=0.499, test=0.421) total time=   3.9s\n",
      "[CV 2/3; 31/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=1.0;, score=(train=0.488, test=0.427) total time=   3.9s\n",
      "[CV 1/3; 32/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=10.0\n",
      "[CV 2/3; 32/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=10.0\n",
      "[CV 3/3; 31/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=1.0;, score=(train=0.490, test=0.417) total time=   3.7s\n",
      "[CV 3/3; 32/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=10.0\n",
      "[CV 1/3; 32/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=10.0;, score=(train=0.492, test=0.428) total time=  17.9s\n",
      "[CV 2/3; 32/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=10.0;, score=(train=0.484, test=0.429) total time=  17.9s\n",
      "[CV 1/3; 33/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01\n",
      "[CV 2/3; 33/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01\n",
      "[CV 3/3; 32/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=10.0;, score=(train=0.491, test=0.427) total time=  17.0s\n",
      "[CV 3/3; 33/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01\n",
      "[CV 1/3; 24/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=10.0;, score=(train=0.611, test=0.453) total time= 1.2min\n",
      "[CV 1/3; 34/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=0.1\n",
      "[CV 2/3; 24/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=10.0;, score=(train=0.614, test=0.436) total time= 1.5min\n",
      "[CV 3/3; 24/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=10.0;, score=(train=0.607, test=0.435) total time= 1.5min\n",
      "[CV 2/3; 34/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=0.1\n",
      "[CV 3/3; 34/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=0.1\n",
      "[CV 2/3; 34/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=0.1;, score=(train=0.566, test=0.455) total time=  20.2s\n",
      "[CV 1/3; 35/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=1.0\n",
      "[CV 1/3; 33/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01;, score=(train=0.500, test=0.451) total time= 1.3min\n",
      "[CV 2/3; 35/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=1.0\n",
      "[CV 2/3; 33/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01;, score=(train=0.509, test=0.470) total time= 1.4min\n",
      "[CV 3/3; 35/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=1.0\n",
      "[CV 3/3; 33/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01;, score=(train=0.513, test=0.448) total time= 1.4min\n",
      "[CV 1/3; 34/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=0.1;, score=(train=0.568, test=0.458) total time= 1.2min\n",
      "[CV 1/3; 36/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=10.0\n",
      "[CV 2/3; 36/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=10.0\n",
      "[CV 3/3; 34/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=0.1;, score=(train=0.578, test=0.432) total time=  58.9s\n",
      "[CV 3/3; 36/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=10.0\n",
      "[CV 1/3; 35/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=1.0;, score=(train=0.597, test=0.423) total time=  56.8s\n",
      "[CV 1/3; 37/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=0.01\n",
      "[CV 1/3; 37/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=0.01;, score=(train=0.420, test=0.407) total time=   0.9s\n",
      "[CV 2/3; 37/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=0.01\n",
      "[CV 2/3; 37/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=0.01;, score=(train=0.410, test=0.409) total time=   0.9s\n",
      "[CV 3/3; 37/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=0.01\n",
      "[CV 3/3; 37/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=0.01;, score=(train=0.413, test=0.395) total time=   0.9s\n",
      "[CV 1/3; 38/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=0.1\n",
      "[CV 1/3; 38/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=0.1;, score=(train=0.427, test=0.395) total time=   0.9s\n",
      "[CV 2/3; 38/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=0.1\n",
      "[CV 2/3; 38/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=0.1;, score=(train=0.412, test=0.403) total time=   1.1s\n",
      "[CV 3/3; 38/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=0.1\n",
      "[CV 3/3; 38/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=0.1;, score=(train=0.413, test=0.386) total time=   1.0s\n",
      "[CV 1/3; 39/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=1.0\n",
      "[CV 1/3; 39/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=1.0;, score=(train=0.420, test=0.394) total time=   1.5s\n",
      "[CV 2/3; 39/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=1.0\n",
      "[CV 2/3; 39/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=1.0;, score=(train=0.411, test=0.398) total time=   1.4s\n",
      "[CV 3/3; 39/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=1.0\n",
      "[CV 3/3; 39/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=1.0;, score=(train=0.414, test=0.378) total time=   1.4s\n",
      "[CV 1/3; 40/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=10.0\n",
      "[CV 1/3; 40/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=10.0;, score=(train=0.424, test=0.399) total time=   6.9s\n",
      "[CV 2/3; 40/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=10.0\n",
      "[CV 2/3; 40/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=10.0;, score=(train=0.413, test=0.394) total time=   8.6s\n",
      "[CV 3/3; 40/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=10.0\n",
      "[CV 2/3; 35/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=1.0;, score=(train=0.591, test=0.422) total time=  58.8s\n",
      "[CV 1/3; 41/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=0.01\n",
      "[CV 3/3; 35/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=1.0;, score=(train=0.595, test=0.417) total time=  59.8s\n",
      "[CV 2/3; 41/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=0.01\n",
      "[CV 1/3; 41/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=0.01;, score=(train=0.420, test=0.405) total time=   2.2s\n",
      "[CV 3/3; 41/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=0.01\n",
      "[CV 2/3; 41/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=0.01;, score=(train=0.415, test=0.406) total time=   2.3s\n",
      "[CV 1/3; 42/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=0.1\n",
      "[CV 3/3; 41/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=0.01;, score=(train=0.413, test=0.392) total time=   2.3s\n",
      "[CV 2/3; 42/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=0.1\n",
      "[CV 3/3; 40/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=10.0;, score=(train=0.420, test=0.380) total time=   8.9s\n",
      "[CV 3/3; 42/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=0.1\n",
      "[CV 1/3; 42/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=0.1;, score=(train=0.439, test=0.392) total time=   2.4s\n",
      "[CV 1/3; 43/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=1.0\n",
      "[CV 2/3; 42/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=0.1;, score=(train=0.428, test=0.378) total time=   2.4s\n",
      "[CV 2/3; 43/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=1.0\n",
      "[CV 3/3; 42/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=0.1;, score=(train=0.433, test=0.381) total time=   2.6s\n",
      "[CV 3/3; 43/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=1.0\n",
      "[CV 1/3; 43/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=1.0;, score=(train=0.436, test=0.377) total time=   3.6s\n",
      "[CV 1/3; 44/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=10.0\n",
      "[CV 2/3; 43/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=1.0;, score=(train=0.432, test=0.376) total time=   3.5s\n",
      "[CV 2/3; 44/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=10.0\n",
      "[CV 3/3; 43/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=1.0;, score=(train=0.433, test=0.360) total time=   3.5s\n",
      "[CV 3/3; 44/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=10.0\n",
      "[CV 1/3; 36/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=10.0;, score=(train=0.590, test=0.421) total time= 1.4min\n",
      "[CV 1/3; 45/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=0.01\n",
      "[CV 3/3; 36/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=10.0;, score=(train=0.603, test=0.423) total time= 1.4min\n",
      "[CV 2/3; 36/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=10.0;, score=(train=0.601, test=0.414) total time= 1.4min\n",
      "[CV 2/3; 45/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=0.01\n",
      "[CV 3/3; 45/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=0.01\n",
      "[CV 1/3; 44/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=10.0;, score=(train=0.442, test=0.378) total time=  16.2s\n",
      "[CV 1/3; 46/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=0.1\n",
      "[CV 2/3; 44/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=10.0;, score=(train=0.428, test=0.370) total time=  16.5s\n",
      "[CV 2/3; 46/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=0.1\n",
      "[CV 3/3; 44/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=10.0;, score=(train=0.438, test=0.366) total time=  17.9s\n",
      "[CV 3/3; 46/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=0.1\n",
      "[CV 1/3; 45/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=0.01;, score=(train=0.419, test=0.407) total time=  19.7s\n",
      "[CV 1/3; 47/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=1.0\n",
      "[CV 1/3; 47/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=1.0;, score=(train=0.529, test=0.350) total time=  13.6s\n",
      "[CV 2/3; 47/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=1.0\n",
      "[CV 2/3; 45/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=0.01;, score=(train=0.410, test=0.408) total time= 1.1min\n",
      "[CV 3/3; 45/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=0.01;, score=(train=0.411, test=0.393) total time= 1.1min\n",
      "[CV 3/3; 47/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=1.0\n",
      "[CV 1/3; 48/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=10.0\n",
      "[CV 1/3; 46/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=0.1;, score=(train=0.508, test=0.372) total time= 1.1min\n",
      "[CV 2/3; 48/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=10.0\n",
      "[CV 2/3; 46/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=0.1;, score=(train=0.506, test=0.356) total time= 1.2min\n",
      "[CV 3/3; 48/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=10.0\n",
      "[CV 3/3; 46/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=0.1;, score=(train=0.511, test=0.352) total time= 1.3min\n",
      "[CV 2/3; 47/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=1.0;, score=(train=0.517, test=0.339) total time= 1.2min\n",
      "[CV 3/3; 47/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=1.0;, score=(train=0.526, test=0.345) total time=  45.8s\n",
      "[CV 2/3; 48/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=10.0;, score=(train=0.522, test=0.345) total time= 1.2min\n",
      "[CV 3/3; 48/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=10.0;, score=(train=0.532, test=0.351) total time= 1.2min\n",
      "[CV 1/3; 48/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=10.0;, score=(train=0.521, test=0.345) total time= 1.3min\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 16\n",
      "n_resources: 37560\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[CV 2/3; 1/16] START kernel__gamma=0.01, kernel__n_components=100, model__C=10.0\n",
      "[CV 1/3; 1/16] START kernel__gamma=0.01, kernel__n_components=100, model__C=10.0\n",
      "[CV 3/3; 1/16] START kernel__gamma=0.01, kernel__n_components=100, model__C=10.0\n",
      "[CV 1/3; 2/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=10.0.\n",
      "[CV 2/3; 2/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=10.0.\n",
      "[CV 3/3; 2/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=10.0.\n",
      "[CV 3/3; 1/16] END kernel__gamma=0.01, kernel__n_components=100, model__C=10.0;, score=(train=0.471, test=0.465) total time=  27.4s\n",
      "[CV 1/3; 3/16] START kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0\n",
      "[CV 1/3; 1/16] END kernel__gamma=0.01, kernel__n_components=100, model__C=10.0;, score=(train=0.474, test=0.465) total time=  32.2s\n",
      "[CV 2/3; 3/16] START kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0\n",
      "[CV 2/3; 1/16] END kernel__gamma=0.01, kernel__n_components=100, model__C=10.0;, score=(train=0.465, test=0.455) total time=  33.1s\n",
      "[CV 3/3; 3/16] START kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0\n",
      "[CV 2/3; 2/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=10.0;, score=(train=0.487, test=0.464) total time=  53.9s\n",
      "[CV 1/3; 4/16] START kernel__gamma=0.01, kernel__n_components=100, model__C=1.0.\n",
      "[CV 1/3; 4/16] END kernel__gamma=0.01, kernel__n_components=100, model__C=1.0;, score=(train=0.469, test=0.467) total time=   4.4s\n",
      "[CV 2/3; 4/16] START kernel__gamma=0.01, kernel__n_components=100, model__C=1.0.\n",
      "[CV 3/3; 2/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=10.0;, score=(train=0.488, test=0.465) total time= 1.0min\n",
      "[CV 3/3; 4/16] START kernel__gamma=0.01, kernel__n_components=100, model__C=1.0.\n",
      "[CV 1/3; 2/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=10.0;, score=(train=0.486, test=0.469) total time= 1.0min\n",
      "[CV 1/3; 5/16] START kernel__gamma=0.1, kernel__n_components=100, model__C=10.0.\n",
      "[CV 2/3; 4/16] END kernel__gamma=0.01, kernel__n_components=100, model__C=1.0;, score=(train=0.465, test=0.449) total time=   5.0s\n",
      "[CV 2/3; 5/16] START kernel__gamma=0.1, kernel__n_components=100, model__C=10.0.\n",
      "[CV 3/3; 4/16] END kernel__gamma=0.01, kernel__n_components=100, model__C=1.0;, score=(train=0.469, test=0.459) total time=   5.0s\n",
      "[CV 3/3; 5/16] START kernel__gamma=0.1, kernel__n_components=100, model__C=10.0.\n",
      "[CV 1/3; 5/16] END kernel__gamma=0.1, kernel__n_components=100, model__C=10.0;, score=(train=0.475, test=0.462) total time=  34.2s\n",
      "[CV 1/3; 6/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=1.0..\n",
      "[CV 3/3; 5/16] END kernel__gamma=0.1, kernel__n_components=100, model__C=10.0;, score=(train=0.462, test=0.455) total time=  40.4s\n",
      "[CV 2/3; 6/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=1.0..\n",
      "[CV 2/3; 5/16] END kernel__gamma=0.1, kernel__n_components=100, model__C=10.0;, score=(train=0.462, test=0.450) total time=  41.5s\n",
      "[CV 3/3; 6/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=1.0..\n",
      "[CV 1/3; 6/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=1.0;, score=(train=0.489, test=0.463) total time=   8.6s\n",
      "[CV 1/3; 7/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.01.\n",
      "[CV 1/3; 7/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.01;, score=(train=0.481, test=0.474) total time=   5.1s\n",
      "[CV 2/3; 7/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.01.\n",
      "[CV 2/3; 6/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=1.0;, score=(train=0.483, test=0.467) total time=   9.2s\n",
      "[CV 3/3; 6/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=1.0;, score=(train=0.491, test=0.476) total time=   9.1s\n",
      "[CV 3/3; 7/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.01.\n",
      "[CV 1/3; 8/16] START kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0.\n",
      "[CV 2/3; 7/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.01;, score=(train=0.483, test=0.476) total time=   4.0s\n",
      "[CV 2/3; 8/16] START kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0.\n",
      "[CV 3/3; 7/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.01;, score=(train=0.481, test=0.472) total time=   4.0s\n",
      "[CV 3/3; 8/16] START kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0.\n",
      "[CV 2/3; 8/16] END kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0;, score=(train=0.532, test=0.463) total time=  37.6s\n",
      "[CV 1/3; 8/16] END kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0;, score=(train=0.532, test=0.477) total time=  38.5s\n",
      "[CV 1/3; 9/16] START kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01\n",
      "[CV 2/3; 9/16] START kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01\n",
      "[CV 3/3; 8/16] END kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0;, score=(train=0.533, test=0.474) total time=  40.8s\n",
      "[CV 3/3; 9/16] START kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01\n",
      "[CV 3/3; 3/16] END kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0;, score=(train=0.529, test=0.474) total time= 2.7min\n",
      "[CV 1/3; 3/16] END kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0;, score=(train=0.531, test=0.467) total time= 2.8min\n",
      "[CV 1/3; 10/16] START kernel__gamma=0.01, kernel__n_components=250, model__C=0.1\n",
      "[CV 2/3; 10/16] START kernel__gamma=0.01, kernel__n_components=250, model__C=0.1\n",
      "[CV 2/3; 10/16] END kernel__gamma=0.01, kernel__n_components=250, model__C=0.1;, score=(train=0.477, test=0.469) total time=   6.2s\n",
      "[CV 3/3; 10/16] START kernel__gamma=0.01, kernel__n_components=250, model__C=0.1\n",
      "[CV 1/3; 10/16] END kernel__gamma=0.01, kernel__n_components=250, model__C=0.1;, score=(train=0.488, test=0.481) total time=   6.8s\n",
      "[CV 1/3; 11/16] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1\n",
      "[CV 3/3; 10/16] END kernel__gamma=0.01, kernel__n_components=250, model__C=0.1;, score=(train=0.479, test=0.465) total time=   5.3s\n",
      "[CV 2/3; 11/16] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1\n",
      "[CV 2/3; 3/16] END kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0;, score=(train=0.522, test=0.468) total time= 3.1min\n",
      "[CV 3/3; 11/16] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1\n",
      "[CV 2/3; 9/16] END kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01;, score=(train=0.503, test=0.468) total time= 2.2min\n",
      "[CV 3/3; 9/16] END kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01;, score=(train=0.506, test=0.470) total time= 2.0min\n",
      "[CV 1/3; 9/16] END kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01;, score=(train=0.502, test=0.475) total time= 2.2min\n",
      "[CV 1/3; 12/16] START kernel__gamma=0.01, kernel__n_components=250, model__C=1.0\n",
      "[CV 2/3; 12/16] START kernel__gamma=0.01, kernel__n_components=250, model__C=1.0\n",
      "[CV 3/3; 12/16] START kernel__gamma=0.01, kernel__n_components=250, model__C=1.0\n",
      "[CV 1/3; 12/16] END kernel__gamma=0.01, kernel__n_components=250, model__C=1.0;, score=(train=0.485, test=0.480) total time=  14.1s\n",
      "[CV 2/3; 12/16] END kernel__gamma=0.01, kernel__n_components=250, model__C=1.0;, score=(train=0.481, test=0.464) total time=  14.0s\n",
      "[CV 1/3; 13/16] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01\n",
      "[CV 3/3; 12/16] END kernel__gamma=0.01, kernel__n_components=250, model__C=1.0;, score=(train=0.486, test=0.472) total time=  14.4s\n",
      "[CV 2/3; 13/16] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01\n",
      "[CV 3/3; 13/16] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01\n",
      "[CV 1/3; 11/16] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1;, score=(train=0.497, test=0.487) total time= 2.4min\n",
      "[CV 1/3; 14/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.1.\n",
      "[CV 2/3; 11/16] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1;, score=(train=0.493, test=0.482) total time= 2.4min\n",
      "[CV 2/3; 14/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.1.\n",
      "[CV 3/3; 11/16] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1;, score=(train=0.495, test=0.485) total time= 2.4min\n",
      "[CV 3/3; 14/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.1.\n",
      "[CV 2/3; 14/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.1;, score=(train=0.486, test=0.468) total time=  12.7s\n",
      "[CV 1/3; 15/16] START kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0\n",
      "[CV 1/3; 14/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.1;, score=(train=0.493, test=0.471) total time=  18.9s\n",
      "[CV 2/3; 15/16] START kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0\n",
      "[CV 3/3; 14/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.1;, score=(train=0.491, test=0.469) total time=  11.2s\n",
      "[CV 3/3; 15/16] START kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0\n",
      "[CV 3/3; 13/16] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01;, score=(train=0.496, test=0.487) total time= 2.5min\n",
      "[CV 2/3; 13/16] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01;, score=(train=0.490, test=0.480) total time= 2.5min\n",
      "[CV 1/3; 13/16] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01;, score=(train=0.496, test=0.488) total time= 2.5min\n",
      "[CV 1/3; 16/16] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1\n",
      "[CV 2/3; 16/16] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1\n",
      "[CV 3/3; 16/16] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1\n",
      "[CV 2/3; 15/16] END kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0;, score=(train=0.513, test=0.474) total time= 2.7min\n",
      "[CV 3/3; 15/16] END kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0;, score=(train=0.520, test=0.489) total time= 2.7min\n",
      "[CV 1/3; 15/16] END kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0;, score=(train=0.523, test=0.482) total time= 2.7min\n",
      "[CV 2/3; 16/16] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1;, score=(train=0.515, test=0.485) total time= 2.0min\n",
      "[CV 1/3; 16/16] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1;, score=(train=0.525, test=0.490) total time= 2.0min\n",
      "[CV 3/3; 16/16] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1;, score=(train=0.517, test=0.484) total time= 2.0min\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 6\n",
      "n_resources: 112680\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV 1/3; 1/6] START kernel__gamma=0.01, kernel__n_components=250, model__C=1.0..\n",
      "[CV 2/3; 1/6] START kernel__gamma=0.01, kernel__n_components=250, model__C=1.0..\n",
      "[CV 3/3; 1/6] START kernel__gamma=0.01, kernel__n_components=250, model__C=1.0..\n",
      "[CV 1/3; 2/6] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.01..\n",
      "[CV 2/3; 2/6] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.01..\n",
      "[CV 3/3; 2/6] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.01..\n",
      "[CV 1/3; 2/6] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.01;, score=(train=0.482, test=0.474) total time=   9.5s\n",
      "[CV 1/3; 3/6] START kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0.\n",
      "[CV 2/3; 2/6] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.01;, score=(train=0.484, test=0.478) total time=  10.0s\n",
      "[CV 3/3; 2/6] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.01;, score=(train=0.479, test=0.477) total time=   9.9s\n",
      "[CV 2/3; 3/6] START kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0.\n",
      "[CV 3/3; 3/6] START kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0.\n",
      "[CV 3/3; 1/6] END kernel__gamma=0.01, kernel__n_components=250, model__C=1.0;, score=(train=0.484, test=0.480) total time=  31.2s\n",
      "[CV 2/3; 1/6] END kernel__gamma=0.01, kernel__n_components=250, model__C=1.0;, score=(train=0.487, test=0.480) total time=  31.2s\n",
      "[CV 1/3; 1/6] END kernel__gamma=0.01, kernel__n_components=250, model__C=1.0;, score=(train=0.488, test=0.475) total time=  31.3s\n",
      "[CV 1/3; 4/6] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1.\n",
      "[CV 2/3; 4/6] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1.\n",
      "[CV 3/3; 4/6] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1.\n",
      "[CV 1/3; 3/6] END kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0;, score=(train=0.507, test=0.487) total time= 4.9min\n",
      "[CV 1/3; 5/6] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01.\n",
      "[CV 2/3; 3/6] END kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0;, score=(train=0.507, test=0.490) total time= 5.4min\n",
      "[CV 2/3; 5/6] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01.\n",
      "[CV 3/3; 3/6] END kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0;, score=(train=0.507, test=0.487) total time= 5.9min\n",
      "[CV 3/3; 5/6] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01.\n",
      "[CV 3/3; 4/6] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1;, score=(train=0.497, test=0.491) total time= 6.4min\n",
      "[CV 2/3; 4/6] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1;, score=(train=0.499, test=0.491) total time= 6.4min\n",
      "[CV 1/3; 4/6] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1;, score=(train=0.497, test=0.490) total time= 6.6min\n",
      "[CV 1/3; 6/6] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1..\n",
      "[CV 2/3; 6/6] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1..\n",
      "[CV 3/3; 6/6] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1..\n",
      "[CV 1/3; 5/6] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01;, score=(train=0.498, test=0.492) total time= 6.7min\n",
      "[CV 2/3; 5/6] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01;, score=(train=0.499, test=0.496) total time= 6.6min\n",
      "[CV 3/3; 5/6] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01;, score=(train=0.499, test=0.491) total time= 6.4min\n",
      "[CV 1/3; 6/6] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1;, score=(train=0.511, test=0.491) total time= 6.4min\n",
      "[CV 3/3; 6/6] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1;, score=(train=0.509, test=0.491) total time= 6.3min\n",
      "[CV 2/3; 6/6] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1;, score=(train=0.510, test=0.491) total time= 6.3min\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 2\n",
      "n_resources: 338040\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[CV 2/3; 1/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1..\n",
      "[CV 1/3; 1/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1..\n",
      "[CV 3/3; 1/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1..\n",
      "[CV 1/3; 2/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01.\n",
      "[CV 2/3; 2/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01.\n",
      "[CV 3/3; 2/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01.\n"
     ]
    }
   ],
   "source": [
    "kernel_search.fit(train_df_transformed, train_df[\"citation_bucket\"])\n",
    "save_model(kernel_search, \"models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```log\n",
    "----------\n",
    "iter: 3\n",
    "n_candidates: 2\n",
    "n_resources: 338040\n",
    "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
    "[CV 2/3; 1/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1..\n",
    "[CV 1/3; 1/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1..\n",
    "[CV 3/3; 1/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1..\n",
    "[CV 1/3; 2/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01.\n",
    "[CV 2/3; 2/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01.\n",
    "[CV 3/3; 2/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about SGD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Size of train set: 304246'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Size of validation set: 33806'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;kernel&#x27;,\n",
       "                 RBFSampler(gamma=0.1, n_components=1000, random_state=42)),\n",
       "                (&#x27;model&#x27;, SGDClassifier(n_jobs=-1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-99\" type=\"checkbox\" ><label for=\"sk-estimator-id-99\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;kernel&#x27;,\n",
       "                 RBFSampler(gamma=0.1, n_components=1000, random_state=42)),\n",
       "                (&#x27;model&#x27;, SGDClassifier(n_jobs=-1))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-100\" type=\"checkbox\" ><label for=\"sk-estimator-id-100\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(n_jobs=-1,\n",
       "                  transformers=[(&#x27;ratio&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                  &#x27;author_count&#x27;]),\n",
       "                                (&#x27;date&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                (&#x27;text&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                  TfidfVectorizer())]),\n",
       "                                 &#x27;text&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-101\" type=\"checkbox\" ><label for=\"sk-estimator-id-101\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ratio</label><div class=\"sk-toggleable__content\"><pre>[&#x27;page_count&#x27;, &#x27;figure_count&#x27;, &#x27;author_count&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-102\" type=\"checkbox\" ><label for=\"sk-estimator-id-102\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-103\" type=\"checkbox\" ><label for=\"sk-estimator-id-103\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">date</label><div class=\"sk-toggleable__content\"><pre>[&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-104\" type=\"checkbox\" ><label for=\"sk-estimator-id-104\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-105\" type=\"checkbox\" ><label for=\"sk-estimator-id-105\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">text</label><div class=\"sk-toggleable__content\"><pre>text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-106\" type=\"checkbox\" ><label for=\"sk-estimator-id-106\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-107\" type=\"checkbox\" ><label for=\"sk-estimator-id-107\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RBFSampler</label><div class=\"sk-toggleable__content\"><pre>RBFSampler(gamma=0.1, n_components=1000, random_state=42)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-108\" type=\"checkbox\" ><label for=\"sk-estimator-id-108\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(n_jobs=-1)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[('ratio',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['page_count', 'figure_count',\n",
       "                                                   'author_count']),\n",
       "                                                 ('date',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['year', 'month', 'day']),\n",
       "                                                 ('text',\n",
       "                                                  Pipeline(steps=[('vectorizer',\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  'text')])),\n",
       "                ('kernel',\n",
       "                 RBFSampler(gamma=0.1, n_components=1000, random_state=42)),\n",
       "                ('model', SGDClassifier(n_jobs=-1))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_pipeline = deepcopy(pipeline)\n",
    "sgd_pipeline.steps.insert(1, (\"kernel\", RBFSampler(gamma=0.1, random_state=42, n_components=1000)))\n",
    "sgd_pipeline.steps[-1] = (\"model\", SGDClassifier(loss=\"hinge\", n_jobs=-1))\n",
    "train_df_trunc, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"citation_bucket\"],\n",
    ")\n",
    "display(f\"Size of train set: {len(train_df_trunc)}\")\n",
    "display(f\"Size of validation set: {len(val_df)}\")\n",
    "sgd_pipeline.fit(train_df_trunc, train_df_trunc[\"citation_bucket\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4475753173418878"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.4454830503460924"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sgd_pipeline.score(train_df_trunc, train_df_trunc[\"citation_bucket\"]))\n",
    "display(sgd_pipeline.score(val_df, val_df[\"citation_bucket\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More educated Grid Search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingGridSearchCV(cv=3,\n",
       "                    estimator=Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                                               ColumnTransformer(n_jobs=-1,\n",
       "                                                                 transformers=[(&#x27;ratio&#x27;,\n",
       "                                                                                Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                                                 MinMaxScaler())]),\n",
       "                                                                                [&#x27;page_count&#x27;,\n",
       "                                                                                 &#x27;figure_count&#x27;,\n",
       "                                                                                 &#x27;author_count&#x27;]),\n",
       "                                                                               (&#x27;date&#x27;,\n",
       "                                                                                Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                                                 MinMaxScaler())]),\n",
       "                                                                                [&#x27;year&#x27;,\n",
       "                                                                                 &#x27;month&#x27;,\n",
       "                                                                                 &#x27;day&#x27;]),\n",
       "                                                                               (&#x27;text&#x27;,\n",
       "                                                                                Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                                                 TfidfVectorizer())]),\n",
       "                                                                                &#x27;text&#x27;)...\n",
       "                                                                 1000],\n",
       "                                 &#x27;model__svm__C&#x27;: [0.01, 0.1, 1.0, 10.0],\n",
       "                                 &#x27;model__svm__class_weight&#x27;: [None, &#x27;balanced&#x27;],\n",
       "                                 &#x27;preprocessor__text__vectorizer__min_df&#x27;: [1,\n",
       "                                                                            3],\n",
       "                                 &#x27;preprocessor__text__vectorizer__strip_accents&#x27;: [&#x27;unicode&#x27;],\n",
       "                                 &#x27;preprocessor__text__vectorizer__tokenizer&#x27;: [None,\n",
       "                                                                               &lt;__main__.LemmaTokenizer object at 0x2bfc1d790&gt;,\n",
       "                                                                               &lt;__main__.StemmingTokenizer object at 0x2bfbbda90&gt;]}],\n",
       "                    verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingGridSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingGridSearchCV(cv=3,\n",
       "                    estimator=Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                                               ColumnTransformer(n_jobs=-1,\n",
       "                                                                 transformers=[(&#x27;ratio&#x27;,\n",
       "                                                                                Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                                                 MinMaxScaler())]),\n",
       "                                                                                [&#x27;page_count&#x27;,\n",
       "                                                                                 &#x27;figure_count&#x27;,\n",
       "                                                                                 &#x27;author_count&#x27;]),\n",
       "                                                                               (&#x27;date&#x27;,\n",
       "                                                                                Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                                                 MinMaxScaler())]),\n",
       "                                                                                [&#x27;year&#x27;,\n",
       "                                                                                 &#x27;month&#x27;,\n",
       "                                                                                 &#x27;day&#x27;]),\n",
       "                                                                               (&#x27;text&#x27;,\n",
       "                                                                                Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                                                 TfidfVectorizer())]),\n",
       "                                                                                &#x27;text&#x27;)...\n",
       "                                                                 1000],\n",
       "                                 &#x27;model__svm__C&#x27;: [0.01, 0.1, 1.0, 10.0],\n",
       "                                 &#x27;model__svm__class_weight&#x27;: [None, &#x27;balanced&#x27;],\n",
       "                                 &#x27;preprocessor__text__vectorizer__min_df&#x27;: [1,\n",
       "                                                                            3],\n",
       "                                 &#x27;preprocessor__text__vectorizer__strip_accents&#x27;: [&#x27;unicode&#x27;],\n",
       "                                 &#x27;preprocessor__text__vectorizer__tokenizer&#x27;: [None,\n",
       "                                                                               &lt;__main__.LemmaTokenizer object at 0x2bfc1d790&gt;,\n",
       "                                                                               &lt;__main__.StemmingTokenizer object at 0x2bfbbda90&gt;]}],\n",
       "                    verbose=10)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;model&#x27;, LinearSVC(loss=&#x27;hinge&#x27;, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(n_jobs=-1,\n",
       "                  transformers=[(&#x27;ratio&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                  &#x27;author_count&#x27;]),\n",
       "                                (&#x27;date&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                (&#x27;text&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                  TfidfVectorizer())]),\n",
       "                                 &#x27;text&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ratio</label><div class=\"sk-toggleable__content\"><pre>[&#x27;page_count&#x27;, &#x27;figure_count&#x27;, &#x27;author_count&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">date</label><div class=\"sk-toggleable__content\"><pre>[&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">text</label><div class=\"sk-toggleable__content\"><pre>text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(loss=&#x27;hinge&#x27;, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HalvingGridSearchCV(cv=3,\n",
       "                    estimator=Pipeline(steps=[('preprocessor',\n",
       "                                               ColumnTransformer(n_jobs=-1,\n",
       "                                                                 transformers=[('ratio',\n",
       "                                                                                Pipeline(steps=[('scaler',\n",
       "                                                                                                 MinMaxScaler())]),\n",
       "                                                                                ['page_count',\n",
       "                                                                                 'figure_count',\n",
       "                                                                                 'author_count']),\n",
       "                                                                               ('date',\n",
       "                                                                                Pipeline(steps=[('scaler',\n",
       "                                                                                                 MinMaxScaler())]),\n",
       "                                                                                ['year',\n",
       "                                                                                 'month',\n",
       "                                                                                 'day']),\n",
       "                                                                               ('text',\n",
       "                                                                                Pipeline(steps=[('vectorizer',\n",
       "                                                                                                 TfidfVectorizer())]),\n",
       "                                                                                'text')...\n",
       "                                                                 1000],\n",
       "                                 'model__svm__C': [0.01, 0.1, 1.0, 10.0],\n",
       "                                 'model__svm__class_weight': [None, 'balanced'],\n",
       "                                 'preprocessor__text__vectorizer__min_df': [1,\n",
       "                                                                            3],\n",
       "                                 'preprocessor__text__vectorizer__strip_accents': ['unicode'],\n",
       "                                 'preprocessor__text__vectorizer__tokenizer': [None,\n",
       "                                                                               <__main__.LemmaTokenizer object at 0x2bfc1d790>,\n",
       "                                                                               <__main__.StemmingTokenizer object at 0x2bfbbda90>]}],\n",
       "                    verbose=10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "text_params = {\n",
    "    \"preprocessor__text__vectorizer__min_df\": [1, 3],\n",
    "    \"preprocessor__text__vectorizer__tokenizer\": [None, LemmaTokenizer(), StemmingTokenizer()],\n",
    "    \"preprocessor__text__vectorizer__strip_accents\": [\"unicode\"],\n",
    "}\n",
    "\n",
    "kernel_pipeline = Pipeline([\n",
    "    (\"kernel\", RBFSampler(gamma=10, n_components=1000)),\n",
    "    (\"svm\", LinearSVC())\n",
    "])\n",
    "\n",
    "param_grid_kernel = {\n",
    "    \"model\": [kernel_pipeline],\n",
    "    \"model__kernel__gamma\": [0.01, 0.1, 1.0, 10.0],\n",
    "    \"model__kernel__n_components\": [100, 250, 1000],\n",
    "    \"model__svm__C\": [0.01, 0.1, 1.0, 10.0],\n",
    "    \"model__svm__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "param_grid_svm = {\n",
    "    \"model\": [LinearSVC()],\n",
    "    \"model__C\": [0.01, 0.1, 1.0, 10.0],\n",
    "    \"model__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "\n",
    "revised_param_grid = [\n",
    "    {**param_grid_svm, **text_params},\n",
    "    {**param_grid_kernel, **text_params},\n",
    "]\n",
    "\n",
    "kernel_search = HalvingGridSearchCV(pipeline, param_grid=revised_param_grid, cv=3, verbose=10, n_jobs=3)\n",
    "display(kernel_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 40/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=250, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x12ad82250>;, score=(train=0.500, test=0.451) total time=   5.3s\n",
      "[CV 2/3; 40/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=250, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1674dc250>;, score=(train=0.501, test=0.461) total time=   5.2s\n",
      "[CV 1/3; 41/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 40/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=250, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c8991310>;, score=(train=0.505, test=0.452) total time=   5.1s\n",
      "[CV 2/3; 41/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 41/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 41/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.453, test=0.440) total time=   2.6s\n",
      "[CV 2/3; 41/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.455, test=0.440) total time=   2.8s\n",
      "[CV 1/3; 42/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=250, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 41/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.457, test=0.437) total time=   2.5s\n",
      "[CV 2/3; 42/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=250, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 42/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=250, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 42/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=250, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.500, test=0.454) total time=   1.7s\n",
      "[CV 2/3; 42/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=250, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.494, test=0.471) total time=   1.6s\n",
      "[CV 1/3; 43/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=250, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c45f3690>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 42/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=250, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.499, test=0.469) total time=   1.7s\n",
      "[CV 3/3; 43/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=250, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c8806110>\n",
      "[CV 2/3; 43/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=250, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2d69987d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 43/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=250, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c45f3690>;, score=(train=0.497, test=0.457) total time=   6.0s\n",
      "[CV 1/3; 44/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2c41e5f90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 43/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=250, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c8806110>;, score=(train=0.493, test=0.476) total time=   5.9s\n",
      "[CV 2/3; 43/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=250, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2d69987d0>;, score=(train=0.496, test=0.471) total time=   6.4s\n",
      "[CV 2/3; 44/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2c0c28550>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 44/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b7761c90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 44/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2c41e5f90>;, score=(train=0.456, test=0.435) total time=  16.3s\n",
      "[CV 1/3; 45/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 44/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2c0c28550>;, score=(train=0.458, test=0.452) total time=  16.1s\n",
      "[CV 3/3; 44/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b7761c90>;, score=(train=0.455, test=0.442) total time=  16.2s\n",
      "[CV 2/3; 45/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 45/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 45/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.460, test=0.445) total time=   3.7s\n",
      "[CV 1/3; 46/70] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x29d6b0390>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 45/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.458, test=0.453) total time=   3.6s\n",
      "[CV 2/3; 46/70] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a7e7da90>\n",
      "[CV 3/3; 45/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.464, test=0.440) total time=   3.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 46/70] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2c60da250>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 46/70] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x29d6b0390>;, score=(train=0.701, test=0.473) total time=  15.0s\n",
      "[CV 1/3; 47/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=250, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 46/70] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a7e7da90>;, score=(train=0.691, test=0.484) total time=  14.8s\n",
      "[CV 2/3; 47/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=250, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 46/70] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2c60da250>;, score=(train=0.711, test=0.497) total time=  14.9s\n",
      "[CV 1/3; 47/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=250, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.449, test=0.439) total time=   1.2s\n",
      "[CV 3/3; 47/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=250, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 48/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 47/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=250, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.456, test=0.440) total time=   1.3s\n",
      "[CV 2/3; 48/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 47/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=250, model__svm__C=0.01, model__svm__class_weight=balanced, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.448, test=0.446) total time=   1.3s\n",
      "[CV 3/3; 48/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 48/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.536, test=0.471) total time=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 49/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x13ce62950>\n",
      "[CV 2/3; 48/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.537, test=0.492) total time=   2.8s\n",
      "[CV 2/3; 49/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a95fdb10>\n",
      "[CV 3/3; 48/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.540, test=0.490) total time=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 49/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2918d4510>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 49/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x13ce62950>;, score=(train=0.540, test=0.473) total time=  19.9s\n",
      "[CV 2/3; 49/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a95fdb10>;, score=(train=0.539, test=0.490) total time=  19.7s\n",
      "[CV 1/3; 50/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2a8515050>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 50/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2a7f2afd0>\n",
      "[CV 3/3; 49/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2918d4510>;, score=(train=0.537, test=0.480) total time=  20.0s\n",
      "[CV 3/3; 50/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2875bff10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 50/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2a8515050>;, score=(train=0.537, test=0.469) total time=   9.9s\n",
      "[CV 2/3; 50/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2a7f2afd0>;, score=(train=0.532, test=0.482) total time=   9.8s\n",
      "[CV 1/3; 51/70] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16b615110>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 51/70] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c1497c50>\n",
      "[CV 3/3; 50/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2875bff10>;, score=(train=0.541, test=0.480) total time=   9.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 51/70] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2b7808510>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 51/70] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16b615110>;, score=(train=0.718, test=0.472) total time=   4.5s\n",
      "[CV 2/3; 51/70] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c1497c50>;, score=(train=0.708, test=0.486) total time=   4.4s\n",
      "[CV 1/3; 52/70] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x14e2c3690>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 52/70] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17d965fd0>\n",
      "[CV 3/3; 51/70] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2b7808510>;, score=(train=0.723, test=0.495) total time=   4.3s\n",
      "[CV 3/3; 52/70] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2857c6810>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 52/70] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x14e2c3690>;, score=(train=0.739, test=0.477) total time=   4.5s\n",
      "[CV 2/3; 52/70] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17d965fd0>;, score=(train=0.730, test=0.487) total time=   4.5s\n",
      "[CV 1/3; 53/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b4e48650>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 52/70] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2857c6810>;, score=(train=0.749, test=0.497) total time=   4.5s\n",
      "[CV 2/3; 53/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x12f048c10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 53/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2857e5a50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 53/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b4e48650>;, score=(train=0.539, test=0.473) total time=  16.8s\n",
      "[CV 2/3; 53/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x12f048c10>;, score=(train=0.539, test=0.491) total time=  16.6s\n",
      "[CV 1/3; 54/70] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a850e490>\n",
      "[CV 3/3; 53/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2857e5a50>;, score=(train=0.543, test=0.483) total time=  16.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 54/70] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x17d8d3250>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 54/70] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x280e003d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 54/70] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a850e490>;, score=(train=0.730, test=0.478) total time=  15.0s\n",
      "[CV 1/3; 55/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 54/70] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x17d8d3250>;, score=(train=0.716, test=0.486) total time=  14.8s\n",
      "[CV 2/3; 55/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 54/70] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x280e003d0>;, score=(train=0.731, test=0.500) total time=  14.9s\n",
      "[CV 3/3; 55/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 55/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.543, test=0.477) total time=   7.0s\n",
      "[CV 1/3; 56/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x14e2c1590>\n",
      "[CV 2/3; 55/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.535, test=0.472) total time=   6.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 56/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x12f757f10>\n",
      "[CV 3/3; 55/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.541, test=0.488) total time=   6.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 56/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2921b03d0>\n",
      "[CV 1/3; 56/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x14e2c1590>;, score=(train=0.532, test=0.476) total time=   9.1s\n",
      "[CV 1/3; 57/70] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 56/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x12f757f10>;, score=(train=0.535, test=0.481) total time=   9.0s\n",
      "[CV 2/3; 57/70] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 56/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2921b03d0>;, score=(train=0.538, test=0.472) total time=   8.9s\n",
      "[CV 1/3; 57/70] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.751, test=0.474) total time=   1.0s\n",
      "[CV 3/3; 57/70] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 57/70] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.750, test=0.492) total time=   1.0s\n",
      "[CV 1/3; 58/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2be418390>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 58/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x105bf8a50>\n",
      "[CV 3/3; 57/70] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.760, test=0.494) total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 58/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x282d39550>\n",
      "[CV 1/3; 58/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2be418390>;, score=(train=0.536, test=0.472) total time=  19.3s\n",
      "[CV 2/3; 58/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x105bf8a50>;, score=(train=0.529, test=0.484) total time=  19.2s\n",
      "[CV 1/3; 59/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 58/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x282d39550>;, score=(train=0.538, test=0.485) total time=  19.3s\n",
      "[CV 2/3; 59/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 59/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 59/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.538, test=0.467) total time=   3.7s\n",
      "[CV 1/3; 60/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 59/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.538, test=0.497) total time=   3.7s\n",
      "[CV 3/3; 59/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.542, test=0.492) total time=   4.0s\n",
      "[CV 2/3; 60/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 60/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 60/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.538, test=0.466) total time=   5.6s\n",
      "[CV 1/3; 61/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16deba650>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 60/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.532, test=0.480) total time=   5.6s\n",
      "[CV 3/3; 60/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.538, test=0.484) total time=   5.5s\n",
      "[CV 2/3; 61/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2974b1690>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 61/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28fba8690>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 61/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16deba650>;, score=(train=0.545, test=0.471) total time=   6.3s\n",
      "[CV 1/3; 62/70] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 61/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2974b1690>;, score=(train=0.534, test=0.486) total time=   6.7s\n",
      "[CV 1/3; 62/70] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.729, test=0.474) total time=   0.9s\n",
      "[CV 3/3; 61/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28fba8690>;, score=(train=0.541, test=0.487) total time=   6.1s\n",
      "[CV 2/3; 62/70] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 62/70] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 62/70] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.725, test=0.488) total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 63/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16ed94890>\n",
      "[CV 2/3; 63/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16b26a890>\n",
      "[CV 3/3; 62/70] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.739, test=0.491) total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 63/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2d0319650>\n",
      "[CV 2/3; 63/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16b26a890>;, score=(train=0.537, test=0.489) total time=   7.0s\n",
      "[CV 1/3; 63/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16ed94890>;, score=(train=0.536, test=0.473) total time=   7.7s\n",
      "[CV 1/3; 64/70] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 63/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2d0319650>;, score=(train=0.543, test=0.488) total time=   7.0s\n",
      "[CV 2/3; 64/70] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 64/70] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.543, test=0.483) total time=   1.0s\n",
      "[CV 3/3; 64/70] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 64/70] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.543, test=0.497) total time=   0.9s\n",
      "[CV 1/3; 65/70] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x14e21d450>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 65/70] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2aadab5d0>\n",
      "[CV 3/3; 64/70] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.547, test=0.504) total time=   1.0s\n",
      "[CV 3/3; 65/70] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x280e02150>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 65/70] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x14e21d450>;, score=(train=0.543, test=0.482) total time=  14.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 66/70] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16b554050>\n",
      "[CV 2/3; 65/70] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2aadab5d0>;, score=(train=0.538, test=0.499) total time=  15.0s\n",
      "[CV 3/3; 65/70] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x280e02150>;, score=(train=0.545, test=0.502) total time=  14.8s\n",
      "[CV 2/3; 66/70] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16f967450>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 66/70] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2873a2e50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 66/70] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16b554050>;, score=(train=0.542, test=0.485) total time=  14.8s\n",
      "[CV 1/3; 67/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10115e810>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 66/70] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16f967450>;, score=(train=0.535, test=0.501) total time=  14.9s\n",
      "[CV 3/3; 66/70] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2873a2e50>;, score=(train=0.541, test=0.504) total time=  14.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 67/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2ac2340d0>\n",
      "[CV 3/3; 67/70] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11f8d1bd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 67/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10115e810>;, score=(train=0.542, test=0.474) total time=  17.2s\n",
      "[CV 1/3; 68/70] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 67/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2ac2340d0>;, score=(train=0.534, test=0.487) total time=  17.3s\n",
      "[CV 3/3; 67/70] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11f8d1bd0>;, score=(train=0.537, test=0.483) total time=  17.2s\n",
      "[CV 1/3; 68/70] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.542, test=0.486) total time=   0.9s\n",
      "[CV 2/3; 68/70] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 68/70] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 69/70] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2be418150>\n",
      "[CV 2/3; 68/70] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.542, test=0.497) total time=   0.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 68/70] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.545, test=0.505) total time=   0.9s\n",
      "[CV 2/3; 69/70] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15cf69910>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 69/70] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c5b47dd0>\n",
      "[CV 1/3; 69/70] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2be418150>;, score=(train=0.542, test=0.483) total time=   4.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 70/70] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c4705dd0>\n",
      "[CV 2/3; 69/70] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15cf69910>;, score=(train=0.540, test=0.498) total time=   4.4s\n",
      "[CV 3/3; 69/70] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c5b47dd0>;, score=(train=0.547, test=0.502) total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 70/70] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15eb5d390>\n",
      "[CV 3/3; 70/70] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2857d87d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 70/70] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c4705dd0>;, score=(train=0.540, test=0.485) total time=   4.4s\n",
      "[CV 2/3; 70/70] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15eb5d390>;, score=(train=0.538, test=0.500) total time=   4.3s\n",
      "[CV 3/3; 70/70] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2857d87d0>;, score=(train=0.544, test=0.504) total time=   4.2s\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 24\n",
      "n_resources: 37557\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "[CV 1/3; 1/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x13cee14d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 1/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1695ae450>\n",
      "[CV 3/3; 1/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x280849650>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 1/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x13cee14d0>;, score=(train=0.519, test=0.480) total time=  27.0s\n",
      "[CV 1/3; 2/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 1/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1695ae450>;, score=(train=0.519, test=0.483) total time=  27.4s\n",
      "[CV 3/3; 1/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x280849650>;, score=(train=0.520, test=0.479) total time=  27.2s\n",
      "[CV 2/3; 2/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 2/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 2/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.518, test=0.478) total time=  17.3s\n",
      "[CV 1/3; 3/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2b573b7d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 2/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.513, test=0.481) total time=  17.2s\n",
      "[CV 2/3; 3/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x103bc0a50>\n",
      "[CV 3/3; 2/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.523, test=0.489) total time=  17.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 3/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x280849790>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 3/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2b573b7d0>;, score=(train=0.513, test=0.486) total time=  29.8s\n",
      "[CV 1/3; 4/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 3/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x103bc0a50>;, score=(train=0.517, test=0.478) total time=  30.3s\n",
      "[CV 3/3; 3/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x280849790>;, score=(train=0.521, test=0.479) total time=  29.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 4/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 4/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 4/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.519, test=0.484) total time=  18.7s\n",
      "[CV 1/3; 5/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16b26bc10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 4/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.514, test=0.487) total time=  18.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 5/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b5b06f10>\n",
      "[CV 3/3; 4/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.527, test=0.477) total time=  19.1s\n",
      "[CV 3/3; 5/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x29d4b0b90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 5/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16b26bc10>;, score=(train=0.517, test=0.483) total time=  57.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 6/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b54bc550>\n",
      "[CV 2/3; 5/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b5b06f10>;, score=(train=0.521, test=0.486) total time=  58.0s\n",
      "[CV 3/3; 5/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x29d4b0b90>;, score=(train=0.523, test=0.480) total time=  57.5s\n",
      "[CV 2/3; 6/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x287531090>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 6/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2ca084490>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 6/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b54bc550>;, score=(train=0.518, test=0.482) total time=  59.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 7/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2be418050>\n",
      "[CV 2/3; 6/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x287531090>;, score=(train=0.511, test=0.481) total time=  59.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 6/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.01, model__kernel__n_components=1000, model__svm__C=1.0, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2ca084490>;, score=(train=0.523, test=0.479) total time=  59.3s\n",
      "[CV 2/3; 7/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x13bfe9310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 7/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2ab2aff10>\n",
      "[CV 1/3; 7/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2be418050>;, score=(train=0.519, test=0.491) total time=  18.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 8/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a19d6210>\n",
      "[CV 2/3; 7/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x13bfe9310>;, score=(train=0.518, test=0.491) total time=  18.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 8/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x282811ad0>\n",
      "[CV 3/3; 7/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2ab2aff10>;, score=(train=0.524, test=0.486) total time=  18.4s\n",
      "[CV 3/3; 8/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b016f590>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 8/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a19d6210>;, score=(train=0.520, test=0.491) total time=  51.3s\n",
      "[CV 2/3; 8/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x282811ad0>;, score=(train=0.520, test=0.489) total time=  51.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 9/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x168afd290>\n",
      "[CV 3/3; 8/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b016f590>;, score=(train=0.528, test=0.485) total time=  51.5s\n",
      "[CV 2/3; 9/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x287799750>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 9/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a837f010>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 9/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x168afd290>;, score=(train=0.521, test=0.487) total time=  49.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 10/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2a3ce6050>\n",
      "[CV 2/3; 9/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x287799750>;, score=(train=0.521, test=0.492) total time=  49.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 10/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2b7930a10>\n",
      "[CV 3/3; 9/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a837f010>;, score=(train=0.528, test=0.481) total time=  49.4s\n",
      "[CV 3/3; 10/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2a84e1a50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 10/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2a3ce6050>;, score=(train=0.522, test=0.484) total time=  20.4s\n",
      "[CV 1/3; 11/24] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2a4b7b7d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 10/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2b7930a10>;, score=(train=0.521, test=0.489) total time=  20.2s\n",
      "[CV 2/3; 11/24] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28287acd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 10/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2a84e1a50>;, score=(train=0.524, test=0.482) total time=  20.5s\n",
      "[CV 3/3; 11/24] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x11840ae90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 11/24] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2a4b7b7d0>;, score=(train=0.682, test=0.492) total time=  13.1s\n",
      "[CV 1/3; 12/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 11/24] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28287acd0>;, score=(train=0.676, test=0.494) total time=  13.0s\n",
      "[CV 2/3; 12/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 11/24] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x11840ae90>;, score=(train=0.677, test=0.492) total time=  12.9s\n",
      "[CV 3/3; 12/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 12/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.516, test=0.483) total time=   8.3s\n",
      "[CV 1/3; 13/24] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 12/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.521, test=0.484) total time=   8.2s\n",
      "[CV 2/3; 13/24] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 12/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.524, test=0.482) total time=   8.6s\n",
      "[CV 3/3; 13/24] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 13/24] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.693, test=0.492) total time=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 14/24] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16b4378d0>\n",
      "[CV 2/3; 13/24] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.687, test=0.490) total time=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 14/24] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2920bdd90>\n",
      "[CV 3/3; 13/24] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.687, test=0.488) total time=   3.0s\n",
      "[CV 3/3; 14/24] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b349dcd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 14/24] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16b4378d0>;, score=(train=0.666, test=0.493) total time=  45.0s\n",
      "[CV 1/3; 15/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 14/24] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2920bdd90>;, score=(train=0.661, test=0.493) total time=  44.7s\n",
      "[CV 2/3; 15/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 14/24] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b349dcd0>;, score=(train=0.663, test=0.488) total time=  44.5s\n",
      "[CV 3/3; 15/24] START model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 15/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.518, test=0.484) total time=   9.9s\n",
      "[CV 1/3; 16/24] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 15/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.519, test=0.491) total time=  10.1s\n",
      "[CV 2/3; 16/24] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 15/24] END model=Pipeline(steps=[('kernel', RBFSampler(gamma=10, n_components=1000)),\n",
      "                ('svm', LinearSVC())]), model__kernel__gamma=0.1, model__kernel__n_components=1000, model__svm__C=0.1, model__svm__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.523, test=0.487) total time=  10.1s\n",
      "[CV 3/3; 16/24] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 16/24] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.712, test=0.494) total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 17/24] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2a4b7a5d0>\n",
      "[CV 2/3; 16/24] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.709, test=0.492) total time=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 17/24] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x282b97f10>\n",
      "[CV 3/3; 16/24] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.708, test=0.489) total time=   3.2s\n",
      "[CV 3/3; 17/24] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x119e7f8d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 17/24] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2a4b7a5d0>;, score=(train=0.702, test=0.493) total time=  14.5s\n",
      "[CV 2/3; 17/24] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x282b97f10>;, score=(train=0.699, test=0.493) total time=  14.3s\n",
      "[CV 1/3; 18/24] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11ea732d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 18/24] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x291e5ebd0>\n",
      "[CV 3/3; 17/24] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x119e7f8d0>;, score=(train=0.696, test=0.492) total time=  14.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 18/24] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x282e74d10>\n",
      "[CV 1/3; 18/24] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11ea732d0>;, score=(train=0.687, test=0.495) total time=  44.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 19/24] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a3d7ba10>\n",
      "[CV 2/3; 18/24] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x291e5ebd0>;, score=(train=0.684, test=0.495) total time=  45.0s\n",
      "[CV 2/3; 19/24] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2c638fe10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 18/24] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x282e74d10>;, score=(train=0.683, test=0.490) total time=  45.1s\n",
      "[CV 3/3; 19/24] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x119f4e510>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 19/24] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2c638fe10>;, score=(train=0.541, test=0.503) total time=  44.5s\n",
      "[CV 1/3; 19/24] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a3d7ba10>;, score=(train=0.544, test=0.503) total time=  45.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 20/24] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2b754dc90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 20/24] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16b0c66d0>\n",
      "[CV 3/3; 19/24] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x119f4e510>;, score=(train=0.543, test=0.495) total time=  44.6s\n",
      "[CV 3/3; 20/24] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28b08b0d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 20/24] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2b754dc90>;, score=(train=0.546, test=0.502) total time=  12.9s\n",
      "[CV 1/3; 21/24] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 20/24] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16b0c66d0>;, score=(train=0.541, test=0.502) total time=  13.0s\n",
      "[CV 2/3; 21/24] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 20/24] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28b08b0d0>;, score=(train=0.542, test=0.496) total time=  12.8s\n",
      "[CV 3/3; 21/24] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 21/24] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.546, test=0.503) total time=   2.9s\n",
      "[CV 1/3; 22/24] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 21/24] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.543, test=0.501) total time=   2.9s\n",
      "[CV 2/3; 22/24] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 21/24] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.544, test=0.498) total time=   3.0s\n",
      "[CV 1/3; 22/24] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.544, test=0.503) total time=   2.9s\n",
      "[CV 3/3; 22/24] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 22/24] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.543, test=0.501) total time=   2.8s\n",
      "[CV 1/3; 23/24] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16b718a10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 23/24] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16b67d0d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 22/24] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.543, test=0.497) total time=   2.7s\n",
      "[CV 3/3; 23/24] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28d839b10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 23/24] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16b718a10>;, score=(train=0.543, test=0.502) total time=  12.7s\n",
      "[CV 1/3; 24/24] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2c647c210>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 23/24] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16b67d0d0>;, score=(train=0.540, test=0.502) total time=  12.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 24/24] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a53d7ed0>\n",
      "[CV 3/3; 23/24] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28d839b10>;, score=(train=0.542, test=0.496) total time=  13.2s\n",
      "[CV 3/3; 24/24] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2c4889b10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 24/24] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2c647c210>;, score=(train=0.542, test=0.504) total time=  44.4s\n",
      "[CV 2/3; 24/24] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a53d7ed0>;, score=(train=0.539, test=0.503) total time=  45.0s\n",
      "[CV 3/3; 24/24] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2c4889b10>;, score=(train=0.542, test=0.494) total time=  44.2s\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 8\n",
      "n_resources: 112671\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV 1/3; 1/8] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x13c756290>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 1/8] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2a7bca850>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 1/8] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1295fbe10>\n",
      "[CV 1/3; 1/8] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x13c756290>;, score=(train=0.660, test=0.504) total time=  40.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 2/8] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2c5b47f90>\n",
      "[CV 2/3; 1/8] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2a7bca850>;, score=(train=0.661, test=0.511) total time=  40.9s\n",
      "[CV 2/3; 2/8] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x150f9fc50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 1/8] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1295fbe10>;, score=(train=0.659, test=0.509) total time=  40.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 2/8] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x13b9640d0>\n",
      "[CV 1/3; 2/8] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2c5b47f90>;, score=(train=0.646, test=0.502) total time= 2.3min\n",
      "[CV 1/3; 3/8] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1382dd4d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 2/8] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x13b9640d0>;, score=(train=0.647, test=0.511) total time= 2.3min\n",
      "[CV 2/3; 2/8] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x150f9fc50>;, score=(train=0.651, test=0.511) total time= 2.3min\n",
      "[CV 2/3; 3/8] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x168a6d510>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 3/8] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x12205f150>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 3/8] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1382dd4d0>;, score=(train=0.547, test=0.506) total time=  39.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 4/8] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c608ff10>\n",
      "[CV 2/3; 3/8] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x168a6d510>;, score=(train=0.546, test=0.514) total time=  39.1s\n",
      "[CV 3/3; 3/8] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x12205f150>;, score=(train=0.545, test=0.510) total time=  38.8s\n",
      "[CV 2/3; 4/8] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x139cb87d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 4/8] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1508c4290>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 4/8] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c608ff10>;, score=(train=0.547, test=0.507) total time=  38.9s\n",
      "[CV 1/3; 5/8] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x285cb4a50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 4/8] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x139cb87d0>;, score=(train=0.547, test=0.514) total time=  39.2s\n",
      "[CV 3/3; 4/8] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1508c4290>;, score=(train=0.547, test=0.510) total time=  38.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 5/8] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x14e0355d0>\n",
      "[CV 3/3; 5/8] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x12197e010>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 5/8] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x285cb4a50>;, score=(train=0.545, test=0.505) total time= 2.2min\n",
      "[CV 1/3; 6/8] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 5/8] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x14e0355d0>;, score=(train=0.544, test=0.514) total time= 2.2min\n",
      "[CV 2/3; 6/8] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 5/8] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x12197e010>;, score=(train=0.543, test=0.510) total time= 2.3min\n",
      "[CV 3/3; 6/8] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 6/8] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.548, test=0.506) total time=   8.8s\n",
      "[CV 1/3; 7/8] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x291e86c90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 6/8] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.548, test=0.514) total time=   8.8s\n",
      "[CV 2/3; 7/8] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a3475e10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 6/8] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=3, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.547, test=0.511) total time=   8.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 7/8] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16f39ec10>\n",
      "[CV 1/3; 7/8] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x291e86c90>;, score=(train=0.545, test=0.506) total time= 2.2min\n",
      "[CV 1/3; 8/8] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 7/8] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a3475e10>;, score=(train=0.546, test=0.514) total time= 2.2min\n",
      "[CV 2/3; 8/8] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 7/8] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16f39ec10>;, score=(train=0.545, test=0.510) total time= 2.2min\n",
      "[CV 3/3; 8/8] START model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 8/8] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.548, test=0.507) total time=   8.9s\n",
      "[CV 2/3; 8/8] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.549, test=0.514) total time=   9.1s\n",
      "[CV 3/3; 8/8] END model=LinearSVC(), model__C=0.01, model__class_weight=None, preprocessor__text__vectorizer__min_df=1, preprocessor__text__vectorizer__strip_accents=unicode, preprocessor__text__vectorizer__tokenizer=None;, score=(train=0.548, test=0.511) total time=   8.8s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'gridsearch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit und Beruf/Uppsala/Data Mining/uu-data-mining-project/exploration_modeling.ipynb Cell 49\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#Y110sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m kernel_search\u001b[39m.\u001b[39mfit(train_df, train_df[\u001b[39m\"\u001b[39m\u001b[39mcitation_bucket\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#Y110sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m save_model(kernel_search, \u001b[39m\"\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit und Beruf/Uppsala/Data Mining/uu-data-mining-project/exploration_modeling.ipynb Cell 49\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#Y110sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m files \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(folder_path)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#Y110sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# get the largest number in the filenames\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#Y110sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m max_num \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m([\u001b[39mint\u001b[39m(f\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m files \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.pkl\u001b[39m\u001b[39m'\u001b[39m)] \u001b[39m+\u001b[39m [\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#Y110sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# create a filename for the new model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#Y110sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m filename_model \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmax_num\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pkl\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit und Beruf/Uppsala/Data Mining/uu-data-mining-project/exploration_modeling.ipynb Cell 49\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#Y110sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m files \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(folder_path)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#Y110sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# get the largest number in the filenames\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#Y110sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m max_num \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m([\u001b[39mint\u001b[39m(f\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m files \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.pkl\u001b[39m\u001b[39m'\u001b[39m)] \u001b[39m+\u001b[39m [\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#Y110sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# create a filename for the new model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#Y110sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m filename_model \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmax_num\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pkl\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'gridsearch'"
     ]
    }
   ],
   "source": [
    "kernel_search.fit(train_df, train_df[\"citation_bucket\"])\n",
    "save_model(kernel_search, \"models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(kernel_search.cv_results_)\n",
    "results.sort_values(\"rank_test_score\").drop(\n",
    "    [\n",
    "        \"mean_fit_time\",\n",
    "        \"std_fit_time\",\n",
    "        \"mean_score_time\",\n",
    "        \"std_score_time\",\n",
    "        \"split0_train_score\",\n",
    "        \"split1_train_score\",\n",
    "        \"split2_train_score\",\n",
    "        \"mean_train_score\",\n",
    "        \"std_train_score\",\n",
    "        \"split0_test_score\",\n",
    "        \"split1_test_score\",\n",
    "        \"split2_test_score\",\n",
    "        \"param_preprocessor__text__vectorizer__strip_accents\",\n",
    "        \"iter\",\n",
    "        \"n_resources\"\n",
    "    ],\n",
    "    axis=1,\n",
    ").to_csv(\"results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier as Benchmark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Strategy: most_frequent'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Train score: 0.40786920355448275'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Strategy: prior'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Train score: 0.40786920355448275'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Strategy: stratified'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Train score: 0.31360559913859404'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Strategy: uniform'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Train score: 0.25007099499485286'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "strategies = [\"most_frequent\", \"prior\", \"stratified\", \"uniform\"]\n",
    "for s in strategies:\n",
    "    dummy = DummyClassifier(strategy=s)\n",
    "    dummy.fit(train_df, train_df[\"citation_bucket\"])\n",
    "    display(f\"Strategy: {s}\")\n",
    "    display(f\"Train score: {dummy.score(train_df, train_df['citation_bucket'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer(strip_accents=&#x27;unicode&#x27;))]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;model&#x27;, LinearSVC(C=0.01))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-50\" type=\"checkbox\" ><label for=\"sk-estimator-id-50\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer(strip_accents=&#x27;unicode&#x27;))]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;model&#x27;, LinearSVC(C=0.01))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-51\" type=\"checkbox\" ><label for=\"sk-estimator-id-51\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(n_jobs=-1,\n",
       "                  transformers=[(&#x27;ratio&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                  &#x27;author_count&#x27;]),\n",
       "                                (&#x27;date&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                (&#x27;text&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                  TfidfVectorizer(strip_accents=&#x27;unicode&#x27;))]),\n",
       "                                 &#x27;text&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-52\" type=\"checkbox\" ><label for=\"sk-estimator-id-52\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ratio</label><div class=\"sk-toggleable__content\"><pre>[&#x27;page_count&#x27;, &#x27;figure_count&#x27;, &#x27;author_count&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-53\" type=\"checkbox\" ><label for=\"sk-estimator-id-53\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-54\" type=\"checkbox\" ><label for=\"sk-estimator-id-54\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">date</label><div class=\"sk-toggleable__content\"><pre>[&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-55\" type=\"checkbox\" ><label for=\"sk-estimator-id-55\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-56\" type=\"checkbox\" ><label for=\"sk-estimator-id-56\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">text</label><div class=\"sk-toggleable__content\"><pre>text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-57\" type=\"checkbox\" ><label for=\"sk-estimator-id-57\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(strip_accents=&#x27;unicode&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-58\" type=\"checkbox\" ><label for=\"sk-estimator-id-58\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=0.01)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[('ratio',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['page_count', 'figure_count',\n",
       "                                                   'author_count']),\n",
       "                                                 ('date',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['year', 'month', 'day']),\n",
       "                                                 ('text',\n",
       "                                                  Pipeline(steps=[('vectorizer',\n",
       "                                                                   TfidfVectorizer(strip_accents='unicode'))]),\n",
       "                                                  'text')])),\n",
       "                ('model', LinearSVC(C=0.01))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_search = load_model(\"kernel_search\", \"models\")\n",
    "best_model = kernel_search.best_estimator_\n",
    "best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer(strip_accents=&#x27;unicode&#x27;))]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;model&#x27;, LinearSVC(C=0.01))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-59\" type=\"checkbox\" ><label for=\"sk-estimator-id-59\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer(strip_accents=&#x27;unicode&#x27;))]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;model&#x27;, LinearSVC(C=0.01))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-60\" type=\"checkbox\" ><label for=\"sk-estimator-id-60\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(n_jobs=-1,\n",
       "                  transformers=[(&#x27;ratio&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                  &#x27;author_count&#x27;]),\n",
       "                                (&#x27;date&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                (&#x27;text&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                  TfidfVectorizer(strip_accents=&#x27;unicode&#x27;))]),\n",
       "                                 &#x27;text&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-61\" type=\"checkbox\" ><label for=\"sk-estimator-id-61\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ratio</label><div class=\"sk-toggleable__content\"><pre>[&#x27;page_count&#x27;, &#x27;figure_count&#x27;, &#x27;author_count&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-62\" type=\"checkbox\" ><label for=\"sk-estimator-id-62\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-63\" type=\"checkbox\" ><label for=\"sk-estimator-id-63\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">date</label><div class=\"sk-toggleable__content\"><pre>[&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-64\" type=\"checkbox\" ><label for=\"sk-estimator-id-64\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-65\" type=\"checkbox\" ><label for=\"sk-estimator-id-65\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">text</label><div class=\"sk-toggleable__content\"><pre>text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-66\" type=\"checkbox\" ><label for=\"sk-estimator-id-66\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(strip_accents=&#x27;unicode&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-67\" type=\"checkbox\" ><label for=\"sk-estimator-id-67\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=0.01)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[('ratio',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['page_count', 'figure_count',\n",
       "                                                   'author_count']),\n",
       "                                                 ('date',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['year', 'month', 'day']),\n",
       "                                                 ('text',\n",
       "                                                  Pipeline(steps=[('vectorizer',\n",
       "                                                                   TfidfVectorizer(strip_accents='unicode'))]),\n",
       "                                                  'text')])),\n",
       "                ('model', LinearSVC(C=0.01))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on the complete Training Set!\n",
    "best_model.fit(train_df, train_df[\"citation_bucket\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation:\n",
      "Accuracy: 0.5501372569900489\n",
      "F1 Score: 0.5309554185131931\n",
      "Precision: 0.5287080980022668\n",
      "Recall: 0.5501372569900489\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.54      0.47      0.50     93503\n",
      "         low       0.64      0.51      0.57     88088\n",
      "      medium       0.52      0.71      0.60    137881\n",
      "        star       0.00      0.00      0.00     18580\n",
      "\n",
      "    accuracy                           0.55    338052\n",
      "   macro avg       0.43      0.42      0.42    338052\n",
      "weighted avg       0.53      0.55      0.53    338052\n",
      "\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Predict the class labels for the training set\n",
    "train_pred = best_model.predict(train_df)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "print(\"Training Set Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(train_df[\"citation_bucket\"], train_pred))\n",
    "print(\"F1 Score:\", f1_score(train_df[\"citation_bucket\"], train_pred, average=\"weighted\"))\n",
    "print(\"Precision:\", precision_score(train_df[\"citation_bucket\"], train_pred, average=\"weighted\"))\n",
    "print(\"Recall:\", recall_score(train_df[\"citation_bucket\"], train_pred, average=\"weighted\"))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(train_df[\"citation_bucket\"], train_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "labels = [\"low\", \"medium\", \"high\", \"star\"]\n",
    "cm = confusion_matrix(train_df[\"citation_bucket\"], train_pred, normalize='true', labels=labels)\n",
    "dsp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "dsp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation:\n",
      "Accuracy: 0.5207702064688887\n",
      "F1 Score: 0.5022187786662511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.49776518847093537\n",
      "Recall: 0.5207702064688887\n",
      "\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.51      0.39      0.44     14863\n",
      "         low       0.59      0.55      0.57     15816\n",
      "      medium       0.49      0.66      0.56     23080\n",
      "        star       0.00      0.00      0.00      3005\n",
      "\n",
      "    accuracy                           0.52     56764\n",
      "   macro avg       0.40      0.40      0.39     56764\n",
      "weighted avg       0.50      0.52      0.50     56764\n",
      "\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict the class labels for the training set\n",
    "test_df = pd.read_parquet(\"dataset/test.parquet\")\n",
    "test_pred = best_model.predict(test_df)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(test_df[\"citation_bucket\"], test_pred))\n",
    "print(\"F1 Score:\", f1_score(test_df[\"citation_bucket\"], test_pred, average=\"weighted\"))\n",
    "print(\"Precision:\", precision_score(test_df[\"citation_bucket\"], test_pred, average=\"weighted\"))\n",
    "print(\"Recall:\", recall_score(test_df[\"citation_bucket\"], test_pred, average=\"weighted\"))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_df[\"citation_bucket\"], test_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "labels = [\"low\", \"medium\", \"high\", \"star\"]\n",
    "cm = confusion_matrix(test_df[\"citation_bucket\"], test_pred, normalize='true', labels=labels)\n",
    "dsp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "dsp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Side quest: No star bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citation_count</th>\n",
       "      <th>page_count</th>\n",
       "      <th>figure_count</th>\n",
       "      <th>author_count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>text</th>\n",
       "      <th>page_imputed</th>\n",
       "      <th>citation_bucket</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doi</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10.1002/adfm.202001307</th>\n",
       "      <td>33</td>\n",
       "      <td>25.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>Qinghua Zhao, Wanqi Jie, Tao Wang, Andres Cast...</td>\n",
       "      <td>False</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.1002/cphc.200900857</th>\n",
       "      <td>33</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Haifeng Ma, Thomas Brugger, Simon Berner, Yun ...</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.1002/prop.200710532</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>Milovan Vasilic, Marko Vojinovic Interaction o...</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.1007/978-3-030-30493-5_44</th>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>Itay Mosafi, Eli David, Nathan S. Netanyahu De...</td>\n",
       "      <td>True</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.1007/s00025-018-0843-4</th>\n",
       "      <td>24</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>Deepshikha and Lalit K. Vashisht Weaving K-fra...</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              citation_count  page_count  figure_count  \\\n",
       "doi                                                                      \n",
       "10.1002/adfm.202001307                    33        25.0          13.0   \n",
       "10.1002/cphc.200900857                    33        15.0           4.0   \n",
       "10.1002/prop.200710532                     1         5.0           0.0   \n",
       "10.1007/978-3-030-30493-5_44               0        15.0           0.0   \n",
       "10.1007/s00025-018-0843-4                 24        15.0           0.0   \n",
       "\n",
       "                              author_count  year  month  day  \\\n",
       "doi                                                            \n",
       "10.1002/adfm.202001307                   5  2020      7   30   \n",
       "10.1002/cphc.200900857                   8  2010      2    4   \n",
       "10.1002/prop.200710532                   2  2015      5   20   \n",
       "10.1007/978-3-030-30493-5_44             3  2019     12    3   \n",
       "10.1007/s00025-018-0843-4                2  2018      6    8   \n",
       "\n",
       "                                                                           text  \\\n",
       "doi                                                                               \n",
       "10.1002/adfm.202001307        Qinghua Zhao, Wanqi Jie, Tao Wang, Andres Cast...   \n",
       "10.1002/cphc.200900857        Haifeng Ma, Thomas Brugger, Simon Berner, Yun ...   \n",
       "10.1002/prop.200710532        Milovan Vasilic, Marko Vojinovic Interaction o...   \n",
       "10.1007/978-3-030-30493-5_44  Itay Mosafi, Eli David, Nathan S. Netanyahu De...   \n",
       "10.1007/s00025-018-0843-4     Deepshikha and Lalit K. Vashisht Weaving K-fra...   \n",
       "\n",
       "                              page_imputed citation_bucket  \n",
       "doi                                                         \n",
       "10.1002/adfm.202001307               False            high  \n",
       "10.1002/cphc.200900857                True            high  \n",
       "10.1002/prop.200710532               False             low  \n",
       "10.1007/978-3-030-30493-5_44          True             low  \n",
       "10.1007/s00025-018-0843-4             True            high  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Citation Bucket Distribution'}, xlabel='Bucket Name', ylabel='Frequency'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_pd = pd.read_parquet(\"dataset/dataset_cleaned.parquet\")\n",
    "\n",
    "# Define the bins and labels for the buckets\n",
    "bins = [-1, 3, 20, float('inf')]\n",
    "labels = ['low', 'medium', 'high']\n",
    "\n",
    "# Create a new column with the bucket id based on the citation count\n",
    "dataset_pd['citation_bucket'] = pd.cut(dataset_pd['citation_count'], bins=bins, labels=labels)\n",
    "\n",
    "# Print the first 5 rows of the dataset with the new column\n",
    "display(dataset_pd.head())\n",
    "\n",
    "# Sort the value counts by the order of labels\n",
    "sorted_counts = dataset_pd['citation_bucket'].value_counts().reindex(labels)\n",
    "\n",
    "# Plot the sorted value counts\n",
    "sorted_counts.plot(kind='bar', figsize=(10, 5), title='Citation Bucket Distribution', xlabel=\"Bucket Name\", ylabel=\"Frequency\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer(strip_accents=&#x27;unicode&#x27;))]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;model&#x27;, LinearSVC(C=0.01))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-68\" type=\"checkbox\" ><label for=\"sk-estimator-id-68\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer(strip_accents=&#x27;unicode&#x27;))]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;model&#x27;, LinearSVC(C=0.01))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-69\" type=\"checkbox\" ><label for=\"sk-estimator-id-69\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(n_jobs=-1,\n",
       "                  transformers=[(&#x27;ratio&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                  &#x27;author_count&#x27;]),\n",
       "                                (&#x27;date&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                (&#x27;text&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                  TfidfVectorizer(strip_accents=&#x27;unicode&#x27;))]),\n",
       "                                 &#x27;text&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-70\" type=\"checkbox\" ><label for=\"sk-estimator-id-70\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ratio</label><div class=\"sk-toggleable__content\"><pre>[&#x27;page_count&#x27;, &#x27;figure_count&#x27;, &#x27;author_count&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-71\" type=\"checkbox\" ><label for=\"sk-estimator-id-71\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-72\" type=\"checkbox\" ><label for=\"sk-estimator-id-72\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">date</label><div class=\"sk-toggleable__content\"><pre>[&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-73\" type=\"checkbox\" ><label for=\"sk-estimator-id-73\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-74\" type=\"checkbox\" ><label for=\"sk-estimator-id-74\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">text</label><div class=\"sk-toggleable__content\"><pre>text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-75\" type=\"checkbox\" ><label for=\"sk-estimator-id-75\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(strip_accents=&#x27;unicode&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-76\" type=\"checkbox\" ><label for=\"sk-estimator-id-76\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=0.01)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[('ratio',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['page_count', 'figure_count',\n",
       "                                                   'author_count']),\n",
       "                                                 ('date',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['year', 'month', 'day']),\n",
       "                                                 ('text',\n",
       "                                                  Pipeline(steps=[('vectorizer',\n",
       "                                                                   TfidfVectorizer(strip_accents='unicode'))]),\n",
       "                                                  'text')])),\n",
       "                ('model', LinearSVC(C=0.01))])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_star_train, no_star_test = train_test_split(dataset_pd, test_size=0.1, random_state=42, stratify=dataset_pd[\"citation_bucket\"])\n",
    "best_model_no_star = deepcopy(best_model)\n",
    "best_model_no_star.fit(no_star_train, no_star_train[\"citation_bucket\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation:\n",
      "Accuracy: 0.5638432809527165\n",
      "F1 Score: 0.5617987281597766\n",
      "Precision: 0.5880061465523717\n",
      "Recall: 0.5638432809527165\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.69      0.44      0.54     17868\n",
      "         low       0.60      0.56      0.58     15816\n",
      "      medium       0.50      0.67      0.57     23080\n",
      "\n",
      "    accuracy                           0.56     56764\n",
      "   macro avg       0.60      0.55      0.56     56764\n",
      "weighted avg       0.59      0.56      0.56     56764\n",
      "\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict the class labels for the training set\n",
    "no_star_test_pred = best_model.predict(no_star_test)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "print(\"No Star Test Set Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(no_star_test[\"citation_bucket\"], no_star_test_pred))\n",
    "print(\"F1 Score:\", f1_score(no_star_test[\"citation_bucket\"], no_star_test_pred, average=\"weighted\"))\n",
    "print(\"Precision:\", precision_score(no_star_test[\"citation_bucket\"], no_star_test_pred, average=\"weighted\"))\n",
    "print(\"Recall:\", recall_score(no_star_test[\"citation_bucket\"], no_star_test_pred, average=\"weighted\"))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(no_star_test[\"citation_bucket\"], no_star_test_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "labels = [\"low\", \"medium\", \"high\"]\n",
    "cm = confusion_matrix(no_star_test[\"citation_bucket\"], no_star_test_pred, normalize='true', labels=labels)\n",
    "dsp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "dsp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uu-data-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
