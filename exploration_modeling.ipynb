{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tillgrutschus/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import nltk\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import PorterStemmer\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from copy import deepcopy\n",
    "\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "import random\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_count</th>\n",
       "      <th>figure_count</th>\n",
       "      <th>author_count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>text</th>\n",
       "      <th>page_imputed</th>\n",
       "      <th>citation_bucket</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doi</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10.1002/adfm.202001307</th>\n",
       "      <td>25.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>Qinghua Zhao, Wanqi Jie, Tao Wang, Andres Cast...</td>\n",
       "      <td>False</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.1002/cphc.200900857</th>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Haifeng Ma, Thomas Brugger, Simon Berner, Yun ...</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.1002/prop.200710532</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>Milovan Vasilic, Marko Vojinovic Interaction o...</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.1007/978-3-030-30493-5_44</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>Itay Mosafi, Eli David, Nathan S. Netanyahu De...</td>\n",
       "      <td>True</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.1007/s00025-018-0843-4</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>Deepshikha and Lalit K. Vashisht Weaving K-fra...</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              page_count  figure_count  author_count  year  \\\n",
       "doi                                                                          \n",
       "10.1002/adfm.202001307              25.0          13.0             5  2020   \n",
       "10.1002/cphc.200900857              15.0           4.0             8  2010   \n",
       "10.1002/prop.200710532               5.0           0.0             2  2015   \n",
       "10.1007/978-3-030-30493-5_44        15.0           0.0             3  2019   \n",
       "10.1007/s00025-018-0843-4           15.0           0.0             2  2018   \n",
       "\n",
       "                              month  day  \\\n",
       "doi                                        \n",
       "10.1002/adfm.202001307            7   30   \n",
       "10.1002/cphc.200900857            2    4   \n",
       "10.1002/prop.200710532            5   20   \n",
       "10.1007/978-3-030-30493-5_44     12    3   \n",
       "10.1007/s00025-018-0843-4         6    8   \n",
       "\n",
       "                                                                           text  \\\n",
       "doi                                                                               \n",
       "10.1002/adfm.202001307        Qinghua Zhao, Wanqi Jie, Tao Wang, Andres Cast...   \n",
       "10.1002/cphc.200900857        Haifeng Ma, Thomas Brugger, Simon Berner, Yun ...   \n",
       "10.1002/prop.200710532        Milovan Vasilic, Marko Vojinovic Interaction o...   \n",
       "10.1007/978-3-030-30493-5_44  Itay Mosafi, Eli David, Nathan S. Netanyahu De...   \n",
       "10.1007/s00025-018-0843-4     Deepshikha and Lalit K. Vashisht Weaving K-fra...   \n",
       "\n",
       "                              page_imputed citation_bucket  \n",
       "doi                                                         \n",
       "10.1002/adfm.202001307               False            high  \n",
       "10.1002/cphc.200900857                True            high  \n",
       "10.1002/prop.200710532               False             low  \n",
       "10.1007/978-3-030-30493-5_44          True             low  \n",
       "10.1007/s00025-018-0843-4             True            high  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the dataset\n",
    "dataset_pd = pd.read_parquet('dataset/dataset_labeled.parquet')\n",
    "dataset_pd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 0.90\n",
      "Test set size: 0.10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TRAIN_RATIO = 0.9\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "# split the dataset into train and test sets\n",
    "train_df, test_df = train_test_split(\n",
    "    dataset_pd,\n",
    "    test_size=TEST_RATIO,\n",
    "    random_state=42,\n",
    "    stratify=dataset_pd[\"citation_bucket\"],\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(train_df) / len(dataset_pd):.2f}\")\n",
    "print(f\"Test set size: {len(test_df) / len(dataset_pd):.2f}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "sns.countplot(x=\"citation_bucket\", data=train_df, ax=ax[0])\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "ax[0].set_title(\"Train set\")\n",
    "sns.countplot(x=\"citation_bucket\", data=test_df, ax=ax[1])\n",
    "ax[1].set_title(\"Test set\")\n",
    "ax[1].set_ylabel(\"\")\n",
    "\n",
    "filtered_train_df = train_df[train_df[\"page_imputed\"] == False]\n",
    "filtered_test_df = test_df[test_df[\"page_imputed\"] == False]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "sns.countplot(x=\"citation_bucket\", data=filtered_train_df, ax=ax[0])\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "ax[0].set_title(\"Train set\")\n",
    "sns.countplot(x=\"citation_bucket\", data=filtered_test_df, ax=ax[1])\n",
    "ax[1].set_title(\"Test set\")\n",
    "ax[1].set_ylabel(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the individual sets\n",
    "train_df.to_parquet(\"dataset/train.parquet\")\n",
    "test_df.to_parquet(\"dataset/test.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Pipeline\n",
    "\n",
    "The following models / algorithms will be tried:\n",
    " \n",
    " **Models**:\n",
    " - SVM\n",
    " - Multinomial Naive Bayes\n",
    "\n",
    "**Vectorizers**:\n",
    "  - Count Vectorizer -> We will not use the CountVectorizer as the SparseMatrix cannot be scaled; we will use the TF-IDF Vectorizer instead\n",
    "  - TF-IDF Vectorizer\n",
    "\n",
    "**Dimensionality Reduction**:\n",
    "  - Stopword removal\n",
    "  - Lemmatization\n",
    "  - Stemming\n",
    "\n",
    "**Scaling:**\n",
    "  - Standard Scaler\n",
    "\n",
    "**Metrics:**\n",
    "  - Accuracy\n",
    "  - Precision\n",
    "  - Recall\n",
    "  - F1 Score\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "- **Ratio attributes**: [\"page_count\", \"figure_count\", \"author_count\"]\n",
    "- **Date attributes**: [\"year\", \"month\", \"day\"]\n",
    "- **Text attributes**: [\"text\"]\n",
    "- **Label**: [\"citation_bucket\"]\n",
    "\n",
    "[Ratio] -> [StandardScaler] -> [Model]\n",
    "\n",
    "Since the three attributes are actually ordinal (not categorical), we treat them as numerical attributes.\n",
    "\n",
    "[Date] -> [StandardScaler] -> [Model]\n",
    "\n",
    "[Text] -> [Vectorizer (w/ stopword removal, lemmatization, stemming)] -> [Model]\n",
    "\n",
    "[Label] -> [OrdinalEncoder] -> [Model]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;model&#x27;, LinearSVC(loss=&#x27;hinge&#x27;, random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;model&#x27;, LinearSVC(loss=&#x27;hinge&#x27;, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(n_jobs=-1,\n",
       "                  transformers=[(&#x27;ratio&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                  &#x27;author_count&#x27;]),\n",
       "                                (&#x27;date&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                (&#x27;text&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                  TfidfVectorizer())]),\n",
       "                                 &#x27;text&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ratio</label><div class=\"sk-toggleable__content\"><pre>[&#x27;page_count&#x27;, &#x27;figure_count&#x27;, &#x27;author_count&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">date</label><div class=\"sk-toggleable__content\"><pre>[&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">text</label><div class=\"sk-toggleable__content\"><pre>text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(loss=&#x27;hinge&#x27;, random_state=42)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[('ratio',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['page_count', 'figure_count',\n",
       "                                                   'author_count']),\n",
       "                                                 ('date',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['year', 'month', 'day']),\n",
       "                                                 ('text',\n",
       "                                                  Pipeline(steps=[('vectorizer',\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  'text')])),\n",
       "                ('model', LinearSVC(loss='hinge', random_state=42))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratio_columns = [\"page_count\", \"figure_count\", \"author_count\"]\n",
    "date_columns = [\"year\", \"month\", \"day\"]\n",
    "text_columns = \"text\"\n",
    "label_columns = [\"citation_bucket\"]\n",
    "\n",
    "# Ratio\n",
    "ratio_pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Date\n",
    "date_pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Text\n",
    "text_pipe = Pipeline(\n",
    "    [\n",
    "        (\"vectorizer\", TfidfVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Column Transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"ratio\", ratio_pipe, ratio_columns),\n",
    "        (\"date\", date_pipe, date_columns),\n",
    "        (\"text\", text_pipe, text_columns),\n",
    "    ], n_jobs=-1\n",
    ")\n",
    "\n",
    "# Total pipeline\n",
    "model = LinearSVC(\n",
    "    loss=\"hinge\",\n",
    "    C=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", model),\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_model(model, folder_path):\n",
    "    # get a list of all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "    # get the largest number in the filenames\n",
    "    max_num = max([int(f.split('.')[0]) for f in files if f.endswith('.pkl')] + [0])\n",
    "    # create a filename for the new model\n",
    "    filename_model = f\"{max_num + 1}.pkl\"\n",
    "    # save the model and parameters\n",
    "    with open(os.path.join(folder_path, filename_model), 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "def load_model(model_number, folder_path):\n",
    "    # create the file path\n",
    "    file_path = os.path.join(folder_path, str(model_number))\n",
    "    # load the model and parameters from the file\n",
    "    with open(file_path + \".pkl\", 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out imputed data\n",
    "\n",
    "We have noticed that the computation time for the full dataset is too long. Thus, we have filtered out the imputed page counts to reduce computational costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set absolute size: 338052\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_parquet(\"dataset/train.parquet\")\n",
    "train_df = train_df[train_df[\"page_imputed\"] == False]\n",
    "\n",
    "print(f\"Train set absolute size: {len(train_df)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, naive training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = pipeline.fit(train_df, train_df[\"citation_bucket\"])\n",
    "save_model(trained, \"models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5118747512272788"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained.score(val_df, val_df[\"citation_bucket\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a custom text preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'sweetheart', 'have', 'show', 'you', 'how', 'shower']\n",
      "['Hey', 'sweetheart', 'have', 'showed', 'you', 'how', 'shower']\n"
     ]
    }
   ],
   "source": [
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "        return [\n",
    "            self.wnl.lemmatize(t)\n",
    "            for t in nltk.regexp_tokenize(doc, token_pattern)\n",
    "        ]\n",
    "\n",
    "\n",
    "class StemmingTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = nltk.stem.PorterStemmer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "        return [\n",
    "            self.wnl.stem(t)\n",
    "            for t in nltk.regexp_tokenize(doc, token_pattern)\n",
    "        ]\n",
    "\n",
    "test_stemmer = StemmingTokenizer()\n",
    "test_lemmatizer = LemmaTokenizer()\n",
    "print(test_stemmer(\"Hey sweetheart, have I showed you how I shower?\"))\n",
    "print(test_lemmatizer(\"Hey sweetheart, have I showed you how I shower?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.3 s  91.7 ns per loop (mean  std. dev. of 7 runs, 10,000 loops each)\n",
      "9.73 s  28.8 ns per loop (mean  std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit test_stemmer(\"Hey sweetheart, have I showed you how I shower?\")\n",
    "%timeit test_lemmatizer(\"Hey sweetheart, have I showed you how I shower?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_tranf_vec\n",
      "Number of features: 156361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sample from the vocabulary: ['karuppath', 'mansuripur', 'naser', 'mandell', 'ippocratis', 'vidaexpert', '66006', 'piotto', 'jianyu', 'valio', 'semianalytical', 'biagi', 'yatian', 'pentti', 'photocentre', 'mu_f', 'hunar', 'roati', 'quentin', 'delbo', 'microphonics', 'cerne', 'paradigm', 'hypocenter', '4408']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trunc_train_df = train_df[:50000]\n",
    "# Let's compare dim reduction models\n",
    "print(\"baseline_tranf_vec\")\n",
    "baseline_tranf_vec = TfidfVectorizer(strip_accents=\"unicode\")\n",
    "baseline_tranf = baseline_tranf_vec.fit_transform(trunc_train_df[\"text\"])\n",
    "print(f\"Number of features: {baseline_tranf.shape[1]}\")\n",
    "display(f\"Sample from the vocabulary: {random.sample(list(baseline_tranf_vec.vocabulary_.keys()), 25)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 156226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sample from the vocabulary: ['m62', 'cib', 'olfactory', 'rijcke', 'b881', 'genetically', 'kegerreis', 'celestial', 'entropic', '273m', 'eva', '15au', 'oabo', 'applic', 'lyophobic', 'tsuji', 'foliation', '5194', 'caines', 'pseudoconvex', 'shanhe', 'mcid', 'host', 'jhep0505', 'contrastive']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Relative dim reduction: 0.00086'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stop word removal\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "stop_word_vec = TfidfVectorizer(strip_accents=\"unicode\", stop_words=stop_words)\n",
    "stop_word_transf = stop_word_vec.fit_transform(trunc_train_df[\"text\"])\n",
    "print(f\"Number of features: {stop_word_transf.shape[1]}\")\n",
    "display(f\"Sample from the vocabulary: {random.sample(list(stop_word_vec.vocabulary_.keys()), 25)}\")\n",
    "display(f\"Relative dim reduction: {1 - (stop_word_transf.shape[1] / baseline_tranf.shape[1]):.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 53726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sample from the vocabulary: ['l5', 'displacement', 'qmw', 'insulated', 'telemetry', 'pioline', 'implementing', 'axp', 'jamiolkowski', 'concealed', 'semihadronic', 'onodera', '4t', 'chong', '150pc', 'colliding', 'anisotropic', 'expansive', 'pentaquarks', 'cleaning', 'haojing', 'neutralization', 'aisi', 'maro', 'radim']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Relative dim reduction: 0.65640'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemma_vec = TfidfVectorizer(strip_accents=\"unicode\", tokenizer=LemmaTokenizer())\n",
    "lemma_transf = lemma_vec.fit_transform(trunc_train_df[\"text\"])\n",
    "print(f\"Number of features: {lemma_transf.shape[1]}\")\n",
    "display(f\"Sample from the vocabulary: {random.sample(list(lemma_vec.vocabulary_.keys()), 25)}\")\n",
    "display(f\"Relative dim reduction: {1 - (lemma_transf.shape[1] / baseline_tranf.shape[1]):.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 134201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sample from the vocabulary: ['rossier', 'feyzabadi', '136393', 'scarpin', 'gsu', 'he1', 'c70', 'gallego', 'ontivero', 'looser', '5e41', 'ahlborn', '084712', 'photodesorb', 'uzundag', '8054', 'slatteri', 'anheier', 'moncelsi', 'peaker', 'polsar', 'janowiecki', '7006', 'h_q', 'leik']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Relative dim reduction: 0.14172'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stemming\n",
    "stem_vec = TfidfVectorizer(strip_accents=\"unicode\", tokenizer=StemmingTokenizer())\n",
    "stem_transf = stem_vec.fit_transform(trunc_train_df[\"text\"])\n",
    "print(f\"Number of features: {stem_transf.shape[1]}\")\n",
    "display(f\"Sample from the vocabulary: {random.sample(list(stem_vec.vocabulary_.keys()), 25)}\")\n",
    "display(f\"Relative dim reduction: {1 - (stem_transf.shape[1] / baseline_tranf.shape[1]):.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tillgrutschus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "param_grid_svc = {\n",
    "    \"model\": [LinearSVC()],\n",
    "    \"model__C\": [0.1, 1.0, 10.0],\n",
    "    \"model__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "param_grid_mnb = {\n",
    "    \"model\": [MultinomialNB()],\n",
    "    \"model__alpha\": [0.1, 1.0, 10.0],\n",
    "}\n",
    "\n",
    "# param_grid_preprocess_ngram = {\n",
    "#     \"preprocessor__text__vectorizer__ngram_range\": [(1, 2)],\n",
    "#     \"preprocessor__text__vectorizer__max_features\": [1000, 10000],\n",
    "#     \"preprocessor__text__vectorizer__max_df\": [0.5, 0.75, 1.0],\n",
    "#     \"preprocessor__text__vectorizer__strip_accents\": [None, \"unicode\"],\n",
    "# }\n",
    "\n",
    "param_grid_preprocess_max_df = {\n",
    "    \"preprocessor__text__vectorizer__max_df\": [0.5, 0.75, 1.0],\n",
    "    \"preprocessor__text__vectorizer__max_features\": [None, 1000, 10000],\n",
    "    \"preprocessor__text__vectorizer__strip_accents\": [None, \"unicode\"],\n",
    "}\n",
    "\n",
    "param_grid_preprocess_tokenizer = {\n",
    "    \"preprocessor__text__vectorizer__tokenizer\": [\n",
    "        None,\n",
    "        LemmaTokenizer(),\n",
    "        StemmingTokenizer(),\n",
    "    ],\n",
    "    \"preprocessor__text__vectorizer__stop_words\": [None, list(stop_words)],\n",
    "}\n",
    "\n",
    "param_grid_scaler_min_max = {\n",
    "    \"preprocessor__ratio__scaler\": [MinMaxScaler()],\n",
    "    \"preprocessor__date__scaler\": [MinMaxScaler()],\n",
    "}\n",
    "\n",
    "param_grid_scaler_std = {\n",
    "    \"preprocessor__ratio__scaler\": [StandardScaler()],\n",
    "    \"preprocessor__date__scaler\": [StandardScaler()],\n",
    "}\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {**param_grid_svc, **param_grid_scaler_min_max, **param_grid_preprocess_max_df},\n",
    "    {**param_grid_mnb, **param_grid_scaler_min_max, **param_grid_preprocess_max_df},\n",
    "    {**param_grid_svc, **param_grid_scaler_std, **param_grid_preprocess_max_df},\n",
    "    {**param_grid_mnb, **param_grid_scaler_std, **param_grid_preprocess_max_df},\n",
    "    {**param_grid_svc, **param_grid_scaler_min_max, **param_grid_preprocess_tokenizer},\n",
    "    {**param_grid_mnb, **param_grid_scaler_min_max, **param_grid_preprocess_tokenizer},\n",
    "    {**param_grid_svc, **param_grid_scaler_std, **param_grid_preprocess_tokenizer},\n",
    "    {**param_grid_mnb, **param_grid_scaler_std, **param_grid_preprocess_tokenizer},\n",
    "]\n",
    "\n",
    "\n",
    "search = GridSearchCV(pipeline, param_grid=param_grid, cv=3, verbose=10, n_jobs=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n",
      "[CV 1/3; 1/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 1/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 1/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 2/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 2/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 2/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 3/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 3/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 1/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.520 total time=  47.7s\n",
      "[CV 2/3; 1/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.522 total time=  48.0s\n",
      "[CV 3/3; 3/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 3/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=  44.0s\n",
      "[CV 2/3; 3/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=  43.4s\n",
      "[CV 3/3; 1/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.521 total time=  49.9s\n",
      "[CV 1/3; 2/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.520 total time=  49.4s\n",
      "[CV 1/3; 4/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 2/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.522 total time=  49.4s\n",
      "[CV 2/3; 4/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 4/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 2/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.521 total time=  49.4s\n",
      "[CV 1/3; 5/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 5/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 5/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 6/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 3/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=  35.2s\n",
      "[CV 2/3; 6/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 4/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=  35.9s\n",
      "[CV 2/3; 4/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=  36.0s\n",
      "[CV 3/3; 6/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 4/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time=  36.1s\n",
      "[CV 1/3; 7/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 5/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time=  37.5s\n",
      "[CV 2/3; 7/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 7/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 5/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.516 total time=  38.6s\n",
      "[CV 3/3; 5/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time=  37.8s\n",
      "[CV 1/3; 8/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 6/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.514 total time=  38.2s\n",
      "[CV 2/3; 8/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 8/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 6/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.516 total time=  38.7s\n",
      "[CV 1/3; 9/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 6/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.515 total time=  39.9s\n",
      "[CV 2/3; 9/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 7/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.520 total time=  42.2s\n",
      "[CV 3/3; 9/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 7/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.521 total time=  41.6s\n",
      "[CV 2/3; 7/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.523 total time=  42.9s\n",
      "[CV 1/3; 10/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 10/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 8/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.520 total time=  42.9s\n",
      "[CV 2/3; 8/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.523 total time=  43.0s\n",
      "[CV 3/3; 10/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 8/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.521 total time=  43.1s\n",
      "[CV 1/3; 11/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 11/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 9/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=  35.0s\n",
      "[CV 3/3; 11/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 9/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=  34.9s\n",
      "[CV 1/3; 12/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 9/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=  35.8s\n",
      "[CV 2/3; 12/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 10/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=  35.8s\n",
      "[CV 3/3; 12/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 10/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=  35.8s\n",
      "[CV 1/3; 13/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 10/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time=  36.0s\n",
      "[CV 2/3; 13/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 11/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time=  36.9s\n",
      "[CV 2/3; 11/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.517 total time=  37.6s\n",
      "[CV 3/3; 13/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 14/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 11/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time=  37.9s\n",
      "[CV 2/3; 14/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 12/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.515 total time=  38.5s\n",
      "[CV 3/3; 14/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 12/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.516 total time=  38.5s\n",
      "[CV 1/3; 15/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 12/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.515 total time=  39.0s\n",
      "[CV 2/3; 15/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 13/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.520 total time=  41.4s\n",
      "[CV 2/3; 13/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.523 total time=  42.3s\n",
      "[CV 3/3; 15/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 16/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 13/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.521 total time=  40.9s\n",
      "[CV 1/3; 14/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.520 total time=  40.5s\n",
      "[CV 2/3; 16/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 16/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 14/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.523 total time=  39.2s\n",
      "[CV 1/3; 17/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 15/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.513 total time=  35.7s\n",
      "[CV 3/3; 14/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.521 total time=  39.2s\n",
      "[CV 2/3; 17/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 17/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 15/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.513 total time=  36.2s\n",
      "[CV 1/3; 18/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 15/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=  36.6s\n",
      "[CV 2/3; 18/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 16/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.513 total time=  36.2s\n",
      "[CV 2/3; 16/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.513 total time=  36.2s\n",
      "[CV 3/3; 18/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 16/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=  35.9s\n",
      "[CV 1/3; 19/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 19/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 17/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time=  38.1s\n",
      "[CV 3/3; 19/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 17/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.517 total time=  38.3s\n",
      "[CV 3/3; 17/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.516 total time=  38.1s\n",
      "[CV 1/3; 20/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 20/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 18/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.515 total time=  38.8s\n",
      "[CV 3/3; 20/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 18/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.517 total time=  39.6s\n",
      "[CV 1/3; 21/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 18/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.516 total time=  39.8s\n",
      "[CV 2/3; 21/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 19/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.493 total time=  44.3s\n",
      "[CV 2/3; 19/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.493 total time=  44.5s\n",
      "[CV 3/3; 21/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 22/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 19/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.492 total time=  45.9s\n",
      "[CV 2/3; 22/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 20/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.493 total time=  45.4s\n",
      "[CV 2/3; 20/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.493 total time=  45.3s\n",
      "[CV 3/3; 22/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 23/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 20/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.492 total time=  45.8s\n",
      "[CV 2/3; 23/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 21/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=  42.0s\n",
      "[CV 3/3; 23/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 21/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time=  41.5s\n",
      "[CV 1/3; 24/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 21/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=  40.6s\n",
      "[CV 2/3; 24/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 22/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=  39.1s\n",
      "[CV 3/3; 24/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 22/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time=  38.7s\n",
      "[CV 1/3; 25/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 22/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time=  38.2s\n",
      "[CV 2/3; 25/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 23/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.477 total time=  40.6s\n",
      "[CV 3/3; 25/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 23/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.479 total time=  41.1s\n",
      "[CV 1/3; 26/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 23/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.478 total time=  40.9s\n",
      "[CV 2/3; 26/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 24/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time=  41.2s\n",
      "[CV 3/3; 26/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 24/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.479 total time=  40.8s\n",
      "[CV 1/3; 27/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 24/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time=  40.4s\n",
      "[CV 2/3; 27/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 25/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.493 total time=  44.5s\n",
      "[CV 3/3; 27/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 25/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.494 total time=  44.2s\n",
      "[CV 1/3; 28/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 25/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.491 total time=  43.7s\n",
      "[CV 2/3; 28/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 26/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.493 total time=  44.6s\n",
      "[CV 3/3; 28/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 26/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.494 total time=  44.6s\n",
      "[CV 1/3; 29/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 27/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=  41.6s\n",
      "[CV 3/3; 26/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.492 total time=  44.6s\n",
      "[CV 2/3; 27/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=  40.8s\n",
      "[CV 2/3; 29/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 29/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 30/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 27/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time=  38.8s\n",
      "[CV 2/3; 30/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 28/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=  38.9s\n",
      "[CV 3/3; 30/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 28/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=  38.7s\n",
      "[CV 1/3; 31/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 28/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time=  39.0s\n",
      "[CV 2/3; 31/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 29/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.478 total time=  40.9s\n",
      "[CV 3/3; 31/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 29/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.479 total time=  40.2s\n",
      "[CV 3/3; 29/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.478 total time=  40.0s\n",
      "[CV 1/3; 32/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 30/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time=  40.6s\n",
      "[CV 2/3; 32/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 32/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 30/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.479 total time=  42.1s\n",
      "[CV 1/3; 33/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 30/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time=  42.2s\n",
      "[CV 2/3; 33/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 31/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.494 total time=  44.9s\n",
      "[CV 3/3; 33/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 31/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.494 total time=  44.5s\n",
      "[CV 1/3; 34/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 31/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.492 total time=  45.5s\n",
      "[CV 2/3; 34/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 32/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.494 total time=  45.3s\n",
      "[CV 2/3; 32/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.494 total time=  45.5s\n",
      "[CV 3/3; 34/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 32/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.492 total time=  45.3s\n",
      "[CV 1/3; 35/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 35/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 33/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=  39.9s\n",
      "[CV 3/3; 35/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 33/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=  40.0s\n",
      "[CV 1/3; 36/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 33/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=  39.9s\n",
      "[CV 2/3; 36/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 34/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=  39.8s\n",
      "[CV 3/3; 36/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 34/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=  39.7s\n",
      "[CV 1/3; 37/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 34/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=  39.8s\n",
      "[CV 2/3; 37/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 35/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.478 total time=  41.0s\n",
      "[CV 2/3; 35/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.480 total time=  41.3s\n",
      "[CV 3/3; 37/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 38/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 35/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.479 total time=  42.8s\n",
      "[CV 2/3; 38/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 36/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.479 total time=  44.3s\n",
      "[CV 3/3; 38/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 36/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.480 total time=  45.0s\n",
      "[CV 1/3; 39/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 36/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time=  44.8s\n",
      "[CV 2/3; 39/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 37/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.500 total time= 1.7min\n",
      "[CV 3/3; 39/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 37/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.501 total time= 1.7min\n",
      "[CV 1/3; 38/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.500 total time= 1.7min\n",
      "[CV 1/3; 40/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 37/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.501 total time= 1.8min\n",
      "[CV 2/3; 40/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 40/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 38/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.501 total time= 1.7min\n",
      "[CV 1/3; 41/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 39/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 1.7min\n",
      "[CV 2/3; 41/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 38/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.501 total time= 1.8min\n",
      "[CV 3/3; 41/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 39/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 1.7min\n",
      "[CV 1/3; 42/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 39/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 1.6min\n",
      "[CV 2/3; 42/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 40/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 1.6min\n",
      "[CV 2/3; 40/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 1.5min\n",
      "[CV 3/3; 42/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 43/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 40/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 1.5min\n",
      "[CV 2/3; 43/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 41/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time= 1.6min\n",
      "[CV 3/3; 43/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 41/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.508 total time= 1.6min\n",
      "[CV 1/3; 44/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 42/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time= 1.6min\n",
      "[CV 3/3; 41/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time= 1.6min\n",
      "[CV 2/3; 44/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 44/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 42/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.508 total time= 1.7min\n",
      "[CV 1/3; 45/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 43/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.500 total time= 1.7min\n",
      "[CV 2/3; 45/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 42/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time= 1.8min\n",
      "[CV 2/3; 43/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.501 total time= 1.7min\n",
      "[CV 3/3; 45/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 46/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 43/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.502 total time= 1.8min\n",
      "[CV 2/3; 46/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 44/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.500 total time= 1.8min\n",
      "[CV 3/3; 46/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 44/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.501 total time= 1.8min\n",
      "[CV 3/3; 44/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.502 total time= 1.8min\n",
      "[CV 1/3; 47/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 47/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 45/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 1.6min\n",
      "[CV 3/3; 47/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 45/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 1.6min\n",
      "[CV 1/3; 48/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 45/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 1.6min\n",
      "[CV 2/3; 48/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 46/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 1.6min\n",
      "[CV 3/3; 48/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 46/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 1.6min\n",
      "[CV 1/3; 49/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 46/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 1.6min\n",
      "[CV 2/3; 49/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 47/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.509 total time= 1.6min\n",
      "[CV 1/3; 47/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time= 1.6min\n",
      "[CV 3/3; 49/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 50/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 47/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time= 1.7min\n",
      "[CV 2/3; 50/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 48/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time= 1.8min\n",
      "[CV 2/3; 48/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.509 total time= 1.7min\n",
      "[CV 3/3; 50/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 51/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 48/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time= 1.7min\n",
      "[CV 2/3; 51/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 49/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.501 total time= 1.8min\n",
      "[CV 3/3; 51/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 49/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.502 total time= 1.8min\n",
      "[CV 1/3; 52/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 50/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.501 total time= 1.8min\n",
      "[CV 2/3; 52/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 49/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.502 total time= 1.9min\n",
      "[CV 3/3; 52/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 50/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.502 total time= 1.8min\n",
      "[CV 1/3; 53/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 51/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 1.7min\n",
      "[CV 2/3; 53/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 50/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.502 total time= 1.8min\n",
      "[CV 2/3; 51/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 1.7min\n",
      "[CV 3/3; 53/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 54/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 51/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 1.7min\n",
      "[CV 2/3; 54/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 52/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 1.7min\n",
      "[CV 3/3; 54/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 52/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 1.7min\n",
      "[CV 1/3; 55/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 52/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 1.7min\n",
      "[CV 2/3; 55/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 53/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time= 1.8min\n",
      "[CV 3/3; 55/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 53/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.509 total time= 1.7min\n",
      "[CV 1/3; 56/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 54/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time= 1.8min\n",
      "[CV 3/3; 53/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time= 1.8min\n",
      "[CV 2/3; 56/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 56/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 54/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.509 total time= 1.8min\n",
      "[CV 1/3; 57/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 54/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time= 1.9min\n",
      "[CV 2/3; 57/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 55/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.483 total time= 2.2min\n",
      "[CV 3/3; 57/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 55/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.485 total time= 2.1min\n",
      "[CV 1/3; 58/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 55/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.484 total time= 2.0min\n",
      "[CV 2/3; 58/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 56/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.483 total time= 2.0min\n",
      "[CV 3/3; 58/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 56/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.485 total time= 2.0min\n",
      "[CV 1/3; 59/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 56/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.484 total time= 2.0min\n",
      "[CV 2/3; 59/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 57/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time= 2.2min\n",
      "[CV 1/3; 57/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 2.3min\n",
      "[CV 3/3; 59/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 60/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 57/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time= 2.2min\n",
      "[CV 2/3; 60/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 58/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 2.2min\n",
      "[CV 3/3; 60/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 58/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time= 2.1min\n",
      "[CV 1/3; 61/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 59/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time= 2.1min\n",
      "[CV 2/3; 59/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 2.0min\n",
      "[CV 2/3; 61/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 58/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time= 2.2min\n",
      "[CV 3/3; 61/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 62/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 59/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time= 2.1min\n",
      "[CV 1/3; 60/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time= 2.1min\n",
      "[CV 2/3; 62/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 62/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 60/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 2.2min\n",
      "[CV 1/3; 63/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 60/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time= 2.1min\n",
      "[CV 2/3; 63/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 61/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.484 total time= 2.1min\n",
      "[CV 3/3; 63/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 61/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.485 total time= 2.1min\n",
      "[CV 1/3; 64/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 61/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.484 total time= 2.1min\n",
      "[CV 2/3; 64/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 62/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.484 total time= 2.1min\n",
      "[CV 3/3; 64/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 62/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.485 total time= 2.0min\n",
      "[CV 1/3; 65/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 62/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.484 total time= 2.0min\n",
      "[CV 2/3; 65/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 63/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time= 2.3min\n",
      "[CV 3/3; 65/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 63/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 2.2min\n",
      "[CV 1/3; 66/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 63/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time= 2.3min\n",
      "[CV 2/3; 66/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 64/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time= 2.2min\n",
      "[CV 2/3; 64/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 2.2min\n",
      "[CV 3/3; 66/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 67/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 64/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time= 2.2min\n",
      "[CV 2/3; 67/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 65/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time= 2.1min\n",
      "[CV 3/3; 67/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 65/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 2.1min\n",
      "[CV 1/3; 68/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 65/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time= 2.2min\n",
      "[CV 2/3; 68/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 66/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time= 2.2min\n",
      "[CV 3/3; 68/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 66/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 2.3min\n",
      "[CV 1/3; 69/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 67/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.485 total time= 2.2min\n",
      "[CV 2/3; 69/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 67/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.486 total time= 2.1min\n",
      "[CV 3/3; 69/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 66/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time= 2.3min\n",
      "[CV 1/3; 70/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 67/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.484 total time= 2.1min\n",
      "[CV 2/3; 70/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 68/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.485 total time= 2.1min\n",
      "[CV 3/3; 70/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 68/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.486 total time= 2.0min\n",
      "[CV 1/3; 71/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 68/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.484 total time= 2.0min\n",
      "[CV 2/3; 71/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 69/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time= 2.3min\n",
      "[CV 3/3; 71/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 69/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 2.3min\n",
      "[CV 1/3; 72/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 69/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 2.3min\n",
      "[CV 1/3; 70/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time= 2.3min\n",
      "[CV 2/3; 72/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 72/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 70/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 2.3min\n",
      "[CV 1/3; 73/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 70/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 2.3min\n",
      "[CV 2/3; 73/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 71/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time= 2.1min\n",
      "[CV 3/3; 73/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 71/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time= 2.1min\n",
      "[CV 1/3; 74/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 71/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time= 2.3min\n",
      "[CV 2/3; 74/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 72/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time= 2.3min\n",
      "[CV 3/3; 72/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time= 2.3min\n",
      "[CV 3/3; 74/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 75/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 72/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time= 2.4min\n",
      "[CV 2/3; 75/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 73/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time= 9.1min\n",
      "[CV 3/3; 75/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 73/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 9.0min\n",
      "[CV 1/3; 76/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 73/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time= 8.7min\n",
      "[CV 2/3; 76/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 74/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.471 total time= 8.9min\n",
      "[CV 3/3; 76/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 74/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 9.0min\n",
      "[CV 1/3; 77/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 74/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.471 total time= 9.1min\n",
      "[CV 2/3; 77/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 75/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=10.0min\n",
      "[CV 3/3; 77/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 75/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=10.0min\n",
      "[CV 1/3; 78/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 75/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 9.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 78/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 76/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 9.5min\n",
      "[CV 2/3; 76/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 9.5min\n",
      "[CV 3/3; 78/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 79/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 76/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 9.5min\n",
      "[CV 2/3; 79/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 77/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.503 total time=10.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 79/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 77/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=10.2min\n",
      "[CV 1/3; 80/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 77/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.504 total time=10.4min\n",
      "[CV 1/3; 78/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=10.3min\n",
      "[CV 2/3; 80/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 80/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 79/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time= 9.0min\n",
      "[CV 1/3; 81/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 79/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 8.9min\n",
      "[CV 2/3; 81/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 78/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=10.5min\n",
      "[CV 3/3; 81/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 78/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.504 total time=10.6min\n",
      "[CV 1/3; 82/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 79/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time= 8.8min\n",
      "[CV 2/3; 82/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 80/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.471 total time= 9.1min\n",
      "[CV 3/3; 82/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 80/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 8.7min\n",
      "[CV 1/3; 83/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 80/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.471 total time= 9.0min\n",
      "[CV 2/3; 83/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 81/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 9.7min\n",
      "[CV 2/3; 81/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 9.8min\n",
      "[CV 3/3; 83/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 84/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 81/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 9.8min\n",
      "[CV 2/3; 84/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 82/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 9.7min\n",
      "[CV 3/3; 84/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 82/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 9.8min\n",
      "[CV 1/3; 85/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 82/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 9.8min\n",
      "[CV 2/3; 85/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 83/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.503 total time=10.0min\n",
      "[CV 3/3; 85/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 83/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=10.1min\n",
      "[CV 1/3; 86/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 83/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.503 total time=10.7min\n",
      "[CV 2/3; 86/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 84/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=10.7min\n",
      "[CV 3/3; 86/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 85/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.472 total time= 9.7min\n",
      "[CV 1/3; 87/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 84/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=10.8min\n",
      "[CV 2/3; 87/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 85/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time= 9.7min\n",
      "[CV 3/3; 84/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=10.8min\n",
      "[CV 3/3; 87/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 88/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 85/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.472 total time= 9.3min\n",
      "[CV 2/3; 88/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 86/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.472 total time= 9.5min\n",
      "[CV 3/3; 88/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 86/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.472 total time= 9.0min\n",
      "[CV 1/3; 89/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 86/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time= 9.2min\n",
      "[CV 2/3; 89/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 87/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=10.6min\n",
      "[CV 3/3; 89/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 87/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.513 total time=10.6min\n",
      "[CV 1/3; 90/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 87/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=10.6min\n",
      "[CV 1/3; 88/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=10.6min\n",
      "[CV 2/3; 90/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 90/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 88/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.513 total time=10.6min\n",
      "[CV 1/3; 91/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 88/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time=10.7min\n",
      "[CV 2/3; 91/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 89/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.503 total time=10.9min\n",
      "[CV 3/3; 91/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 89/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=10.9min\n",
      "[CV 1/3; 92/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 91/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time= 9.3min\n",
      "[CV 2/3; 92/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 91/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time= 8.9min\n",
      "[CV 3/3; 92/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 89/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.503 total time=11.0min\n",
      "[CV 1/3; 93/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 90/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=11.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 93/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 90/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=11.0min\n",
      "[CV 3/3; 90/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=11.0min\n",
      "[CV 3/3; 93/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 94/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 91/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time= 9.4min\n",
      "[CV 2/3; 94/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 92/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time= 9.4min\n",
      "[CV 3/3; 94/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 92/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time= 8.5min\n",
      "[CV 1/3; 95/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 92/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time= 8.4min\n",
      "[CV 2/3; 95/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 93/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.463 total time=10.3min\n",
      "[CV 3/3; 95/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 93/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=10.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 96/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 93/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=10.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 96/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 94/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=10.4min\n",
      "[CV 3/3; 96/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 94/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=10.2min\n",
      "[CV 1/3; 97/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 94/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time=10.2min\n",
      "[CV 2/3; 97/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 95/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.464 total time=10.8min\n",
      "[CV 3/3; 97/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 95/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.466 total time=10.9min\n",
      "[CV 1/3; 98/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 95/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.464 total time=10.9min\n",
      "[CV 2/3; 98/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 96/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.464 total time=11.0min\n",
      "[CV 2/3; 96/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time=10.8min\n",
      "[CV 3/3; 98/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 99/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 96/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time=11.2min\n",
      "[CV 2/3; 99/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 97/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time= 9.3min\n",
      "[CV 3/3; 99/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 97/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time= 9.8min\n",
      "[CV 1/3; 100/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 97/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time= 9.0min\n",
      "[CV 2/3; 100/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 98/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time= 9.0min\n",
      "[CV 3/3; 100/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 98/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time= 9.0min\n",
      "[CV 1/3; 101/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 98/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time= 9.3min\n",
      "[CV 2/3; 101/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 99/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=10.7min\n",
      "[CV 3/3; 101/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 99/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time=10.8min\n",
      "[CV 1/3; 102/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 99/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time=10.5min\n",
      "[CV 2/3; 102/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 100/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=10.5min\n",
      "[CV 3/3; 102/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 100/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=10.5min\n",
      "[CV 1/3; 103/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 100/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time=10.6min\n",
      "[CV 2/3; 103/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 101/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.464 total time=10.9min\n",
      "[CV 3/3; 103/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 101/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=11.1min\n",
      "[CV 1/3; 104/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 101/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.464 total time=11.2min\n",
      "[CV 2/3; 104/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 102/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.464 total time=11.3min\n",
      "[CV 3/3; 104/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 103/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.466 total time= 9.6min\n",
      "[CV 1/3; 105/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 102/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time=11.5min\n",
      "[CV 2/3; 105/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 103/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.466 total time= 9.9min\n",
      "[CV 3/3; 105/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 102/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.464 total time=11.8min\n",
      "[CV 1/3; 106/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 103/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.466 total time=10.0min\n",
      "[CV 2/3; 106/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 104/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time= 9.8min\n",
      "[CV 3/3; 106/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 104/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time=10.0min\n",
      "[CV 1/3; 107/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 104/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time= 9.7min\n",
      "[CV 2/3; 107/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 105/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=11.4min\n",
      "[CV 3/3; 107/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 105/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=11.5min\n",
      "[CV 1/3; 108/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 105/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time=11.3min\n",
      "[CV 2/3; 108/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 106/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=11.0min\n",
      "[CV 3/3; 108/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 106/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=11.0min\n",
      "[CV 1/3; 109/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 106/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=11.1min\n",
      "[CV 2/3; 109/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 109/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.451 total time=  34.5s\n",
      "[CV 3/3; 109/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 109/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.450 total time=  33.6s\n",
      "[CV 1/3; 110/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 109/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.450 total time=  30.5s\n",
      "[CV 2/3; 110/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 110/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.451 total time=  37.6s\n",
      "[CV 3/3; 110/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 110/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.450 total time=  36.4s\n",
      "[CV 1/3; 111/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 107/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.464 total time=11.0min\n",
      "[CV 2/3; 111/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 110/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.450 total time=  31.3s\n",
      "[CV 1/3; 111/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  28.4s\n",
      "[CV 3/3; 111/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 112/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 107/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=10.9min\n",
      "[CV 2/3; 112/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 111/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.443 total time=  29.2s\n",
      "[CV 3/3; 112/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 111/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.444 total time=  29.8s\n",
      "[CV 1/3; 112/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  29.5s\n",
      "[CV 1/3; 113/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 113/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 112/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.443 total time=  34.0s\n",
      "[CV 3/3; 113/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 112/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.444 total time=  32.3s\n",
      "[CV 1/3; 114/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 113/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.437 total time=  33.4s\n",
      "[CV 2/3; 113/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.435 total time=  33.2s\n",
      "[CV 2/3; 114/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 114/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 113/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.435 total time=  29.1s\n",
      "[CV 1/3; 115/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 114/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.437 total time=  29.5s\n",
      "[CV 2/3; 115/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 114/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.435 total time=  29.4s\n",
      "[CV 3/3; 114/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.435 total time=  30.0s\n",
      "[CV 3/3; 115/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 116/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 115/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.451 total time=  29.6s\n",
      "[CV 2/3; 116/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 115/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.450 total time=  30.4s\n",
      "[CV 3/3; 116/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 115/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.450 total time=  33.1s\n",
      "[CV 1/3; 116/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.451 total time=  33.1s\n",
      "[CV 1/3; 117/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 117/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 116/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.450 total time=  34.0s\n",
      "[CV 3/3; 117/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 116/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.450 total time=  32.5s\n",
      "[CV 1/3; 118/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 117/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  28.2s\n",
      "[CV 2/3; 118/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 117/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.444 total time=  27.0s\n",
      "[CV 3/3; 118/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 117/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.444 total time=  27.6s\n",
      "[CV 1/3; 119/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 118/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  27.9s\n",
      "[CV 2/3; 119/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 118/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.444 total time=  27.8s\n",
      "[CV 3/3; 119/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 118/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.444 total time=  28.1s\n",
      "[CV 1/3; 120/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 119/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.437 total time=  29.1s\n",
      "[CV 2/3; 120/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 119/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.435 total time=  29.1s\n",
      "[CV 3/3; 120/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 119/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.435 total time=  29.4s\n",
      "[CV 1/3; 121/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 120/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.437 total time=  29.3s\n",
      "[CV 2/3; 121/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 120/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.435 total time=  34.3s\n",
      "[CV 3/3; 121/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 120/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.435 total time=  34.1s\n",
      "[CV 1/3; 121/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.452 total time=  34.8s\n",
      "[CV 1/3; 122/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 121/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.450 total time=  34.2s\n",
      "[CV 2/3; 122/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 122/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 121/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.450 total time=  28.1s\n",
      "[CV 1/3; 123/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 122/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.452 total time=  27.7s\n",
      "[CV 2/3; 123/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 122/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.450 total time=  27.7s\n",
      "[CV 3/3; 122/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.450 total time=  27.7s\n",
      "[CV 3/3; 123/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 124/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 123/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.448 total time=  28.8s\n",
      "[CV 2/3; 124/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 107/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.464 total time=10.8min\n",
      "[CV 2/3; 123/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  29.7s\n",
      "[CV 3/3; 123/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.447 total time=  28.9s\n",
      "[CV 3/3; 124/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 124/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.448 total time=  29.3s\n",
      "[CV 1/3; 125/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 125/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 125/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 124/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  30.8s\n",
      "[CV 1/3; 126/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 124/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.447 total time=  32.1s\n",
      "[CV 1/3; 125/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.438 total time=  32.2s\n",
      "[CV 2/3; 126/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 125/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.436 total time=  32.7s\n",
      "[CV 3/3; 126/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 108/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.464 total time=10.8min\n",
      "[CV 3/3; 125/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.435 total time=  31.9s\n",
      "[CV 1/3; 127/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 127/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 127/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 126/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.438 total time=  29.4s\n",
      "[CV 2/3; 108/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time=10.9min\n",
      "[CV 1/3; 128/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 128/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 108/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.464 total time=10.7min\n",
      "[CV 2/3; 126/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.436 total time=  29.6s\n",
      "[CV 3/3; 126/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.435 total time=  29.3s\n",
      "[CV 3/3; 128/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 129/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 127/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.448 total time=  30.2s\n",
      "[CV 2/3; 129/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 127/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.447 total time=  30.6s\n",
      "[CV 3/3; 127/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.448 total time=  30.0s\n",
      "[CV 3/3; 129/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 130/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 130/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 128/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.448 total time=  29.5s\n",
      "[CV 3/3; 130/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 128/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.447 total time=  30.0s\n",
      "[CV 1/3; 131/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 129/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  32.9s\n",
      "[CV 3/3; 128/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.448 total time=  34.2s\n",
      "[CV 2/3; 131/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 129/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.443 total time=  32.9s\n",
      "[CV 3/3; 129/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.444 total time=  33.1s\n",
      "[CV 1/3; 130/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  31.4s\n",
      "[CV 3/3; 131/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 132/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 130/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.443 total time=  32.0s\n",
      "[CV 2/3; 132/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 132/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 133/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 130/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.444 total time=  32.0s\n",
      "[CV 2/3; 133/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 131/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.437 total time=  32.6s\n",
      "[CV 3/3; 133/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 131/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.435 total time=  28.2s\n",
      "[CV 3/3; 131/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.435 total time=  27.9s\n",
      "[CV 1/3; 134/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 132/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.437 total time=  28.0s\n",
      "[CV 2/3; 134/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 132/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.435 total time=  27.4s\n",
      "[CV 3/3; 134/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 132/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.435 total time=  27.5s\n",
      "[CV 1/3; 135/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 133/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.448 total time=  27.7s\n",
      "[CV 2/3; 135/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 135/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 133/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.447 total time=  26.4s\n",
      "[CV 1/3; 136/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 133/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.448 total time=  26.6s\n",
      "[CV 2/3; 136/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 134/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.448 total time=  26.1s\n",
      "[CV 2/3; 134/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.447 total time=  25.8s\n",
      "[CV 3/3; 136/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 135/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  24.5s\n",
      "[CV 3/3; 134/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.448 total time=  25.8s\n",
      "[CV 1/3; 137/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 135/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.444 total time=  24.4s\n",
      "[CV 2/3; 137/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 137/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 135/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.444 total time=  24.7s\n",
      "[CV 1/3; 138/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 138/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 136/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  23.9s\n",
      "[CV 3/3; 138/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 136/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.444 total time=  24.0s\n",
      "[CV 1/3; 139/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 136/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.444 total time=  24.3s\n",
      "[CV 1/3; 137/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.437 total time=  24.8s\n",
      "[CV 2/3; 137/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.436 total time=  24.8s\n",
      "[CV 2/3; 139/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 137/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.435 total time=  25.2s\n",
      "[CV 3/3; 139/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 138/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.437 total time=  24.6s\n",
      "[CV 1/3; 140/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 138/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.436 total time=  24.6s\n",
      "[CV 2/3; 140/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 140/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 141/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 138/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.435 total time=  24.7s\n",
      "[CV 2/3; 141/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 139/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.448 total time=  23.9s\n",
      "[CV 3/3; 141/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 139/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  24.5s\n",
      "[CV 3/3; 139/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.447 total time=  24.6s\n",
      "[CV 1/3; 142/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 140/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.448 total time=  24.7s\n",
      "[CV 2/3; 142/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 142/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 140/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  25.5s\n",
      "[CV 1/3; 143/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 141/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.448 total time=  24.0s\n",
      "[CV 3/3; 140/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.447 total time=  25.8s\n",
      "[CV 2/3; 143/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 143/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 141/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  24.3s\n",
      "[CV 3/3; 141/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.447 total time=  23.7s\n",
      "[CV 1/3; 144/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 144/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 142/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.448 total time=  23.5s\n",
      "[CV 2/3; 142/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  23.8s\n",
      "[CV 3/3; 144/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 142/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.447 total time=  23.7s\n",
      "[CV 1/3; 145/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 143/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.437 total time=  24.2s\n",
      "[CV 2/3; 145/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 143/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.436 total time=  23.7s\n",
      "[CV 3/3; 145/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 143/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.436 total time=  23.9s\n",
      "[CV 1/3; 146/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 146/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 144/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.437 total time=  24.6s\n",
      "[CV 2/3; 144/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.436 total time=  24.0s\n",
      "[CV 3/3; 146/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 147/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 144/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.436 total time=  25.1s\n",
      "[CV 2/3; 147/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 145/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.408 total time=  26.1s\n",
      "[CV 2/3; 145/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.408 total time=  25.7s\n",
      "[CV 3/3; 147/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 148/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 145/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.408 total time=  26.3s\n",
      "[CV 1/3; 146/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.408 total time=  26.6s\n",
      "[CV 2/3; 148/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 146/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.408 total time=  26.2s\n",
      "[CV 3/3; 148/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 149/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 146/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.408 total time=  25.7s\n",
      "[CV 1/3; 147/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  24.7s\n",
      "[CV 2/3; 149/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 149/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 147/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.443 total time=  24.3s\n",
      "[CV 1/3; 150/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 147/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.445 total time=  24.2s\n",
      "[CV 1/3; 148/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  24.3s\n",
      "[CV 2/3; 150/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 150/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 148/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.443 total time=  24.2s\n",
      "[CV 3/3; 148/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.445 total time=  23.7s\n",
      "[CV 1/3; 151/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 151/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 149/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.441 total time=  24.9s\n",
      "[CV 3/3; 151/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 149/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.439 total time=  24.8s\n",
      "[CV 3/3; 149/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.440 total time=  24.5s\n",
      "[CV 1/3; 152/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 152/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 150/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.441 total time=  25.7s\n",
      "[CV 3/3; 152/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 150/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.439 total time=  25.4s\n",
      "[CV 3/3; 150/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.440 total time=  25.1s\n",
      "[CV 1/3; 153/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 153/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 151/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.408 total time=  25.7s\n",
      "[CV 2/3; 151/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.408 total time=  25.8s\n",
      "[CV 3/3; 153/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 154/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 151/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.408 total time=  26.0s\n",
      "[CV 2/3; 154/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 152/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.408 total time=  26.0s\n",
      "[CV 2/3; 152/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.408 total time=  26.2s\n",
      "[CV 3/3; 154/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 155/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 152/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.408 total time=  26.5s\n",
      "[CV 1/3; 153/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  25.3s\n",
      "[CV 2/3; 155/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 153/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.444 total time=  25.0s\n",
      "[CV 3/3; 155/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 153/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.445 total time=  25.2s\n",
      "[CV 1/3; 156/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 154/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  24.7s\n",
      "[CV 2/3; 156/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 156/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 154/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.444 total time=  25.0s\n",
      "[CV 3/3; 154/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.445 total time=  24.9s\n",
      "[CV 1/3; 157/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 155/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.441 total time=  25.0s\n",
      "[CV 2/3; 157/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 157/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 155/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.439 total time=  25.4s\n",
      "[CV 3/3; 155/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.440 total time=  25.1s\n",
      "[CV 1/3; 158/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 158/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 156/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.441 total time=  25.7s\n",
      "[CV 2/3; 156/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.439 total time=  25.9s\n",
      "[CV 3/3; 158/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 156/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.440 total time=  26.3s\n",
      "[CV 1/3; 159/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 159/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 157/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.408 total time=  26.4s\n",
      "[CV 2/3; 157/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.408 total time=  26.4s\n",
      "[CV 3/3; 159/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 157/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.408 total time=  27.1s\n",
      "[CV 1/3; 160/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 160/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 158/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.408 total time=  28.3s\n",
      "[CV 2/3; 158/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.408 total time=  28.0s\n",
      "[CV 3/3; 160/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 158/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.408 total time=  27.9s\n",
      "[CV 1/3; 159/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.449 total time=  25.9s\n",
      "[CV 1/3; 161/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 161/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 159/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.446 total time=  25.9s\n",
      "[CV 3/3; 161/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 162/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 159/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.447 total time=  25.5s\n",
      "[CV 1/3; 160/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.449 total time=  25.3s\n",
      "[CV 2/3; 162/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 160/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.446 total time=  24.6s\n",
      "[CV 3/3; 162/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 163/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 160/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.447 total time=  24.8s\n",
      "[CV 2/3; 163/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 161/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.442 total time=  24.6s\n",
      "[CV 2/3; 161/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.440 total time=  24.5s\n",
      "[CV 3/3; 163/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 161/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.441 total time=  24.5s\n",
      "[CV 1/3; 164/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 162/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.442 total time=  25.2s\n",
      "[CV 2/3; 164/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 164/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 162/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.440 total time=  25.8s\n",
      "[CV 1/3; 165/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 162/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.441 total time=  26.3s\n",
      "[CV 2/3; 165/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 165/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 6.3min\n",
      "[CV 3/3; 165/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 163/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.520 total time= 7.5min\n",
      "[CV 1/3; 166/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 165/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 7.2min\n",
      "[CV 2/3; 166/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 164/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.522 total time= 7.5min\n",
      "[CV 3/3; 166/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 164/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.520 total time= 7.7min\n",
      "[CV 1/3; 167/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 164/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.521 total time= 8.3min\n",
      "[CV 2/3; 167/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 163/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.521 total time= 9.0min\n",
      "[CV 3/3; 167/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 163/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.522 total time= 9.8min\n",
      "[CV 1/3; 168/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 165/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 6.8min\n",
      "[CV 2/3; 168/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 166/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 5.9min\n",
      "[CV 2/3; 166/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 6.1min\n",
      "[CV 3/3; 168/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 169/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 166/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 6.4min\n",
      "[CV 2/3; 169/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 167/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time= 8.0min\n",
      "[CV 3/3; 169/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 168/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.514 total time= 6.4min\n",
      "[CV 1/3; 170/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 167/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time= 7.2min\n",
      "[CV 2/3; 170/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 167/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.516 total time= 8.3min\n",
      "[CV 3/3; 170/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 168/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.516 total time= 7.1min\n",
      "[CV 1/3; 171/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 169/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.520 total time= 7.9min\n",
      "[CV 2/3; 171/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 168/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.515 total time= 8.7min\n",
      "[CV 3/3; 171/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 169/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.523 total time= 8.7min\n",
      "[CV 1/3; 172/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 169/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.521 total time= 7.9min\n",
      "[CV 2/3; 172/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 170/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.521 total time= 7.1min\n",
      "[CV 3/3; 172/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 170/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.520 total time= 7.8min\n",
      "[CV 1/3; 173/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 170/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.523 total time= 7.9min\n",
      "[CV 2/3; 173/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 171/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 6.2min\n",
      "[CV 3/3; 173/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 171/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 6.2min\n",
      "[CV 1/3; 174/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 172/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 6.1min\n",
      "[CV 2/3; 174/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 172/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 6.3min\n",
      "[CV 3/3; 174/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 171/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 7.8min\n",
      "[CV 1/3; 175/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 173/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time= 6.3min\n",
      "[CV 2/3; 175/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 172/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 7.1min\n",
      "[CV 3/3; 175/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 173/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.517 total time= 7.6min\n",
      "[CV 1/3; 176/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 173/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time= 7.7min\n",
      "[CV 2/3; 176/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 174/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.515 total time= 7.1min\n",
      "[CV 3/3; 176/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 174/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.516 total time= 7.6min\n",
      "[CV 1/3; 177/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 175/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.520 total time= 8.1min\n",
      "[CV 2/3; 177/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 174/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.515 total time= 8.5min\n",
      "[CV 3/3; 177/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 176/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.520 total time= 8.6min\n",
      "[CV 1/3; 178/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 175/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.521 total time=10.1min\n",
      "[CV 2/3; 178/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 175/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.523 total time=11.5min\n",
      "[CV 3/3; 178/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 176/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.523 total time= 7.6min\n",
      "[CV 1/3; 179/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 176/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.521 total time= 8.4min\n",
      "[CV 2/3; 179/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 177/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.513 total time= 7.8min\n",
      "[CV 3/3; 179/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 177/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.513 total time= 7.2min\n",
      "[CV 1/3; 180/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 178/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.513 total time= 6.7min\n",
      "[CV 2/3; 180/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 177/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time= 9.2min\n",
      "[CV 3/3; 180/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 178/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.513 total time= 7.0min\n",
      "[CV 1/3; 181/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 179/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.515 total time= 7.6min\n",
      "[CV 2/3; 181/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 178/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time= 8.6min\n",
      "[CV 3/3; 181/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 179/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.517 total time= 8.0min\n",
      "[CV 1/3; 182/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 179/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.516 total time= 8.8min\n",
      "[CV 2/3; 182/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 180/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.515 total time= 7.5min\n",
      "[CV 3/3; 182/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 180/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.518 total time= 8.5min\n",
      "[CV 1/3; 183/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 180/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.516 total time= 8.1min\n",
      "[CV 2/3; 183/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 181/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.493 total time= 8.3min\n",
      "[CV 3/3; 183/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 181/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.493 total time= 8.7min\n",
      "[CV 1/3; 184/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 182/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.493 total time= 8.3min\n",
      "[CV 2/3; 184/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 181/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.492 total time=10.2min\n",
      "[CV 3/3; 184/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 182/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.492 total time= 8.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 185/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 182/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.493 total time= 8.7min\n",
      "[CV 2/3; 185/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 183/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time= 7.3min\n",
      "[CV 3/3; 185/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 183/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time= 7.5min\n",
      "[CV 1/3; 186/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 183/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time= 7.6min\n",
      "[CV 2/3; 186/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 184/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time= 6.5min\n",
      "[CV 3/3; 186/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 184/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time= 7.3min\n",
      "[CV 1/3; 187/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 184/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time= 7.1min\n",
      "[CV 2/3; 187/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 185/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.477 total time= 7.6min\n",
      "[CV 3/3; 187/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 185/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.478 total time= 7.2min\n",
      "[CV 1/3; 188/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 185/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.479 total time= 9.2min\n",
      "[CV 2/3; 188/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 186/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time= 8.0min\n",
      "[CV 3/3; 188/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 186/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.479 total time= 7.8min\n",
      "[CV 1/3; 189/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 186/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time= 8.2min\n",
      "[CV 2/3; 189/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 187/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.493 total time= 8.7min\n",
      "[CV 3/3; 189/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 187/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.494 total time=10.6min\n",
      "[CV 1/3; 190/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 188/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.494 total time= 8.0min\n",
      "[CV 1/3; 188/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.493 total time= 8.5min\n",
      "[CV 2/3; 190/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 190/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 187/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.492 total time=10.0min\n",
      "[CV 1/3; 191/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 189/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time= 7.9min\n",
      "[CV 2/3; 191/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 188/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.492 total time= 9.1min\n",
      "[CV 3/3; 191/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 189/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 7.9min\n",
      "[CV 1/3; 192/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 189/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time= 8.1min\n",
      "[CV 2/3; 192/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 190/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time= 6.7min\n",
      "[CV 3/3; 192/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 190/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time= 6.7min\n",
      "[CV 1/3; 193/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 190/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 7.0min\n",
      "[CV 2/3; 193/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 191/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.479 total time= 7.2min\n",
      "[CV 1/3; 191/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.478 total time= 8.0min\n",
      "[CV 3/3; 193/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 194/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 191/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.478 total time= 7.8min\n",
      "[CV 2/3; 194/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 192/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time= 7.9min\n",
      "[CV 3/3; 194/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 192/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.479 total time= 9.0min\n",
      "[CV 1/3; 195/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 193/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.494 total time= 9.3min\n",
      "[CV 2/3; 195/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 192/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time=10.0min\n",
      "[CV 3/3; 195/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 193/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.494 total time=10.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 196/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 194/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.494 total time= 9.5min\n",
      "[CV 2/3; 196/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 193/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.492 total time=10.9min\n",
      "[CV 3/3; 196/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 194/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.492 total time= 9.1min\n",
      "[CV 1/3; 197/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 194/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.494 total time=10.6min\n",
      "[CV 2/3; 197/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 195/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time= 7.9min\n",
      "[CV 3/3; 197/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 195/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 7.9min\n",
      "[CV 1/3; 198/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 195/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time= 7.9min\n",
      "[CV 2/3; 198/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 196/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time= 7.7min\n",
      "[CV 3/3; 198/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 196/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 8.5min\n",
      "[CV 1/3; 199/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 196/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time= 8.6min\n",
      "[CV 2/3; 199/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 197/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.480 total time= 7.7min\n",
      "[CV 3/3; 199/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 197/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.479 total time= 9.4min\n",
      "[CV 1/3; 200/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 197/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.479 total time= 8.7min\n",
      "[CV 2/3; 200/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 198/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.479 total time= 7.6min\n",
      "[CV 3/3; 200/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 198/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.480 total time= 8.7min\n",
      "[CV 1/3; 201/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 198/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.478 total time= 9.4min\n",
      "[CV 2/3; 201/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 199/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.500 total time=11.5min\n",
      "[CV 3/3; 201/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 199/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.501 total time=11.5min\n",
      "[CV 1/3; 202/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 199/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.501 total time=11.5min\n",
      "[CV 2/3; 202/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 200/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.500 total time=11.4min\n",
      "[CV 3/3; 202/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 200/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.501 total time=11.3min\n",
      "[CV 1/3; 203/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 200/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.501 total time=11.3min\n",
      "[CV 2/3; 203/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 201/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=10.0min\n",
      "[CV 3/3; 203/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 201/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 9.9min\n",
      "[CV 1/3; 204/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 201/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time= 9.8min\n",
      "[CV 2/3; 204/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 202/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 9.7min\n",
      "[CV 3/3; 204/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 202/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 9.7min\n",
      "[CV 1/3; 205/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 202/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time= 9.7min\n",
      "[CV 2/3; 205/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 203/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=10.5min\n",
      "[CV 3/3; 205/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 203/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.508 total time=10.6min\n",
      "[CV 1/3; 206/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 203/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=10.6min\n",
      "[CV 2/3; 206/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 204/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=10.6min\n",
      "[CV 3/3; 206/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 204/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.508 total time=10.9min\n",
      "[CV 1/3; 207/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 204/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=10.9min\n",
      "[CV 2/3; 207/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 205/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.500 total time=11.6min\n",
      "[CV 3/3; 207/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 205/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.501 total time=11.6min\n",
      "[CV 1/3; 208/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 205/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.502 total time=11.7min\n",
      "[CV 2/3; 208/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 206/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.500 total time=11.6min\n",
      "[CV 3/3; 208/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 206/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.501 total time=11.6min\n",
      "[CV 1/3; 209/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 206/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.502 total time=11.5min\n",
      "[CV 2/3; 209/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 207/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=10.2min\n",
      "[CV 3/3; 209/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 207/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=10.2min\n",
      "[CV 1/3; 210/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 207/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=10.1min\n",
      "[CV 2/3; 210/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 208/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=10.1min\n",
      "[CV 3/3; 210/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 208/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=10.1min\n",
      "[CV 1/3; 211/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 208/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time=10.1min\n",
      "[CV 2/3; 211/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 209/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=10.5min\n",
      "[CV 3/3; 211/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 209/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.509 total time=10.7min\n",
      "[CV 1/3; 212/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 209/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=10.7min\n",
      "[CV 2/3; 212/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 210/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=10.8min\n",
      "[CV 3/3; 212/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 210/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.509 total time=11.0min\n",
      "[CV 1/3; 213/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 210/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=11.0min\n",
      "[CV 2/3; 213/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 211/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.501 total time=12.1min\n",
      "[CV 3/3; 213/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 211/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.502 total time=12.2min\n",
      "[CV 1/3; 214/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 211/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.502 total time=12.2min\n",
      "[CV 2/3; 214/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 212/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.501 total time=12.0min\n",
      "[CV 3/3; 214/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 212/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.502 total time=12.0min\n",
      "[CV 1/3; 215/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 213/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=10.9min\n",
      "[CV 3/3; 212/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.502 total time=11.9min\n",
      "[CV 2/3; 215/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 215/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 213/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=10.8min\n",
      "[CV 1/3; 216/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 213/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=10.7min\n",
      "[CV 2/3; 216/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 214/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=10.6min\n",
      "[CV 3/3; 216/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 214/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=10.6min\n",
      "[CV 1/3; 217/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 214/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=10.6min\n",
      "[CV 2/3; 217/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 215/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=10.8min\n",
      "[CV 3/3; 217/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 215/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.509 total time=10.9min\n",
      "[CV 3/3; 215/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=10.9min\n",
      "[CV 1/3; 218/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 218/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 216/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=10.9min\n",
      "[CV 3/3; 218/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 216/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.509 total time=11.1min\n",
      "[CV 1/3; 219/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 216/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=11.1min\n",
      "[CV 2/3; 219/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 217/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.483 total time=11.4min\n",
      "[CV 3/3; 219/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 217/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.485 total time=11.5min\n",
      "[CV 1/3; 220/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 217/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.484 total time=11.4min\n",
      "[CV 2/3; 220/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 218/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.484 total time=11.4min\n",
      "[CV 3/3; 220/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 218/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.485 total time=11.4min\n",
      "[CV 1/3; 221/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 218/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.484 total time=11.4min\n",
      "[CV 2/3; 221/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 219/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=10.2min\n",
      "[CV 3/3; 221/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 219/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time=10.1min\n",
      "[CV 1/3; 222/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 219/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=10.1min\n",
      "[CV 2/3; 222/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 220/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=10.1min\n",
      "[CV 3/3; 222/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 220/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time=10.0min\n",
      "[CV 1/3; 223/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 220/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time=10.0min\n",
      "[CV 2/3; 223/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 221/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=10.8min\n",
      "[CV 3/3; 223/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 221/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=10.8min\n",
      "[CV 1/3; 224/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 221/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=10.8min\n",
      "[CV 2/3; 224/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 222/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time=10.9min\n",
      "[CV 3/3; 224/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 222/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=11.0min\n",
      "[CV 1/3; 225/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 222/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time=11.1min\n",
      "[CV 2/3; 225/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 223/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.484 total time=11.6min\n",
      "[CV 3/3; 225/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 223/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.485 total time=11.6min\n",
      "[CV 1/3; 226/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 223/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.484 total time=11.6min\n",
      "[CV 2/3; 226/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 224/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.484 total time=11.5min\n",
      "[CV 3/3; 226/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 224/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.485 total time=11.6min\n",
      "[CV 1/3; 227/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 224/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.484 total time=11.5min\n",
      "[CV 2/3; 227/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 225/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=10.6min\n",
      "[CV 3/3; 227/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 225/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=10.6min\n",
      "[CV 1/3; 228/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 225/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time=10.4min\n",
      "[CV 2/3; 228/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 226/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=10.4min\n",
      "[CV 3/3; 228/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 226/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=10.4min\n",
      "[CV 1/3; 229/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 226/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time=10.4min\n",
      "[CV 2/3; 229/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 227/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=10.9min\n",
      "[CV 3/3; 229/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 227/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=10.9min\n",
      "[CV 3/3; 227/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=10.9min\n",
      "[CV 1/3; 230/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 230/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 228/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time=11.0min\n",
      "[CV 3/3; 230/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 228/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=11.1min\n",
      "[CV 1/3; 231/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 228/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time=11.2min\n",
      "[CV 2/3; 231/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 229/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.485 total time=12.1min\n",
      "[CV 3/3; 231/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 229/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.486 total time=12.1min\n",
      "[CV 1/3; 232/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 229/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.485 total time=12.1min\n",
      "[CV 2/3; 232/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 230/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.485 total time=12.0min\n",
      "[CV 3/3; 232/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 230/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.486 total time=12.1min\n",
      "[CV 1/3; 233/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 230/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.485 total time=12.0min\n",
      "[CV 2/3; 233/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 231/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=11.2min\n",
      "[CV 3/3; 233/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 231/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=11.0min\n",
      "[CV 1/3; 234/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 231/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=11.0min\n",
      "[CV 2/3; 234/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 232/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=11.0min\n",
      "[CV 3/3; 234/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 232/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=11.0min\n",
      "[CV 1/3; 235/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 232/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=11.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 235/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 233/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time=11.1min\n",
      "[CV 3/3; 235/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 233/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=11.3min\n",
      "[CV 1/3; 236/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 233/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=11.2min\n",
      "[CV 2/3; 236/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 234/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time=11.3min\n",
      "[CV 3/3; 236/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 234/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=11.5min\n",
      "[CV 1/3; 237/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 234/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.467 total time=11.6min\n",
      "[CV 2/3; 237/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 235/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time=11.2min\n",
      "[CV 3/3; 237/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 235/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=11.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 238/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 235/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time=11.2min\n",
      "[CV 2/3; 238/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 236/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.471 total time=11.1min\n",
      "[CV 3/3; 238/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 236/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=11.0min\n",
      "[CV 1/3; 239/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 236/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.471 total time=10.9min\n",
      "[CV 2/3; 239/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 237/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=10.7min\n",
      "[CV 3/3; 239/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 237/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=10.7min\n",
      "[CV 1/3; 240/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 237/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=10.5min\n",
      "[CV 2/3; 240/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 238/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time=10.5min\n",
      "[CV 3/3; 240/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 238/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=10.5min\n",
      "[CV 1/3; 241/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 238/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.510 total time=10.5min\n",
      "[CV 2/3; 241/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 239/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.503 total time=11.6min\n",
      "[CV 3/3; 241/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 239/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.505 total time=11.6min\n",
      "[CV 1/3; 242/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 239/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.503 total time=11.7min\n",
      "[CV 2/3; 242/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 240/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=11.7min\n",
      "[CV 3/3; 242/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 240/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=11.9min\n",
      "[CV 1/3; 243/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 241/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time=11.4min\n",
      "[CV 2/3; 243/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 240/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=11.9min\n",
      "[CV 3/3; 243/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 241/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=11.4min\n",
      "[CV 1/3; 244/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 241/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time=11.4min\n",
      "[CV 2/3; 244/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 242/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.471 total time=11.3min\n",
      "[CV 3/3; 244/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 242/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=11.3min\n",
      "[CV 1/3; 245/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 242/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.471 total time=11.2min\n",
      "[CV 2/3; 245/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 243/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.513 total time=11.0min\n",
      "[CV 3/3; 245/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 243/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=11.0min\n",
      "[CV 1/3; 246/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 243/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=11.0min\n",
      "[CV 2/3; 246/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 244/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=11.0min\n",
      "[CV 3/3; 246/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 244/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.513 total time=10.9min\n",
      "[CV 1/3; 247/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 244/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time=10.9min\n",
      "[CV 2/3; 247/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 245/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.503 total time=11.5min\n",
      "[CV 3/3; 247/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 245/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.506 total time=11.6min\n",
      "[CV 1/3; 248/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 245/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.504 total time=11.7min\n",
      "[CV 2/3; 248/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 246/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=11.8min\n",
      "[CV 3/3; 248/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 246/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=11.9min\n",
      "[CV 1/3; 249/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 246/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=12.0min\n",
      "[CV 2/3; 249/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 247/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.472 total time=11.7min\n",
      "[CV 3/3; 249/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 247/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=11.8min\n",
      "[CV 1/3; 250/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 247/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.472 total time=11.7min\n",
      "[CV 2/3; 250/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 248/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.472 total time=11.6min\n",
      "[CV 3/3; 250/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 248/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.471 total time=11.6min\n",
      "[CV 1/3; 251/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 248/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.472 total time=11.4min\n",
      "[CV 2/3; 251/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 249/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=11.6min\n",
      "[CV 3/3; 251/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 249/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.511 total time=11.6min\n",
      "[CV 1/3; 252/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 249/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.512 total time=11.4min\n",
      "[CV 2/3; 252/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 250/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.512 total time=11.4min\n",
      "[CV 3/3; 252/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 250/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.513 total time=11.4min\n",
      "[CV 1/3; 253/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 250/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.511 total time=11.4min\n",
      "[CV 2/3; 253/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 251/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.503 total time=11.9min\n",
      "[CV 3/3; 253/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 251/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.505 total time=11.8min\n",
      "[CV 1/3; 254/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 251/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.504 total time=12.0min\n",
      "[CV 2/3; 254/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 252/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=12.1min\n",
      "[CV 3/3; 254/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 252/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.506 total time=12.2min\n",
      "[CV 1/3; 255/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 252/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.503 total time=12.3min\n",
      "[CV 2/3; 255/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 253/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time=11.2min\n",
      "[CV 3/3; 255/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 253/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time=11.2min\n",
      "[CV 1/3; 256/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 253/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time=11.1min\n",
      "[CV 2/3; 256/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 254/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time=11.1min\n",
      "[CV 3/3; 256/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 254/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time=11.0min\n",
      "[CV 1/3; 257/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 254/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time=11.0min\n",
      "[CV 2/3; 257/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 255/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.473 total time=10.8min\n",
      "[CV 3/3; 257/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 255/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.477 total time=10.7min\n",
      "[CV 1/3; 258/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 255/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.477 total time=10.6min\n",
      "[CV 2/3; 258/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 256/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.473 total time=10.7min\n",
      "[CV 3/3; 258/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 256/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.477 total time=10.6min\n",
      "[CV 1/3; 259/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 256/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.477 total time=10.6min\n",
      "[CV 2/3; 259/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 257/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=11.8min\n",
      "[CV 3/3; 259/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 257/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=11.9min\n",
      "[CV 1/3; 260/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 257/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time=12.0min\n",
      "[CV 2/3; 260/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 258/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.464 total time=12.0min\n",
      "[CV 3/3; 260/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 258/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=12.1min\n",
      "[CV 1/3; 261/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 258/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=12.2min\n",
      "[CV 2/3; 261/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 259/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time=11.3min\n",
      "[CV 3/3; 261/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 259/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.466 total time=11.3min\n",
      "[CV 1/3; 262/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 259/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.465 total time=11.3min\n",
      "[CV 2/3; 262/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 260/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time=11.2min\n",
      "[CV 3/3; 262/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 260/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.465 total time=11.2min\n",
      "[CV 1/3; 263/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 260/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time=11.0min\n",
      "[CV 2/3; 263/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 261/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.480 total time=11.2min\n",
      "[CV 3/3; 263/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 261/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.480 total time=11.1min\n",
      "[CV 1/3; 264/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 261/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.469 total time=11.0min\n",
      "[CV 2/3; 264/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 262/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.456 total time=11.0min\n",
      "[CV 3/3; 264/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 262/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.473 total time=11.0min\n",
      "[CV 1/3; 265/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 262/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.479 total time=11.0min\n",
      "[CV 2/3; 265/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 263/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.467 total time=11.8min\n",
      "[CV 3/3; 265/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 263/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.471 total time=11.9min\n",
      "[CV 1/3; 266/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 263/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.468 total time=12.0min\n",
      "[CV 2/3; 266/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 264/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.464 total time=12.1min\n",
      "[CV 3/3; 266/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 264/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time=12.3min\n",
      "[CV 1/3; 267/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 264/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.468 total time=12.3min\n",
      "[CV 2/3; 267/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 265/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.466 total time=11.8min\n",
      "[CV 3/3; 267/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 265/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.466 total time=11.8min\n",
      "[CV 1/3; 268/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 265/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=0.466 total time=11.8min\n",
      "[CV 2/3; 268/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 266/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time=11.8min\n",
      "[CV 3/3; 268/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 266/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time=11.7min\n",
      "[CV 1/3; 269/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 266/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.466 total time=11.6min\n",
      "[CV 2/3; 269/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 267/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.479 total time=11.8min\n",
      "[CV 3/3; 269/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 267/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.480 total time=11.7min\n",
      "[CV 1/3; 270/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 267/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=0.479 total time=11.8min\n",
      "[CV 2/3; 270/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 268/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.481 total time=11.7min\n",
      "[CV 3/3; 270/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 268/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.479 total time=11.7min\n",
      "[CV 1/3; 271/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 271/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  20.2s\n",
      "[CV 2/3; 271/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 271/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  19.3s\n",
      "[CV 3/3; 271/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 268/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.463 total time=11.7min\n",
      "[CV 3/3; 271/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  20.2s\n",
      "[CV 1/3; 272/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 272/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 272/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  21.0s\n",
      "[CV 3/3; 272/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 272/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  20.7s\n",
      "[CV 1/3; 273/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 272/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  20.0s\n",
      "[CV 1/3; 273/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.6s\n",
      "[CV 2/3; 273/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 273/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 273/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.6s\n",
      "[CV 1/3; 274/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 273/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.1s\n",
      "[CV 2/3; 274/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 274/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.4s\n",
      "[CV 3/3; 274/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 274/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.0s\n",
      "[CV 1/3; 275/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 274/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.0s\n",
      "[CV 2/3; 275/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 275/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.8s\n",
      "[CV 3/3; 275/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 269/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.466 total time=12.3min\n",
      "[CV 1/3; 276/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 275/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  20.5s\n",
      "[CV 3/3; 275/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  20.0s\n",
      "[CV 2/3; 276/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 276/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 276/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  19.8s\n",
      "[CV 1/3; 277/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 276/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  20.4s\n",
      "[CV 3/3; 276/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  19.2s\n",
      "[CV 2/3; 277/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 277/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 277/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  20.2s\n",
      "[CV 2/3; 269/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.470 total time=12.2min\n",
      "[CV 1/3; 278/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 278/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 277/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  20.8s\n",
      "[CV 3/3; 277/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  22.0s\n",
      "[CV 3/3; 278/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 279/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 278/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  21.4s\n",
      "[CV 2/3; 278/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  21.0s\n",
      "[CV 2/3; 279/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 279/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 278/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  21.1s\n",
      "[CV 1/3; 279/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  20.1s\n",
      "[CV 1/3; 280/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 280/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 279/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.4s\n",
      "[CV 3/3; 279/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.5s\n",
      "[CV 3/3; 280/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 280/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.7s\n",
      "[CV 1/3; 281/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 280/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.3s\n",
      "[CV 2/3; 281/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 281/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 280/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.5s\n",
      "[CV 1/3; 282/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 281/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  19.0s\n",
      "[CV 2/3; 281/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  19.3s\n",
      "[CV 2/3; 282/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 282/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 281/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  19.3s\n",
      "[CV 1/3; 283/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 282/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.4s\n",
      "[CV 2/3; 282/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.3s\n",
      "[CV 2/3; 283/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 282/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.8s\n",
      "[CV 3/3; 283/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 284/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 283/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  19.2s\n",
      "[CV 2/3; 284/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 283/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  20.2s\n",
      "[CV 3/3; 283/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  19.7s\n",
      "[CV 3/3; 284/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 284/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  21.0s\n",
      "[CV 1/3; 285/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 284/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  20.2s\n",
      "[CV 2/3; 285/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 285/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 269/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=0.462 total time=12.3min\n",
      "[CV 3/3; 284/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  19.5s\n",
      "[CV 1/3; 285/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.8s\n",
      "[CV 1/3; 286/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 285/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.0s\n",
      "[CV 2/3; 286/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 285/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.8s\n",
      "[CV 3/3; 286/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 287/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 287/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 286/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.8s\n",
      "[CV 2/3; 286/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.2s\n",
      "[CV 3/3; 286/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.9s\n",
      "[CV 3/3; 287/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 288/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 287/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.0s\n",
      "[CV 2/3; 288/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 270/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=12.3min\n",
      "[CV 2/3; 287/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.1s\n",
      "[CV 3/3; 288/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 289/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 289/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 287/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.9s\n",
      "[CV 1/3; 288/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.8s\n",
      "[CV 3/3; 289/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 288/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.8s\n",
      "[CV 3/3; 288/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  19.2s\n",
      "[CV 1/3; 290/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 289/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  19.0s\n",
      "[CV 2/3; 290/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 290/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 289/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.7s\n",
      "[CV 1/3; 291/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 291/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 289/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.9s\n",
      "[CV 3/3; 291/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 290/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.8s\n",
      "[CV 2/3; 290/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  19.1s\n",
      "[CV 1/3; 292/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 290/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  19.5s\n",
      "[CV 1/3; 291/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.8s\n",
      "[CV 2/3; 292/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 291/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.5s\n",
      "[CV 3/3; 292/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 293/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 293/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 291/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.6s\n",
      "[CV 1/3; 292/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.7s\n",
      "[CV 3/3; 293/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 292/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.1s\n",
      "[CV 1/3; 294/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 292/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.9s\n",
      "[CV 2/3; 294/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 293/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.6s\n",
      "[CV 3/3; 294/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 293/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.8s\n",
      "[CV 1/3; 295/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 295/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 293/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 294/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.0s\n",
      "[CV 3/3; 295/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 294/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.0s\n",
      "[CV 1/3; 296/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 294/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.1s\n",
      "[CV 2/3; 296/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 295/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  19.2s\n",
      "[CV 3/3; 296/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 295/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.9s\n",
      "[CV 1/3; 297/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 297/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 270/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.470 total time=12.1min\n",
      "[CV 3/3; 297/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 295/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.6s\n",
      "[CV 1/3; 298/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 296/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  19.0s\n",
      "[CV 2/3; 296/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.3s\n",
      "[CV 2/3; 298/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 298/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 296/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  19.3s\n",
      "[CV 1/3; 297/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.2s\n",
      "[CV 1/3; 299/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 297/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.2s\n",
      "[CV 2/3; 299/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 299/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 297/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.9s\n",
      "[CV 1/3; 300/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 298/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.1s\n",
      "[CV 2/3; 300/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 298/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.3s\n",
      "[CV 3/3; 298/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.6s\n",
      "[CV 3/3; 300/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 301/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 299/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.1s\n",
      "[CV 2/3; 301/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 299/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.5s\n",
      "[CV 3/3; 299/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.7s\n",
      "[CV 3/3; 301/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 302/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 300/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.8s\n",
      "[CV 2/3; 302/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 300/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 300/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.8s\n",
      "[CV 3/3; 302/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 301/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.8s\n",
      "[CV 1/3; 303/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 301/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.4s\n",
      "[CV 2/3; 303/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 303/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 301/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.7s\n",
      "[CV 1/3; 302/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.4s\n",
      "[CV 1/3; 304/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 270/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=0.469 total time=11.9min\n",
      "[CV 2/3; 304/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 304/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 302/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.9s\n",
      "[CV 1/3; 305/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 302/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.6s\n",
      "[CV 1/3; 303/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.4s\n",
      "[CV 2/3; 305/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 303/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.6s\n",
      "[CV 3/3; 305/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 303/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.3s\n",
      "[CV 1/3; 306/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 306/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 304/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.0s\n",
      "[CV 2/3; 304/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.6s\n",
      "[CV 3/3; 306/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 304/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.2s\n",
      "[CV 1/3; 307/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 305/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.9s\n",
      "[CV 2/3; 307/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 307/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 305/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.0s\n",
      "[CV 3/3; 305/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.6s\n",
      "[CV 1/3; 308/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 306/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.1s\n",
      "[CV 2/3; 308/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 306/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.5s\n",
      "[CV 3/3; 308/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 309/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 306/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.6s\n",
      "[CV 1/3; 307/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.2s\n",
      "[CV 2/3; 309/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 307/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.7s\n",
      "[CV 3/3; 309/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 307/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.6s\n",
      "[CV 1/3; 310/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 310/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 308/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.5s\n",
      "[CV 3/3; 310/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 308/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.7s\n",
      "[CV 3/3; 308/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.7s\n",
      "[CV 1/3; 311/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 309/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.7s\n",
      "[CV 2/3; 311/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 311/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 309/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.3s\n",
      "[CV 3/3; 309/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  15.8s\n",
      "[CV 1/3; 312/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 310/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.0s\n",
      "[CV 2/3; 312/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 312/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 310/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.4s\n",
      "[CV 1/3; 313/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 310/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  15.7s\n",
      "[CV 1/3; 311/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  15.9s\n",
      "[CV 2/3; 313/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 311/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.5s\n",
      "[CV 3/3; 313/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 311/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.5s\n",
      "[CV 1/3; 314/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 314/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 312/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.9s\n",
      "[CV 2/3; 312/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.4s\n",
      "[CV 3/3; 314/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 312/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.5, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.7s\n",
      "[CV 1/3; 315/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 313/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.7s\n",
      "[CV 2/3; 315/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 315/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 313/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.9s\n",
      "[CV 3/3; 313/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.6s\n",
      "[CV 1/3; 316/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 314/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.0s\n",
      "[CV 2/3; 316/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 314/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.3s\n",
      "[CV 3/3; 316/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 317/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 314/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.3s\n",
      "[CV 1/3; 315/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.1s\n",
      "[CV 2/3; 315/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.9s\n",
      "[CV 2/3; 317/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 315/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.2s\n",
      "[CV 3/3; 317/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 318/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 318/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 316/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.5s\n",
      "[CV 2/3; 316/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.1s\n",
      "[CV 3/3; 318/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 316/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  15.9s\n",
      "[CV 1/3; 319/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 319/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 1/3; 317/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  15.7s\n",
      "[CV 3/3; 319/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 317/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  15.8s\n",
      "[CV 1/3; 320/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 317/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.7s\n",
      "[CV 2/3; 320/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 318/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.9s\n",
      "[CV 2/3; 318/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  17.2s\n",
      "[CV 3/3; 320/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 321/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 318/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=0.75, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.9s\n",
      "[CV 1/3; 319/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.7s\n",
      "[CV 2/3; 321/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 321/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 319/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.3s\n",
      "[CV 1/3; 322/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 3/3; 319/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  18.2s\n",
      "[CV 2/3; 322/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 320/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.2s\n",
      "[CV 3/3; 322/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 320/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.1s\n",
      "[CV 1/3; 323/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 320/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=None, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  18.1s\n",
      "[CV 1/3; 321/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.7s\n",
      "[CV 2/3; 323/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 3/3; 323/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None\n",
      "[CV 2/3; 321/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.9s\n",
      "[CV 3/3; 321/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.9s\n",
      "[CV 1/3; 324/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 322/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.6s\n",
      "[CV 2/3; 324/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 2/3; 322/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.1s\n",
      "[CV 3/3; 324/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode\n",
      "[CV 1/3; 325/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 322/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=1000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  16.4s\n",
      "[CV 1/3; 323/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  16.3s\n",
      "[CV 2/3; 325/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 325/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 323/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.5s\n",
      "[CV 3/3; 323/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=None;, score=nan total time=  17.2s\n",
      "[CV 1/3; 326/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x12f0e2290>\n",
      "[CV 1/3; 326/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x12f0e2290>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 326/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1394594d0>\n",
      "[CV 2/3; 326/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1394594d0>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 326/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x12d66ff10>\n",
      "[CV 3/3; 326/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x12d66ff10>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 327/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x139063f10>\n",
      "[CV 1/3; 324/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  20.0s\n",
      "[CV 2/3; 327/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x33473bf50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 324/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  20.3s\n",
      "[CV 3/3; 327/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x168044890>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 324/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__max_df=1.0, preprocessor__text__vectorizer__max_features=10000, preprocessor__text__vectorizer__strip_accents=unicode;, score=nan total time=  21.0s\n",
      "[CV 1/3; 328/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 328/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 325/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.520 total time=  43.3s\n",
      "[CV 3/3; 328/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 325/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.523 total time=  44.5s\n",
      "[CV 3/3; 325/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.521 total time=  42.8s\n",
      "[CV 1/3; 329/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16eaa4910>\n",
      "[CV 1/3; 329/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16eaa4910>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 329/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1523e9550>\n",
      "[CV 2/3; 329/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1523e9550>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 329/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x168bf4f90>\n",
      "[CV 3/3; 329/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x168bf4f90>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 330/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x139946450>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 330/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b5a62990>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 328/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.520 total time=  37.5s\n",
      "[CV 3/3; 330/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x12a2d9750>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 328/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.522 total time=  36.7s\n",
      "[CV 1/3; 331/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 328/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.521 total time=  35.6s\n",
      "[CV 2/3; 331/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 331/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.494 total time=  35.6s\n",
      "[CV 3/3; 331/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 331/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.494 total time=  35.0s\n",
      "[CV 1/3; 332/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c8f6e390>\n",
      "[CV 1/3; 332/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c8f6e390>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 332/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16f0964d0>\n",
      "[CV 2/3; 332/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16f0964d0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 332/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2ac1a2cd0>\n",
      "[CV 3/3; 332/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2ac1a2cd0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 333/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x30c3dd790>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 331/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.492 total time=  35.4s\n",
      "[CV 2/3; 333/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10a316210>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 327/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x139063f10>;, score=0.522 total time=10.1min\n",
      "[CV 3/3; 327/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x168044890>;, score=0.521 total time=10.0min\n",
      "[CV 3/3; 333/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x282c4d610>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 334/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 327/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x33473bf50>;, score=0.522 total time=10.1min\n",
      "[CV 2/3; 334/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 330/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b5a62990>;, score=0.522 total time=10.0min\n",
      "[CV 1/3; 330/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x139946450>;, score=0.521 total time=10.0min\n",
      "[CV 3/3; 334/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 334/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.493 total time=  34.1s\n",
      "[CV 1/3; 335/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2ae855010>\n",
      "[CV 1/3; 335/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2ae855010>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 335/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1669a0e10>\n",
      "[CV 2/3; 335/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1669a0e10>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 335/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2ae877f10>\n",
      "[CV 3/3; 335/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2ae877f10>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 336/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16a5a1710>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 330/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x12a2d9750>;, score=0.519 total time=10.1min\n",
      "[CV 2/3; 336/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2ae555210>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 336/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x14e2c2110>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 334/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.492 total time=  36.0s\n",
      "[CV 1/3; 337/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 334/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.491 total time=  36.0s\n",
      "[CV 2/3; 337/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 333/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x30c3dd790>;, score=0.493 total time= 9.9min\n",
      "[CV 2/3; 333/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10a316210>;, score=0.493 total time= 9.9min\n",
      "[CV 3/3; 337/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 338/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17761a2d0>\n",
      "[CV 1/3; 338/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17761a2d0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 338/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177618790>\n",
      "[CV 2/3; 338/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177618790>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 338/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177312c90>\n",
      "[CV 3/3; 338/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177312c90>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 339/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1775f3f10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 337/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.501 total time= 1.3min\n",
      "[CV 2/3; 339/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x285879f50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 337/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.502 total time= 1.3min\n",
      "[CV 3/3; 339/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16c09dad0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 337/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.502 total time= 1.2min\n",
      "[CV 1/3; 340/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 340/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.499 total time= 1.1min\n",
      "[CV 2/3; 340/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 340/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.501 total time= 1.1min\n",
      "[CV 3/3; 340/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 340/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.500 total time= 1.1min\n",
      "[CV 1/3; 341/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x176c9dcd0>\n",
      "[CV 1/3; 341/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x176c9dcd0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 341/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17ce14e10>\n",
      "[CV 2/3; 341/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17ce14e10>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 341/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x176a82290>\n",
      "[CV 3/3; 341/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x176a82290>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 342/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x176b15090>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 333/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x282c4d610>;, score=0.493 total time=10.1min\n",
      "[CV 2/3; 342/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a522ec90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 336/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16a5a1710>;, score=0.492 total time=10.0min\n",
      "[CV 3/3; 342/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x291c75190>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 336/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2ae555210>;, score=0.492 total time=10.1min\n",
      "[CV 1/3; 343/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 336/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x14e2c2110>;, score=0.492 total time=10.2min\n",
      "[CV 2/3; 343/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 343/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.485 total time= 1.5min\n",
      "[CV 3/3; 343/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 343/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.486 total time= 1.5min\n",
      "[CV 1/3; 344/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x295c6e3d0>\n",
      "[CV 1/3; 344/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x295c6e3d0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 344/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28eecead0>\n",
      "[CV 2/3; 344/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28eecead0>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 344/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28eece210>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 344/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28eece210>;, score=nan total time=   0.2s\n",
      "[CV 1/3; 345/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x30e3bf310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 339/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1775f3f10>;, score=0.506 total time=10.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 345/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10a366ed0>\n",
      "[CV 2/3; 339/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x285879f50>;, score=0.505 total time=10.9min\n",
      "[CV 3/3; 345/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15cf7ff10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 339/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16c09dad0>;, score=0.504 total time=10.8min\n",
      "[CV 1/3; 346/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 343/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.484 total time= 1.5min\n",
      "[CV 2/3; 346/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 346/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.483 total time= 1.4min\n",
      "[CV 3/3; 346/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 346/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.484 total time= 1.4min\n",
      "[CV 1/3; 347/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x292ace790>\n",
      "[CV 1/3; 347/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x292ace790>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 347/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x292add010>\n",
      "[CV 2/3; 347/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x292add010>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 347/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2928167d0>\n",
      "[CV 3/3; 347/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2928167d0>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 348/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x292808850>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 346/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.483 total time= 1.3min\n",
      "[CV 2/3; 348/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x169575610>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 342/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x176b15090>;, score=0.504 total time=10.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 348/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1137c9e50>\n",
      "[CV 2/3; 342/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a522ec90>;, score=0.503 total time=10.7min\n",
      "[CV 1/3; 349/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 342/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x291c75190>;, score=0.503 total time=10.7min\n",
      "[CV 2/3; 349/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 345/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x30e3bf310>;, score=0.490 total time=11.0min\n",
      "[CV 3/3; 349/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 345/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10a366ed0>;, score=0.490 total time=11.1min\n",
      "[CV 1/3; 350/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177374950>\n",
      "[CV 1/3; 350/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177374950>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 350/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177376b10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 350/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177376b10>;, score=nan total time=   0.3s\n",
      "[CV 3/3; 350/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177953f10>\n",
      "[CV 3/3; 350/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x177953f10>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 351/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x177268a10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 345/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15cf7ff10>;, score=0.491 total time=11.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 351/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x12e0d1f50>\n",
      "[CV 1/3; 348/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x292808850>;, score=0.489 total time=11.2min\n",
      "[CV 3/3; 351/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2ae017f10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 348/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x169575610>;, score=0.488 total time=11.2min\n",
      "[CV 1/3; 352/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 349/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.472 total time= 7.2min\n",
      "[CV 2/3; 352/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 348/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1137c9e50>;, score=0.489 total time=11.2min\n",
      "[CV 3/3; 352/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 349/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.470 total time= 7.7min\n",
      "[CV 1/3; 353/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d74ef50>\n",
      "[CV 1/3; 353/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d74ef50>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 353/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d74ee10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 353/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d74ee10>;, score=nan total time=   0.3s\n",
      "[CV 3/3; 353/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10da2c090>\n",
      "[CV 3/3; 353/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10da2c090>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 354/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10da3fa90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 349/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.472 total time= 7.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 354/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x28cf03b90>\n",
      "[CV 1/3; 352/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.470 total time= 7.2min\n",
      "[CV 3/3; 354/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15f3c7990>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 352/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.469 total time= 7.5min\n",
      "[CV 1/3; 355/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 352/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.470 total time= 7.3min\n",
      "[CV 2/3; 355/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 351/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x12e0d1f50>;, score=0.475 total time=17.8min\n",
      "[CV 3/3; 355/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 351/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x177268a10>;, score=0.475 total time=18.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 356/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15af8f610>\n",
      "[CV 1/3; 356/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15af8f610>;, score=nan total time=   0.1s\n",
      "[CV 2/3; 356/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15af8d050>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 356/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15af8d050>;, score=nan total time=   0.9s\n",
      "[CV 3/3; 356/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15af276d0>\n",
      "[CV 3/3; 356/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15af276d0>;, score=nan total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 357/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16002edd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 355/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.466 total time= 8.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 357/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x108f19a10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 355/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.466 total time= 8.4min\n",
      "[CV 3/3; 357/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d1d6e3d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 351/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2ae017f10>;, score=0.475 total time=18.8min\n",
      "[CV 1/3; 358/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 354/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10da3fa90>;, score=0.473 total time=17.4min\n",
      "[CV 2/3; 358/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 354/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x28cf03b90>;, score=0.475 total time=17.3min\n",
      "[CV 3/3; 358/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 355/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.466 total time= 7.4min\n",
      "[CV 1/3; 359/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28124ce90>\n",
      "[CV 1/3; 359/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28124ce90>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 359/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28124f850>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 359/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x28124f850>;, score=nan total time=   0.3s\n",
      "[CV 3/3; 359/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16de6cb10>\n",
      "[CV 3/3; 359/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16de6cb10>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 360/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x28123ff10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 354/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15f3c7990>;, score=0.473 total time=17.2min\n",
      "[CV 2/3; 360/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x168ff51d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 358/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.465 total time= 7.2min\n",
      "[CV 3/3; 360/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10ad5ead0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 358/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.464 total time= 7.4min\n",
      "[CV 1/3; 361/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 361/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.452 total time=  27.6s\n",
      "[CV 2/3; 361/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 361/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.450 total time=  27.8s\n",
      "[CV 3/3; 361/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 361/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.450 total time=  27.5s\n",
      "[CV 1/3; 362/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15a984690>\n",
      "[CV 1/3; 362/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15a984690>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 362/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15b237f10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 362/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15b237f10>;, score=nan total time=   0.3s\n",
      "[CV 3/3; 362/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15a1c0450>\n",
      "[CV 3/3; 362/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15a1c0450>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 363/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15a9c9c10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 358/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.465 total time= 7.5min\n",
      "[CV 2/3; 363/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2923a1150>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 357/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16002edd0>;, score=0.471 total time=17.8min\n",
      "[CV 3/3; 363/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x17784f010>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 357/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x108f19a10>;, score=0.471 total time=17.4min\n",
      "[CV 1/3; 364/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 357/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d1d6e3d0>;, score=0.472 total time=17.4min\n",
      "[CV 2/3; 364/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 364/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.451 total time=  27.2s\n",
      "[CV 3/3; 364/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 364/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.449 total time=  27.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 365/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1158e8890>\n",
      "[CV 1/3; 365/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1158e8890>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 365/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1158eabd0>\n",
      "[CV 2/3; 365/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1158eabd0>;, score=nan total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 365/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x115bb7010>\n",
      "[CV 3/3; 365/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x115bb7010>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 366/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1158eff10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 364/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.450 total time=  28.8s\n",
      "[CV 2/3; 366/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10748f290>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 363/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15a9c9c10>;, score=0.458 total time=10.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 366/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c58e7d0>\n",
      "[CV 1/3; 360/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x28123ff10>;, score=0.469 total time=16.1min\n",
      "[CV 1/3; 367/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 363/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2923a1150>;, score=0.455 total time= 9.9min\n",
      "[CV 2/3; 367/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 367/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.448 total time=  21.1s\n",
      "[CV 2/3; 367/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.446 total time=  21.2s\n",
      "[CV 3/3; 367/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 368/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c9da1290>\n",
      "[CV 1/3; 368/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c9da1290>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 368/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2aa2edbd0>\n",
      "[CV 2/3; 368/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2aa2edbd0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 368/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2aa2ed2d0>\n",
      "[CV 3/3; 368/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2aa2ed2d0>;, score=nan total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 369/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x30f006cd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 367/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.447 total time=  23.4s\n",
      "[CV 2/3; 369/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a80b4f90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 360/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x168ff51d0>;, score=0.470 total time=15.7min\n",
      "[CV 3/3; 369/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1090a1c90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 360/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10ad5ead0>;, score=0.470 total time=16.1min\n",
      "[CV 1/3; 370/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 370/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.449 total time=  22.0s\n",
      "[CV 2/3; 370/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 370/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.447 total time=  20.9s\n",
      "[CV 3/3; 370/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 370/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.448 total time=  20.6s\n",
      "[CV 1/3; 371/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c548fad0>\n",
      "[CV 1/3; 371/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2c548fad0>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 371/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x29d7c7c90>\n",
      "[CV 2/3; 371/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x29d7c7c90>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 371/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x29d7c7390>\n",
      "[CV 3/3; 371/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x29d7c7390>;, score=nan total time=   0.1s\n",
      "[CV 1/3; 372/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d42fba50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 363/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x17784f010>;, score=0.455 total time= 9.9min\n",
      "[CV 2/3; 372/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b64a3990>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 366/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1158eff10>;, score=0.457 total time= 9.7min\n",
      "[CV 3/3; 372/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a825db50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 366/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10748f290>;, score=0.455 total time= 9.7min\n",
      "[CV 1/3; 373/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 373/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.408 total time=  21.0s\n",
      "[CV 2/3; 373/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 373/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.408 total time=  21.4s\n",
      "[CV 3/3; 373/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 373/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.408 total time=  20.4s\n",
      "[CV 1/3; 374/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d04ee90>\n",
      "[CV 1/3; 374/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d04ee90>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 374/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d44de50>\n",
      "[CV 2/3; 374/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d44de50>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 374/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10cfa3f10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 374/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10cfa3f10>;, score=nan total time=   0.2s\n",
      "[CV 1/3; 375/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c8abf10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 366/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c58e7d0>;, score=0.454 total time= 9.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 375/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10da61390>\n",
      "[CV 1/3; 369/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x30f006cd0>;, score=0.426 total time= 9.7min\n",
      "[CV 2/3; 369/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a80b4f90>;, score=0.427 total time= 9.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 375/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x148a64ed0>\n",
      "[CV 1/3; 376/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 376/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.408 total time=  21.5s\n",
      "[CV 2/3; 376/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 376/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.408 total time=  21.3s\n",
      "[CV 3/3; 376/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 369/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1090a1c90>;, score=0.426 total time= 9.6min\n",
      "[CV 1/3; 377/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2d48a3dd0>\n",
      "[CV 1/3; 377/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2d48a3dd0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 377/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2f00cff90>\n",
      "[CV 2/3; 377/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2f00cff90>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 377/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2efad0ad0>\n",
      "[CV 3/3; 377/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2efad0ad0>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 376/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.408 total time=  23.1s\n",
      "[CV 1/3; 378/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x3104fa090>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 378/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x17ba97690>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 372/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d42fba50>;, score=0.431 total time= 9.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 378/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11a5a3b10>\n",
      "[CV 2/3; 372/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b64a3990>;, score=0.431 total time= 9.7min\n",
      "[CV 1/3; 379/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 372/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a825db50>;, score=0.431 total time= 9.7min\n",
      "[CV 2/3; 379/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 375/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c8abf10>;, score=0.408 total time= 9.7min\n",
      "[CV 3/3; 379/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 375/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10da61390>;, score=0.408 total time= 9.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 380/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10cb21e50>\n",
      "[CV 1/3; 380/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10cb21e50>;, score=nan total time=   0.1s\n",
      "[CV 2/3; 380/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c7c6350>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 380/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c7c6350>;, score=nan total time=   0.3s\n",
      "[CV 3/3; 380/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c34a390>\n",
      "[CV 3/3; 380/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c34a390>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 381/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c33f2d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 375/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x148a64ed0>;, score=0.408 total time= 9.9min\n",
      "[CV 2/3; 381/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d426e010>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 379/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.520 total time= 6.1min\n",
      "[CV 3/3; 381/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11a415110>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 378/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x3104fa090>;, score=0.408 total time= 9.8min\n",
      "[CV 2/3; 378/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x17ba97690>;, score=0.408 total time= 9.8min\n",
      "[CV 1/3; 382/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 382/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 379/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.523 total time= 6.7min\n",
      "[CV 3/3; 382/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 378/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=MinMaxScaler(), preprocessor__ratio__scaler=MinMaxScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11a5a3b10>;, score=0.408 total time=10.0min\n",
      "[CV 1/3; 383/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2b6996990>\n",
      "[CV 1/3; 383/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2b6996990>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 383/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2b6995010>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 383/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2b6995010>;, score=nan total time=   0.3s\n",
      "[CV 3/3; 383/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x29d7e0fd0>\n",
      "[CV 3/3; 383/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x29d7e0fd0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 384/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x29d724650>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 379/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.521 total time= 7.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 384/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1234d6f50>\n",
      "[CV 1/3; 382/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.520 total time= 6.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 384/432] START model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16efa4850>\n",
      "[CV 2/3; 382/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.522 total time= 6.9min\n",
      "[CV 1/3; 385/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 382/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.521 total time= 7.6min\n",
      "[CV 2/3; 385/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 385/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.494 total time= 7.7min\n",
      "[CV 3/3; 385/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 381/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c33f2d0>;, score=0.522 total time=16.9min\n",
      "[CV 1/3; 386/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15f921e50>\n",
      "[CV 1/3; 386/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15f921e50>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 386/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15b27be10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 386/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15b27be10>;, score=nan total time=   0.9s\n",
      "[CV 3/3; 386/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15a761050>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 386/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x15a761050>;, score=nan total time=   1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 387/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x159d0ff10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 385/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.494 total time= 8.1min\n",
      "[CV 2/3; 387/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d0e869d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 381/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11a415110>;, score=0.521 total time=19.7min\n",
      "[CV 3/3; 387/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x12ad32110>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 381/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d426e010>;, score=0.522 total time=20.4min\n",
      "[CV 1/3; 388/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 384/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x29d724650>;, score=0.521 total time=17.0min\n",
      "[CV 2/3; 388/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 384/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1234d6f50>;, score=0.522 total time=17.7min\n",
      "[CV 3/3; 388/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 385/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.492 total time= 8.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 389/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c253990>\n",
      "[CV 1/3; 389/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c253990>;, score=nan total time=   0.1s\n",
      "[CV 2/3; 389/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c250950>\n",
      "[CV 2/3; 389/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c250950>;, score=nan total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 389/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c542850>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 389/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c542850>;, score=nan total time=   0.5s\n",
      "[CV 1/3; 390/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c546ad0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 384/432] END model=LinearSVC(), model__C=0.1, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16efa4850>;, score=0.519 total time=17.8min\n",
      "[CV 2/3; 390/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15c72cf10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 388/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.492 total time= 8.9min\n",
      "[CV 3/3; 390/432] START model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1187687d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 388/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.492 total time= 8.9min\n",
      "[CV 1/3; 391/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 388/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.491 total time= 9.2min\n",
      "[CV 2/3; 391/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 387/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x159d0ff10>;, score=0.493 total time=18.3min\n",
      "[CV 3/3; 391/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 387/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d0e869d0>;, score=0.493 total time=20.4min\n",
      "[CV 1/3; 392/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x118a51ed0>\n",
      "[CV 1/3; 392/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x118a51ed0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 392/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x118a563d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 392/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x118a563d0>;, score=nan total time=   0.5s\n",
      "[CV 3/3; 392/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x11873cbd0>\n",
      "[CV 3/3; 392/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x11873cbd0>;, score=nan total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 393/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11872c6d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 387/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x12ad32110>;, score=0.493 total time=20.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 393/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11eeee350>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 391/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.501 total time=11.2min\n",
      "[CV 3/3; 393/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x285eebe10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 390/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c546ad0>;, score=0.493 total time=17.9min\n",
      "[CV 1/3; 394/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 391/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.502 total time=11.0min\n",
      "[CV 2/3; 394/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 390/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15c72cf10>;, score=0.492 total time=18.1min\n",
      "[CV 3/3; 394/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 391/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.502 total time=10.7min\n",
      "[CV 1/3; 395/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10edb7cd0>\n",
      "[CV 1/3; 395/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10edb7cd0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 395/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10ed1ff10>\n",
      "[CV 2/3; 395/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10ed1ff10>;, score=nan total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 395/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10985f7d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 395/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10985f7d0>;, score=nan total time=   0.3s\n",
      "[CV 1/3; 396/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10ed07f10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 390/432] END model=LinearSVC(), model__C=0.1, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1187687d0>;, score=0.492 total time=17.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 396/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1181dc250>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 394/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.499 total time= 9.9min\n",
      "[CV 3/3; 396/432] START model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x17496ebd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 394/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.501 total time=10.0min\n",
      "[CV 1/3; 397/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 394/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.500 total time=10.1min\n",
      "[CV 2/3; 397/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 393/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11872c6d0>;, score=0.506 total time=21.5min\n",
      "[CV 3/3; 397/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 393/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x11eeee350>;, score=0.505 total time=21.7min\n",
      "[CV 1/3; 398/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x11ecb6090>\n",
      "[CV 1/3; 398/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x11ecb6090>;, score=nan total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 398/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2a833a990>\n",
      "[CV 2/3; 398/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2a833a990>;, score=nan total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 398/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1284aa5d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 398/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1284aa5d0>;, score=nan total time=   0.5s\n",
      "[CV 1/3; 399/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a7fbd2d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 393/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x285eebe10>;, score=0.504 total time=21.8min\n",
      "[CV 2/3; 399/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x14a0f4210>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 397/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.485 total time=11.6min\n",
      "[CV 3/3; 399/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10d3e9590>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 397/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.486 total time=11.6min\n",
      "[CV 1/3; 400/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 396/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10ed07f10>;, score=0.504 total time=20.8min\n",
      "[CV 2/3; 400/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 396/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1181dc250>;, score=0.503 total time=21.0min\n",
      "[CV 3/3; 400/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 397/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.485 total time=10.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 401/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c399050>\n",
      "[CV 1/3; 401/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c399050>;, score=nan total time=   0.1s\n",
      "[CV 2/3; 401/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c398610>\n",
      "[CV 2/3; 401/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c398610>;, score=nan total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 401/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c0ea790>\n",
      "[CV 3/3; 401/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c0ea790>;, score=nan total time=   0.1s\n",
      "[CV 1/3; 402/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c0c9d90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 396/432] END model=LinearSVC(), model__C=1.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x17496ebd0>;, score=0.503 total time=20.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 402/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x171f17f10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 400/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.483 total time=10.1min\n",
      "[CV 3/3; 402/432] START model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2aa8bea90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 400/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.484 total time=10.3min\n",
      "[CV 1/3; 403/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 400/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.483 total time=10.5min\n",
      "[CV 2/3; 403/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 399/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2a7fbd2d0>;, score=0.490 total time=22.1min\n",
      "[CV 3/3; 403/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 399/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x14a0f4210>;, score=0.490 total time=22.2min\n",
      "[CV 1/3; 404/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16b72ff10>\n",
      "[CV 1/3; 404/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16b72ff10>;, score=nan total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 404/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x169c25ad0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 404/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x169c25ad0>;, score=nan total time=   0.7s\n",
      "[CV 3/3; 404/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16b1a6010>\n",
      "[CV 3/3; 404/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16b1a6010>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 405/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16b1b4c50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 399/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10d3e9590>;, score=0.491 total time=22.4min\n",
      "[CV 2/3; 405/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10cdcce10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 403/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.472 total time=11.6min\n",
      "[CV 3/3; 405/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15a6b8090>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 403/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.471 total time=11.5min\n",
      "[CV 1/3; 406/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 402/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c0c9d90>;, score=0.489 total time=21.3min\n",
      "[CV 2/3; 406/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 402/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x171f17f10>;, score=0.488 total time=21.4min\n",
      "[CV 3/3; 406/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 403/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.472 total time=10.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 407/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17ec14210>\n",
      "[CV 1/3; 407/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17ec14210>;, score=nan total time=   0.1s\n",
      "[CV 2/3; 407/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17ec37f10>\n",
      "[CV 2/3; 407/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17ec37f10>;, score=nan total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 407/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17e934a10>\n",
      "[CV 3/3; 407/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x17e934a10>;, score=nan total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 408/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x17e929c10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 402/432] END model=LinearSVC(), model__C=1.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2aa8bea90>;, score=0.489 total time=20.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 408/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2abf92b50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 406/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.470 total time= 9.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 408/432] START model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2916c2810>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 406/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.469 total time=10.2min\n",
      "[CV 1/3; 409/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 406/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.470 total time=10.3min\n",
      "[CV 2/3; 409/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 405/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16b1b4c50>;, score=0.475 total time=21.4min\n",
      "[CV 3/3; 409/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 405/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10cdcce10>;, score=0.475 total time=21.6min\n",
      "[CV 1/3; 410/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x123005e50>\n",
      "[CV 1/3; 410/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x123005e50>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 410/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d64bf10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 410/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10d64bf10>;, score=nan total time=   0.7s\n",
      "[CV 3/3; 410/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x13304ab50>\n",
      "[CV 3/3; 410/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x13304ab50>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 411/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1226f1650>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 405/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15a6b8090>;, score=0.475 total time=21.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 411/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15dd19e10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 409/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.466 total time=11.2min\n",
      "[CV 3/3; 411/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d0082410>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 409/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.466 total time=11.1min\n",
      "[CV 1/3; 412/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 408/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x17e929c10>;, score=0.473 total time=20.9min\n",
      "[CV 2/3; 412/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 408/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2abf92b50>;, score=0.474 total time=20.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 412/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 409/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=0.466 total time=11.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 413/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c53aa10>\n",
      "[CV 1/3; 413/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c53aa10>;, score=nan total time=   0.1s\n",
      "[CV 2/3; 413/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c53ee50>\n",
      "[CV 2/3; 413/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c53ee50>;, score=nan total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 413/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c287f10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 413/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c287f10>;, score=nan total time=   0.4s\n",
      "[CV 1/3; 414/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c812490>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 408/432] END model=LinearSVC(), model__C=10.0, model__class_weight=None, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2916c2810>;, score=0.473 total time=20.5min\n",
      "[CV 2/3; 414/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1663af310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 412/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.465 total time=10.4min\n",
      "[CV 3/3; 414/432] START model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10b854b50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 412/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.464 total time=10.7min\n",
      "[CV 1/3; 415/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 412/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=0.465 total time=10.7min\n",
      "[CV 2/3; 415/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 415/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  20.2s\n",
      "[CV 3/3; 415/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 415/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  19.6s\n",
      "[CV 1/3; 416/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2aa450c90>\n",
      "[CV 1/3; 416/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2aa450c90>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 416/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x29cb97e90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 416/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x29cb97e90>;, score=nan total time=   0.2s\n",
      "[CV 3/3; 416/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2d030f610>\n",
      "[CV 3/3; 416/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2d030f610>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 415/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  21.5s\n",
      "[CV 1/3; 417/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b9f52010>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 417/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2aaf45150>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 411/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1226f1650>;, score=0.471 total time=22.4min\n",
      "[CV 3/3; 417/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1227e9290>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 411/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x15dd19e10>;, score=0.471 total time=22.4min\n",
      "[CV 1/3; 418/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 418/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  19.2s\n",
      "[CV 2/3; 418/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 418/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  19.0s\n",
      "[CV 3/3; 418/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 418/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  18.5s\n",
      "[CV 1/3; 419/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10bfb1d10>\n",
      "[CV 1/3; 419/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10bfb1d10>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 419/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10bfb1010>\n",
      "[CV 2/3; 419/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10bfb1010>;, score=nan total time=   0.1s\n",
      "[CV 3/3; 419/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10bf57f10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 419/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10bf57f10>;, score=nan total time=   0.2s\n",
      "[CV 1/3; 420/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c3a4bd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 417/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2b9f52010>;, score=nan total time= 7.2min\n",
      "[CV 2/3; 417/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2aaf45150>;, score=nan total time= 7.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 420/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1380abbd0>\n",
      "[CV 3/3; 420/432] START model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d54a3c10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 411/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d0082410>;, score=0.472 total time=22.0min\n",
      "[CV 1/3; 421/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 421/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  18.0s\n",
      "[CV 2/3; 421/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 421/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  18.0s\n",
      "[CV 3/3; 421/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 421/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  16.1s\n",
      "[CV 1/3; 422/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x119ab9210>\n",
      "[CV 1/3; 422/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x119ab9210>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 422/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x11990a5d0>\n",
      "[CV 2/3; 422/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x11990a5d0>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 422/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x119909010>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 422/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x119909010>;, score=nan total time=   0.3s\n",
      "[CV 1/3; 423/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1199d7d50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 417/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1227e9290>;, score=nan total time= 7.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 423/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16e10bf10>\n",
      "[CV 1/3; 414/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c812490>;, score=0.469 total time=20.7min\n",
      "[CV 3/3; 423/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10bfa58d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 414/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1663af310>;, score=0.470 total time=20.1min\n",
      "[CV 1/3; 424/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 424/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  16.0s\n",
      "[CV 1/3; 420/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c3a4bd0>;, score=nan total time= 7.0min\n",
      "[CV 2/3; 424/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 424/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 424/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  16.6s\n",
      "[CV 3/3; 424/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  16.4s\n",
      "[CV 1/3; 425/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16dc3f8d0>\n",
      "[CV 1/3; 425/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x16dc3f8d0>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 425/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c6816d0>\n",
      "[CV 2/3; 425/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x10c6816d0>;, score=nan total time=   0.0s\n",
      "[CV 3/3; 425/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2d528fbd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 425/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x2d528fbd0>;, score=nan total time=   0.4s\n",
      "[CV 1/3; 426/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c667f10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 426/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2cda0b990>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 420/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1380abbd0>;, score=nan total time= 6.8min\n",
      "[CV 3/3; 420/432] END model=MultinomialNB(), model__alpha=0.1, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2d54a3c10>;, score=nan total time= 6.8min\n",
      "[CV 3/3; 426/432] START model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x122ac7810>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 427/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 427/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  14.4s\n",
      "[CV 2/3; 427/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 427/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  14.6s\n",
      "[CV 3/3; 427/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 427/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  15.1s\n",
      "[CV 1/3; 428/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1202e53d0>\n",
      "[CV 1/3; 428/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1202e53d0>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 428/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1202e6050>\n",
      "[CV 2/3; 428/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1202e6050>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 428/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x12081d790>\n",
      "[CV 3/3; 428/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x12081d790>;, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 429/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x120826010>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 423/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x1199d7d50>;, score=nan total time= 6.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 429/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x13a4d9650>\n",
      "[CV 3/3; 414/432] END model=LinearSVC(), model__C=10.0, model__class_weight=balanced, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10b854b50>;, score=0.470 total time=19.0min\n",
      "[CV 3/3; 429/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10be6db50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 423/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x16e10bf10>;, score=nan total time= 6.8min\n",
      "[CV 1/3; 430/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 1/3; 430/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  15.4s\n",
      "[CV 3/3; 423/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10bfa58d0>;, score=nan total time= 6.7min\n",
      "[CV 2/3; 430/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 3/3; 430/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None\n",
      "[CV 2/3; 430/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  15.5s\n",
      "[CV 3/3; 430/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=None;, score=nan total time=  15.1s\n",
      "[CV 1/3; 431/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1221dde50>\n",
      "[CV 1/3; 431/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x1221dde50>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 431/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x148f75350>\n",
      "[CV 2/3; 431/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x148f75350>;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 431/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x122408e50>\n",
      "[CV 3/3; 431/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.LemmaTokenizer object at 0x122408e50>;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 432/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10d29f050>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 432/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10cf27dd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 426/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10c667f10>;, score=nan total time= 6.7min\n",
      "[CV 3/3; 432/432] START model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10b82e2d0>\n",
      "[CV 2/3; 426/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x2cda0b990>;, score=nan total time= 6.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 426/432] END model=MultinomialNB(), model__alpha=1.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x122ac7810>;, score=nan total time= 6.6min\n",
      "[CV 1/3; 429/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x120826010>;, score=nan total time= 6.6min\n",
      "[CV 2/3; 429/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x13a4d9650>;, score=nan total time= 6.6min\n",
      "[CV 3/3; 429/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=None, preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10be6db50>;, score=nan total time= 6.5min\n",
      "[CV 1/3; 432/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10d29f050>;, score=nan total time= 6.5min\n",
      "[CV 2/3; 432/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10cf27dd0>;, score=nan total time= 6.5min\n",
      "[CV 3/3; 432/432] END model=MultinomialNB(), model__alpha=10.0, preprocessor__date__scaler=StandardScaler(), preprocessor__ratio__scaler=StandardScaler(), preprocessor__text__vectorizer__stop_words=['doing', 'shouldn', \"you'd\", 'do', 'yourself', 'before', 'weren', 'through', 'over', 'ours', 'this', 'needn', 'in', 'aren', 'an', 'were', 'o', 'which', \"weren't\", 'while', 'no', 'by', 'them', 'out', 'nor', 'herself', 'themselves', 'did', 'isn', 'when', \"it's\", 'our', 'are', 'very', 'him', 'or', 'there', \"she's\", \"that'll\", \"wasn't\", 'a', 'hers', 'at', 'm', 'the', 'of', 'few', \"hadn't\", 'does', 'because', 'itself', 'theirs', 'hadn', 'between', \"you'll\", \"doesn't\", 'into', 'didn', 'shan', 'where', \"should've\", 'for', 'himself', 'but', 'why', 'these', \"didn't\", 'wasn', 'doesn', 'most', 'only', 'hasn', 'her', 'not', 'me', 'just', 'won', 'what', 'they', 'all', \"shouldn't\", 'own', \"you've\", \"shan't\", 'up', 'have', 'then', 'some', 'as', 'both', 'too', 're', 'haven', 'ourselves', 'here', 've', 'mightn', \"mustn't\", 'll', 'has', \"couldn't\", 'under', 'so', 'those', 'more', 'couldn', 'yourselves', \"won't\", 'his', 'than', 'mustn', 'was', 'been', 't', 'i', 'she', 'd', 'wouldn', 'other', \"isn't\", \"wouldn't\", 'should', 'now', 'off', 'having', 'their', 'below', 's', 'will', 'is', 'above', 'ain', 'y', 'who', 'on', 'yours', 'any', 'we', \"hasn't\", 'ma', \"needn't\", 'it', 'being', 'if', 'had', 'further', 'until', 'after', 'down', 'to', 'during', 'how', 'that', 'same', 'your', 'you', 'from', 'each', 'my', 'don', 'whom', 'about', 'myself', 'he', 'and', 'with', 'can', \"don't\", 'be', \"you're\", 'am', \"mightn't\", 'again', 'such', \"aren't\", \"haven't\", 'its', 'once', 'against'], preprocessor__text__vectorizer__tokenizer=<__main__.StemmingTokenizer object at 0x10b82e2d0>;, score=nan total time= 6.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "306 fits failed out of a total of 1296.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "198 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/naive_bayes.py\", line 776, in fit\n",
      "    self._count(X, Y)\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/naive_bayes.py\", line 898, in _count\n",
      "    check_non_negative(X, \"MultinomialNB (input X)\")\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1418, in check_non_negative\n",
      "    raise ValueError(\"Negative values in data passed to %s\" % whom)\n",
      "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "108 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/corpus/util.py\", line 84, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{zip_name}\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/data.py\", line 583, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mwordnet\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('wordnet')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/Users/tillgrutschus/nltk_data'\n",
      "    - '/opt/homebrew/anaconda3/envs/uu-data-mining/nltk_data'\n",
      "    - '/opt/homebrew/anaconda3/envs/uu-data-mining/share/nltk_data'\n",
      "    - '/opt/homebrew/anaconda3/envs/uu-data-mining/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/pipeline.py\", line 401, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/pipeline.py\", line 359, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py\", line 727, in fit_transform\n",
      "    result = self._fit_transform(X, y, _fit_transform_one)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py\", line 658, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/joblib/parallel.py\", line 1098, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/joblib/parallel.py\", line 975, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/multiprocessing/pool.py\", line 774, in get\n",
      "    raise self._value\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/pipeline.py\", line 445, in fit_transform\n",
      "    return last_step.fit_transform(Xt, y, **fit_params_last_step)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\", line 2133, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\", line 1388, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\", line 1275, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "                   ^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\", line 113, in _analyze\n",
      "    doc = tokenizer(doc)\n",
      "          ^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/_p/1z6zm9vn1h909hckkyp2mjhc0000gn/T/ipykernel_55720/2406683290.py\", line 41, in __call__\n",
      "  File \"/var/folders/_p/1z6zm9vn1h909hckkyp2mjhc0000gn/T/ipykernel_55720/2406683290.py\", line 42, in <listcomp>\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/stem/wordnet.py\", line 45, in lemmatize\n",
      "    lemmas = wn._morphy(word, pos)\n",
      "             ^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/corpus/util.py\", line 121, in __getattr__\n",
      "    self.__load()\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/corpus/util.py\", line 86, in __load\n",
      "    raise e\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/corpus/util.py\", line 81, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{self.__name}\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/data.py\", line 583, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mwordnet\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('wordnet')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/Users/tillgrutschus/nltk_data'\n",
      "    - '/opt/homebrew/anaconda3/envs/uu-data-mining/nltk_data'\n",
      "    - '/opt/homebrew/anaconda3/envs/uu-data-mining/share/nltk_data'\n",
      "    - '/opt/homebrew/anaconda3/envs/uu-data-mining/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.52108847 0.52109735 0.51151894 0.5115219  0.51523434 0.51508644\n",
      " 0.52116242 0.5211713  0.51183842 0.51183842 0.51537337 0.51533492\n",
      " 0.52143161 0.52142866 0.51240046 0.51240046 0.51599458 0.51604487\n",
      " 0.49269047 0.49268752 0.46821199 0.46821199 0.47829624 0.47829328\n",
      " 0.49270526 0.49271414 0.4687977  0.4687977  0.47831695 0.47835836\n",
      " 0.49329393 0.49329393 0.46926508 0.46926804 0.47891449 0.4789204\n",
      " 0.50078094 0.50075728 0.51119059 0.51119059 0.50661141 0.50656112\n",
      " 0.5008194  0.50081644 0.51169051 0.51169051 0.50673565 0.50679185\n",
      " 0.50166838 0.50166838 0.51204253 0.51204253 0.50706696 0.50705809\n",
      " 0.48389005 0.48389597 0.46794576 0.46794576 0.46742217 0.46748429\n",
      " 0.48411487 0.484106   0.4688894  0.46888644 0.46776531 0.46776236\n",
      " 0.48487511 0.4848899  0.46926804 0.46926804 0.46822087 0.46820016\n",
      " 0.47034184 0.47037142 0.5111758  0.5111758  0.50413546 0.50419758\n",
      " 0.47048975 0.47047496 0.51160177 0.51159289 0.50415025 0.504168\n",
      " 0.4714541  0.4714541  0.51212831 0.51213127 0.50408221 0.50408517\n",
      " 0.46504976 0.46507046 0.46641049 0.46865867 0.46476578 0.46493735\n",
      " 0.46525682 0.46525387 0.46903435 0.46873558 0.46501426 0.46479536\n",
      " 0.46582478 0.46582774 0.46945145 0.46989516 0.46512075 0.46449363\n",
      " 0.45008164 0.45008164 0.44450558 0.44450558 0.43562233 0.43562529\n",
      " 0.45017335 0.45017335 0.44483985 0.44483985 0.43564895 0.43564895\n",
      " 0.45079751 0.45079751 0.44678038 0.44678038 0.43629087 0.4363027\n",
      " 0.44772994 0.44772994 0.4444967  0.4444967  0.43565191 0.43564304\n",
      " 0.44757611 0.44757907 0.44483985 0.44483985 0.43573474 0.43577319\n",
      " 0.44709985 0.44710873 0.44673009 0.44673009 0.43644173 0.43643286\n",
      " 0.40787216 0.40787216 0.44474815 0.44474815 0.44009501 0.44009797\n",
      " 0.40787216 0.40787216 0.44505875 0.44505875 0.44025771 0.44024884\n",
      " 0.4078692  0.4078692  0.44710281 0.44710281 0.44114515 0.44116881\n",
      " 0.52107368 0.5210796  0.51152485 0.51152485 0.51528759 0.51513672\n",
      " 0.52122455 0.52124229 0.511868   0.511868   0.51535563 0.51534379\n",
      " 0.52144345 0.52144345 0.51240638 0.51240638 0.51599458 0.51605374\n",
      " 0.49270231 0.49270526 0.46820312 0.46821791 0.47828736 0.47832878\n",
      " 0.49277034 0.49277922 0.46883024 0.46881841 0.47834948 0.47836132\n",
      " 0.49332647 0.49334718 0.46927987 0.46927987 0.47892632 0.47893815\n",
      " 0.50078686 0.50080165 0.51115213 0.51119354 0.50664691 0.50655816\n",
      " 0.50081644 0.50081053 0.51172896 0.51169347 0.50679481 0.5067534\n",
      " 0.50161514 0.50160922 0.51206027 0.51207802 0.50712612 0.50704034\n",
      " 0.48392851 0.48399063 0.46818241 0.46817058 0.4674695  0.46761741\n",
      " 0.48413853 0.48413558 0.46903435 0.46902843 0.46782152 0.46777715\n",
      " 0.48490173 0.4848899  0.46934791 0.46940116 0.46822087 0.46828594\n",
      " 0.47057849 0.47043946 0.51127046 0.51094802 0.50373611 0.50379232\n",
      " 0.47062286 0.47059624 0.51162543 0.51181771 0.50407334 0.50404967\n",
      " 0.4714748  0.47147184 0.51131483 0.51193603 0.50401713 0.50419166\n",
      " 0.46524499 0.46534853 0.47578183 0.47550968 0.46941299 0.46729793\n",
      " 0.46533374 0.4654491  0.47631134 0.46949582 0.46888349 0.46655544\n",
      " 0.46602002 0.46606735 0.47927834 0.47416965 0.46586028 0.46985671\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.52143457        nan 0.52155881 0.52082224        nan 0.52068025\n",
      " 0.49329097        nan 0.49276147 0.49167288        nan 0.49203377\n",
      " 0.50166838        nan 0.50511756 0.50004141        nan 0.50338114\n",
      " 0.48487215        nan 0.49003408 0.48343154        nan 0.48875617\n",
      " 0.47145705        nan 0.47481157 0.4699632         nan 0.47344787\n",
      " 0.46582774        nan 0.4713476  0.46454392        nan 0.46972655\n",
      " 0.45079751        nan 0.45600085 0.45001952        nan 0.45544768\n",
      " 0.44709985        nan 0.42642552 0.44783939        nan 0.43104315\n",
      " 0.4078692         nan 0.4078692  0.40788991        nan 0.4078692\n",
      " 0.52144345        nan 0.52161502 0.52087549        nan 0.52071575\n",
      " 0.49332647        nan 0.49284134 0.49165513        nan 0.49210772\n",
      " 0.50161218        nan 0.50511164 0.50008579        nan 0.50336339\n",
      " 0.48489581        nan 0.49000745 0.48340492        nan 0.48876208\n",
      " 0.47169666        nan 0.47471987 0.4699632         nan 0.47344196\n",
      " 0.4659017         nan 0.47146593 0.46468886        nan 0.4698005\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Warning - Takes ages!\n",
    "search.fit(train_df, train_df[\"citation_bucket\"])\n",
    "save_model(search, \"models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = load_model(\"gridsearch\", \"models\")\n",
    "cv_results = pd.DataFrame(search.cv_results_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the results of the Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_model</th>\n",
       "      <th>param_model__C</th>\n",
       "      <th>param_model__class_weight</th>\n",
       "      <th>param_preprocessor__date__scaler</th>\n",
       "      <th>param_preprocessor__ratio__scaler</th>\n",
       "      <th>param_preprocessor__text__vectorizer__max_df</th>\n",
       "      <th>...</th>\n",
       "      <th>param_model__alpha</th>\n",
       "      <th>param_preprocessor__text__vectorizer__stop_words</th>\n",
       "      <th>param_preprocessor__text__vectorizer__tokenizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>402.458091</td>\n",
       "      <td>1.176289</td>\n",
       "      <td>194.640154</td>\n",
       "      <td>0.884686</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.StemmingTokenizer object at 0x2b87d8...</td>\n",
       "      <td>{'model': MultinomialNB(), 'model__alpha': 0.1...</td>\n",
       "      <td>0.457776</td>\n",
       "      <td>0.455096</td>\n",
       "      <td>0.455131</td>\n",
       "      <td>0.456001</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "362     402.458091      1.176289       194.640154        0.884686   \n",
       "\n",
       "         param_model param_model__C param_model__class_weight  \\\n",
       "362  MultinomialNB()            NaN                       NaN   \n",
       "\n",
       "    param_preprocessor__date__scaler param_preprocessor__ratio__scaler  \\\n",
       "362                   MinMaxScaler()                    MinMaxScaler()   \n",
       "\n",
       "    param_preprocessor__text__vectorizer__max_df  ... param_model__alpha  \\\n",
       "362                                          NaN  ...                0.1   \n",
       "\n",
       "    param_preprocessor__text__vectorizer__stop_words  \\\n",
       "362                                             None   \n",
       "\n",
       "       param_preprocessor__text__vectorizer__tokenizer  \\\n",
       "362  <__main__.StemmingTokenizer object at 0x2b87d8...   \n",
       "\n",
       "                                                params split0_test_score  \\\n",
       "362  {'model': MultinomialNB(), 'model__alpha': 0.1...          0.457776   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "362          0.455096           0.455131         0.456001        0.001255   \n",
       "\n",
       "     rank_test_score  \n",
       "362              265  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(cv_results[cv_results[\"param_model\"].astype(str).str.contains(\"MultinomialNB\")].sort_values(\"rank_test_score\").head(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> We use the LinearSVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_model</th>\n",
       "      <th>param_model__C</th>\n",
       "      <th>param_model__class_weight</th>\n",
       "      <th>param_preprocessor__date__scaler</th>\n",
       "      <th>param_preprocessor__ratio__scaler</th>\n",
       "      <th>param_preprocessor__text__vectorizer__max_df</th>\n",
       "      <th>...</th>\n",
       "      <th>param_model__alpha</th>\n",
       "      <th>param_preprocessor__text__vectorizer__stop_words</th>\n",
       "      <th>param_preprocessor__text__vectorizer__tokenizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38.837949</td>\n",
       "      <td>1.054323</td>\n",
       "      <td>9.715375</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.519879</td>\n",
       "      <td>0.522301</td>\n",
       "      <td>0.521086</td>\n",
       "      <td>0.521088</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.734405</td>\n",
       "      <td>0.121402</td>\n",
       "      <td>9.690159</td>\n",
       "      <td>0.122804</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.519879</td>\n",
       "      <td>0.522328</td>\n",
       "      <td>0.521086</td>\n",
       "      <td>0.521097</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.384088</td>\n",
       "      <td>4.302849</td>\n",
       "      <td>8.487338</td>\n",
       "      <td>0.322813</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.511874</td>\n",
       "      <td>0.511528</td>\n",
       "      <td>0.511155</td>\n",
       "      <td>0.511519</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.294612</td>\n",
       "      <td>0.266356</td>\n",
       "      <td>8.703293</td>\n",
       "      <td>0.170315</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.511883</td>\n",
       "      <td>0.511528</td>\n",
       "      <td>0.511155</td>\n",
       "      <td>0.511522</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.202874</td>\n",
       "      <td>0.178631</td>\n",
       "      <td>8.765668</td>\n",
       "      <td>0.313946</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.514714</td>\n",
       "      <td>0.516063</td>\n",
       "      <td>0.514927</td>\n",
       "      <td>0.515234</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.308460</td>\n",
       "      <td>0.251459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.LemmaTokenizer object at 0x1664f9210&gt;</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1113.252318</td>\n",
       "      <td>7.008182</td>\n",
       "      <td>223.910202</td>\n",
       "      <td>6.115537</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.StemmingTokenizer object at 0x2b87d8...</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>0.470812</td>\n",
       "      <td>0.471194</td>\n",
       "      <td>0.472392</td>\n",
       "      <td>0.471466</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>626.791677</td>\n",
       "      <td>8.300176</td>\n",
       "      <td>10.630937</td>\n",
       "      <td>0.491366</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[doing, shouldn, you'd, do, yourself, before, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>0.465159</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>0.464991</td>\n",
       "      <td>0.464689</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>0.374433</td>\n",
       "      <td>0.191894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[doing, shouldn, you'd, do, yourself, before, ...</td>\n",
       "      <td>&lt;__main__.LemmaTokenizer object at 0x1664f9210&gt;</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>983.944981</td>\n",
       "      <td>38.647208</td>\n",
       "      <td>212.710723</td>\n",
       "      <td>4.590152</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[doing, shouldn, you'd, do, yourself, before, ...</td>\n",
       "      <td>&lt;__main__.StemmingTokenizer object at 0x2b87d8...</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>0.469366</td>\n",
       "      <td>0.469774</td>\n",
       "      <td>0.470262</td>\n",
       "      <td>0.469801</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        38.837949      1.054323         9.715375        0.103300   \n",
       "1        39.734405      0.121402         9.690159        0.122804   \n",
       "2        32.384088      4.302849         8.487338        0.322813   \n",
       "3        27.294612      0.266356         8.703293        0.170315   \n",
       "4        29.202874      0.178631         8.765668        0.313946   \n",
       "..             ...           ...              ...             ...   \n",
       "409       0.308460      0.251459         0.000000        0.000000   \n",
       "410    1113.252318      7.008182       223.910202        6.115537   \n",
       "411     626.791677      8.300176        10.630937        0.491366   \n",
       "412       0.374433      0.191894         0.000000        0.000000   \n",
       "413     983.944981     38.647208       212.710723        4.590152   \n",
       "\n",
       "          param_model param_model__C param_model__class_weight  \\\n",
       "0    LinearSVC(C=0.1)            0.1                      None   \n",
       "1    LinearSVC(C=0.1)            0.1                      None   \n",
       "2    LinearSVC(C=0.1)            0.1                      None   \n",
       "3    LinearSVC(C=0.1)            0.1                      None   \n",
       "4    LinearSVC(C=0.1)            0.1                      None   \n",
       "..                ...            ...                       ...   \n",
       "409  LinearSVC(C=0.1)           10.0                  balanced   \n",
       "410  LinearSVC(C=0.1)           10.0                  balanced   \n",
       "411  LinearSVC(C=0.1)           10.0                  balanced   \n",
       "412  LinearSVC(C=0.1)           10.0                  balanced   \n",
       "413  LinearSVC(C=0.1)           10.0                  balanced   \n",
       "\n",
       "    param_preprocessor__date__scaler param_preprocessor__ratio__scaler  \\\n",
       "0                     MinMaxScaler()                    MinMaxScaler()   \n",
       "1                     MinMaxScaler()                    MinMaxScaler()   \n",
       "2                     MinMaxScaler()                    MinMaxScaler()   \n",
       "3                     MinMaxScaler()                    MinMaxScaler()   \n",
       "4                     MinMaxScaler()                    MinMaxScaler()   \n",
       "..                               ...                               ...   \n",
       "409                 StandardScaler()                  StandardScaler()   \n",
       "410                 StandardScaler()                  StandardScaler()   \n",
       "411                 StandardScaler()                  StandardScaler()   \n",
       "412                 StandardScaler()                  StandardScaler()   \n",
       "413                 StandardScaler()                  StandardScaler()   \n",
       "\n",
       "    param_preprocessor__text__vectorizer__max_df  ... param_model__alpha  \\\n",
       "0                                            0.5  ...                NaN   \n",
       "1                                            0.5  ...                NaN   \n",
       "2                                            0.5  ...                NaN   \n",
       "3                                            0.5  ...                NaN   \n",
       "4                                            0.5  ...                NaN   \n",
       "..                                           ...  ...                ...   \n",
       "409                                          NaN  ...                NaN   \n",
       "410                                          NaN  ...                NaN   \n",
       "411                                          NaN  ...                NaN   \n",
       "412                                          NaN  ...                NaN   \n",
       "413                                          NaN  ...                NaN   \n",
       "\n",
       "      param_preprocessor__text__vectorizer__stop_words  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "409                                               None   \n",
       "410                                               None   \n",
       "411  [doing, shouldn, you'd, do, yourself, before, ...   \n",
       "412  [doing, shouldn, you'd, do, yourself, before, ...   \n",
       "413  [doing, shouldn, you'd, do, yourself, before, ...   \n",
       "\n",
       "       param_preprocessor__text__vectorizer__tokenizer  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "409    <__main__.LemmaTokenizer object at 0x1664f9210>   \n",
       "410  <__main__.StemmingTokenizer object at 0x2b87d8...   \n",
       "411                                               None   \n",
       "412    <__main__.LemmaTokenizer object at 0x1664f9210>   \n",
       "413  <__main__.StemmingTokenizer object at 0x2b87d8...   \n",
       "\n",
       "                                                params split0_test_score  \\\n",
       "0    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.519879   \n",
       "1    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.519879   \n",
       "2    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.511874   \n",
       "3    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.511883   \n",
       "4    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.514714   \n",
       "..                                                 ...               ...   \n",
       "409  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...               NaN   \n",
       "410  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...          0.470812   \n",
       "411  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...          0.465159   \n",
       "412  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...               NaN   \n",
       "413  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...          0.469366   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0            0.522301           0.521086         0.521088        0.000989   \n",
       "1            0.522328           0.521086         0.521097        0.001000   \n",
       "2            0.511528           0.511155         0.511519        0.000294   \n",
       "3            0.511528           0.511155         0.511522        0.000297   \n",
       "4            0.516063           0.514927         0.515234        0.000592   \n",
       "..                ...                ...              ...             ...   \n",
       "409               NaN                NaN              NaN             NaN   \n",
       "410          0.471194           0.472392         0.471466        0.000673   \n",
       "411          0.463917           0.464991         0.464689        0.000550   \n",
       "412               NaN                NaN              NaN             NaN   \n",
       "413          0.469774           0.470262         0.469801        0.000366   \n",
       "\n",
       "     rank_test_score  \n",
       "0                 14  \n",
       "1                 13  \n",
       "2                 59  \n",
       "3                 58  \n",
       "4                 30  \n",
       "..               ...  \n",
       "409              331  \n",
       "410              177  \n",
       "411              262  \n",
       "412              331  \n",
       "413              194  \n",
       "\n",
       "[288 rows x 22 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = cv_results[cv_results[\"param_model\"].astype(str).str.contains(\"LinearSVC\")].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models have not converged. We drop them from consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_model</th>\n",
       "      <th>param_model__C</th>\n",
       "      <th>param_model__class_weight</th>\n",
       "      <th>param_preprocessor__date__scaler</th>\n",
       "      <th>param_preprocessor__ratio__scaler</th>\n",
       "      <th>param_preprocessor__text__vectorizer__max_df</th>\n",
       "      <th>...</th>\n",
       "      <th>param_model__alpha</th>\n",
       "      <th>param_preprocessor__text__vectorizer__stop_words</th>\n",
       "      <th>param_preprocessor__text__vectorizer__tokenizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38.837949</td>\n",
       "      <td>1.054323</td>\n",
       "      <td>9.715375</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.519879</td>\n",
       "      <td>0.522301</td>\n",
       "      <td>0.521086</td>\n",
       "      <td>0.521088</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.734405</td>\n",
       "      <td>0.121402</td>\n",
       "      <td>9.690159</td>\n",
       "      <td>0.122804</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.519879</td>\n",
       "      <td>0.522328</td>\n",
       "      <td>0.521086</td>\n",
       "      <td>0.521097</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.384088</td>\n",
       "      <td>4.302849</td>\n",
       "      <td>8.487338</td>\n",
       "      <td>0.322813</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.511874</td>\n",
       "      <td>0.511528</td>\n",
       "      <td>0.511155</td>\n",
       "      <td>0.511519</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.294612</td>\n",
       "      <td>0.266356</td>\n",
       "      <td>8.703293</td>\n",
       "      <td>0.170315</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.511883</td>\n",
       "      <td>0.511528</td>\n",
       "      <td>0.511155</td>\n",
       "      <td>0.511522</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.202874</td>\n",
       "      <td>0.178631</td>\n",
       "      <td>8.765668</td>\n",
       "      <td>0.313946</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.514714</td>\n",
       "      <td>0.516063</td>\n",
       "      <td>0.514927</td>\n",
       "      <td>0.515234</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1015.551673</td>\n",
       "      <td>9.235389</td>\n",
       "      <td>224.987650</td>\n",
       "      <td>3.563917</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>None</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[doing, shouldn, you'd, do, yourself, before, ...</td>\n",
       "      <td>&lt;__main__.StemmingTokenizer object at 0x2b87d8...</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>0.473155</td>\n",
       "      <td>0.474371</td>\n",
       "      <td>0.472800</td>\n",
       "      <td>0.473442</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>656.491602</td>\n",
       "      <td>5.376175</td>\n",
       "      <td>10.435120</td>\n",
       "      <td>0.922083</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>0.465701</td>\n",
       "      <td>0.466073</td>\n",
       "      <td>0.465931</td>\n",
       "      <td>0.465902</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1113.252318</td>\n",
       "      <td>7.008182</td>\n",
       "      <td>223.910202</td>\n",
       "      <td>6.115537</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.StemmingTokenizer object at 0x2b87d8...</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>0.470812</td>\n",
       "      <td>0.471194</td>\n",
       "      <td>0.472392</td>\n",
       "      <td>0.471466</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>626.791677</td>\n",
       "      <td>8.300176</td>\n",
       "      <td>10.630937</td>\n",
       "      <td>0.491366</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[doing, shouldn, you'd, do, yourself, before, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>0.465159</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>0.464991</td>\n",
       "      <td>0.464689</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>983.944981</td>\n",
       "      <td>38.647208</td>\n",
       "      <td>212.710723</td>\n",
       "      <td>4.590152</td>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[doing, shouldn, you'd, do, yourself, before, ...</td>\n",
       "      <td>&lt;__main__.StemmingTokenizer object at 0x2b87d8...</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 10.0, ...</td>\n",
       "      <td>0.469366</td>\n",
       "      <td>0.469774</td>\n",
       "      <td>0.470262</td>\n",
       "      <td>0.469801</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        38.837949      1.054323         9.715375        0.103300   \n",
       "1        39.734405      0.121402         9.690159        0.122804   \n",
       "2        32.384088      4.302849         8.487338        0.322813   \n",
       "3        27.294612      0.266356         8.703293        0.170315   \n",
       "4        29.202874      0.178631         8.765668        0.313946   \n",
       "..             ...           ...              ...             ...   \n",
       "407    1015.551673      9.235389       224.987650        3.563917   \n",
       "408     656.491602      5.376175        10.435120        0.922083   \n",
       "410    1113.252318      7.008182       223.910202        6.115537   \n",
       "411     626.791677      8.300176        10.630937        0.491366   \n",
       "413     983.944981     38.647208       212.710723        4.590152   \n",
       "\n",
       "          param_model param_model__C param_model__class_weight  \\\n",
       "0    LinearSVC(C=0.1)            0.1                      None   \n",
       "1    LinearSVC(C=0.1)            0.1                      None   \n",
       "2    LinearSVC(C=0.1)            0.1                      None   \n",
       "3    LinearSVC(C=0.1)            0.1                      None   \n",
       "4    LinearSVC(C=0.1)            0.1                      None   \n",
       "..                ...            ...                       ...   \n",
       "407  LinearSVC(C=0.1)           10.0                      None   \n",
       "408  LinearSVC(C=0.1)           10.0                  balanced   \n",
       "410  LinearSVC(C=0.1)           10.0                  balanced   \n",
       "411  LinearSVC(C=0.1)           10.0                  balanced   \n",
       "413  LinearSVC(C=0.1)           10.0                  balanced   \n",
       "\n",
       "    param_preprocessor__date__scaler param_preprocessor__ratio__scaler  \\\n",
       "0                     MinMaxScaler()                    MinMaxScaler()   \n",
       "1                     MinMaxScaler()                    MinMaxScaler()   \n",
       "2                     MinMaxScaler()                    MinMaxScaler()   \n",
       "3                     MinMaxScaler()                    MinMaxScaler()   \n",
       "4                     MinMaxScaler()                    MinMaxScaler()   \n",
       "..                               ...                               ...   \n",
       "407                 StandardScaler()                  StandardScaler()   \n",
       "408                 StandardScaler()                  StandardScaler()   \n",
       "410                 StandardScaler()                  StandardScaler()   \n",
       "411                 StandardScaler()                  StandardScaler()   \n",
       "413                 StandardScaler()                  StandardScaler()   \n",
       "\n",
       "    param_preprocessor__text__vectorizer__max_df  ... param_model__alpha  \\\n",
       "0                                            0.5  ...                NaN   \n",
       "1                                            0.5  ...                NaN   \n",
       "2                                            0.5  ...                NaN   \n",
       "3                                            0.5  ...                NaN   \n",
       "4                                            0.5  ...                NaN   \n",
       "..                                           ...  ...                ...   \n",
       "407                                          NaN  ...                NaN   \n",
       "408                                          NaN  ...                NaN   \n",
       "410                                          NaN  ...                NaN   \n",
       "411                                          NaN  ...                NaN   \n",
       "413                                          NaN  ...                NaN   \n",
       "\n",
       "      param_preprocessor__text__vectorizer__stop_words  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "407  [doing, shouldn, you'd, do, yourself, before, ...   \n",
       "408                                               None   \n",
       "410                                               None   \n",
       "411  [doing, shouldn, you'd, do, yourself, before, ...   \n",
       "413  [doing, shouldn, you'd, do, yourself, before, ...   \n",
       "\n",
       "       param_preprocessor__text__vectorizer__tokenizer  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "407  <__main__.StemmingTokenizer object at 0x2b87d8...   \n",
       "408                                               None   \n",
       "410  <__main__.StemmingTokenizer object at 0x2b87d8...   \n",
       "411                                               None   \n",
       "413  <__main__.StemmingTokenizer object at 0x2b87d8...   \n",
       "\n",
       "                                                params split0_test_score  \\\n",
       "0    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.519879   \n",
       "1    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.519879   \n",
       "2    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.511874   \n",
       "3    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.511883   \n",
       "4    {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...          0.514714   \n",
       "..                                                 ...               ...   \n",
       "407  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...          0.473155   \n",
       "408  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...          0.465701   \n",
       "410  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...          0.470812   \n",
       "411  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...          0.465159   \n",
       "413  {'model': LinearSVC(C=0.1), 'model__C': 10.0, ...          0.469366   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0            0.522301           0.521086         0.521088        0.000989   \n",
       "1            0.522328           0.521086         0.521097        0.001000   \n",
       "2            0.511528           0.511155         0.511519        0.000294   \n",
       "3            0.511528           0.511155         0.511522        0.000297   \n",
       "4            0.516063           0.514927         0.515234        0.000592   \n",
       "..                ...                ...              ...             ...   \n",
       "407          0.474371           0.472800         0.473442        0.000673   \n",
       "408          0.466073           0.465931         0.465902        0.000154   \n",
       "410          0.471194           0.472392         0.471466        0.000673   \n",
       "411          0.463917           0.464991         0.464689        0.000550   \n",
       "413          0.469774           0.470262         0.469801        0.000366   \n",
       "\n",
       "     rank_test_score  \n",
       "0                 14  \n",
       "1                 13  \n",
       "2                 59  \n",
       "3                 58  \n",
       "4                 30  \n",
       "..               ...  \n",
       "407              173  \n",
       "408              244  \n",
       "410              177  \n",
       "411              262  \n",
       "413              194  \n",
       "\n",
       "[264 rows x 22 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = cv_results[cv_results[\"mean_test_score\"].notna()].copy()\n",
    "cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_model</th>\n",
       "      <th>param_model__C</th>\n",
       "      <th>param_model__class_weight</th>\n",
       "      <th>param_preprocessor__date__scaler</th>\n",
       "      <th>param_preprocessor__ratio__scaler</th>\n",
       "      <th>param_preprocessor__text__vectorizer__max_df</th>\n",
       "      <th>param_preprocessor__text__vectorizer__max_features</th>\n",
       "      <th>param_preprocessor__text__vectorizer__strip_accents</th>\n",
       "      <th>param_model__alpha</th>\n",
       "      <th>param_preprocessor__text__vectorizer__stop_words</th>\n",
       "      <th>param_preprocessor__text__vectorizer__tokenizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.StemmingTokenizer object at 0x2b87d8...</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.521591</td>\n",
       "      <td>0.522346</td>\n",
       "      <td>0.520908</td>\n",
       "      <td>0.521615</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.StemmingTokenizer object at 0x2b87d8...</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.521511</td>\n",
       "      <td>0.522363</td>\n",
       "      <td>0.520802</td>\n",
       "      <td>0.521559</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.520367</td>\n",
       "      <td>0.522523</td>\n",
       "      <td>0.521440</td>\n",
       "      <td>0.521443</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.520367</td>\n",
       "      <td>0.522523</td>\n",
       "      <td>0.521440</td>\n",
       "      <td>0.521443</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>unicode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.520358</td>\n",
       "      <td>0.522514</td>\n",
       "      <td>0.521458</td>\n",
       "      <td>0.521443</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.520322</td>\n",
       "      <td>0.522559</td>\n",
       "      <td>0.521423</td>\n",
       "      <td>0.521435</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.520322</td>\n",
       "      <td>0.522559</td>\n",
       "      <td>0.521414</td>\n",
       "      <td>0.521432</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>unicode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.520313</td>\n",
       "      <td>0.522550</td>\n",
       "      <td>0.521423</td>\n",
       "      <td>0.521429</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>0.75</td>\n",
       "      <td>None</td>\n",
       "      <td>unicode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.519719</td>\n",
       "      <td>0.522843</td>\n",
       "      <td>0.521165</td>\n",
       "      <td>0.521242</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>LinearSVC(C=0.1)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>0.75</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'model': LinearSVC(C=0.1), 'model__C': 0.1, '...</td>\n",
       "      <td>0.519710</td>\n",
       "      <td>0.522843</td>\n",
       "      <td>0.521121</td>\n",
       "      <td>0.521225</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          param_model param_model__C param_model__class_weight  \\\n",
       "380  LinearSVC(C=0.1)            0.1                      None   \n",
       "326  LinearSVC(C=0.1)            0.1                      None   \n",
       "378  LinearSVC(C=0.1)            0.1                      None   \n",
       "174  LinearSVC(C=0.1)            0.1                      None   \n",
       "175  LinearSVC(C=0.1)            0.1                      None   \n",
       "324  LinearSVC(C=0.1)            0.1                      None   \n",
       "12   LinearSVC(C=0.1)            0.1                      None   \n",
       "13   LinearSVC(C=0.1)            0.1                      None   \n",
       "169  LinearSVC(C=0.1)            0.1                      None   \n",
       "168  LinearSVC(C=0.1)            0.1                      None   \n",
       "\n",
       "    param_preprocessor__date__scaler param_preprocessor__ratio__scaler  \\\n",
       "380                 StandardScaler()                  StandardScaler()   \n",
       "326                   MinMaxScaler()                    MinMaxScaler()   \n",
       "378                 StandardScaler()                  StandardScaler()   \n",
       "174                 StandardScaler()                  StandardScaler()   \n",
       "175                 StandardScaler()                  StandardScaler()   \n",
       "324                   MinMaxScaler()                    MinMaxScaler()   \n",
       "12                    MinMaxScaler()                    MinMaxScaler()   \n",
       "13                    MinMaxScaler()                    MinMaxScaler()   \n",
       "169                 StandardScaler()                  StandardScaler()   \n",
       "168                 StandardScaler()                  StandardScaler()   \n",
       "\n",
       "    param_preprocessor__text__vectorizer__max_df  \\\n",
       "380                                          NaN   \n",
       "326                                          NaN   \n",
       "378                                          NaN   \n",
       "174                                          1.0   \n",
       "175                                          1.0   \n",
       "324                                          NaN   \n",
       "12                                           1.0   \n",
       "13                                           1.0   \n",
       "169                                         0.75   \n",
       "168                                         0.75   \n",
       "\n",
       "    param_preprocessor__text__vectorizer__max_features  \\\n",
       "380                                                NaN   \n",
       "326                                                NaN   \n",
       "378                                                NaN   \n",
       "174                                               None   \n",
       "175                                               None   \n",
       "324                                                NaN   \n",
       "12                                                None   \n",
       "13                                                None   \n",
       "169                                               None   \n",
       "168                                               None   \n",
       "\n",
       "    param_preprocessor__text__vectorizer__strip_accents param_model__alpha  \\\n",
       "380                                                NaN                 NaN   \n",
       "326                                                NaN                 NaN   \n",
       "378                                                NaN                 NaN   \n",
       "174                                               None                 NaN   \n",
       "175                                            unicode                 NaN   \n",
       "324                                                NaN                 NaN   \n",
       "12                                                None                 NaN   \n",
       "13                                             unicode                 NaN   \n",
       "169                                            unicode                 NaN   \n",
       "168                                               None                 NaN   \n",
       "\n",
       "    param_preprocessor__text__vectorizer__stop_words  \\\n",
       "380                                             None   \n",
       "326                                             None   \n",
       "378                                             None   \n",
       "174                                              NaN   \n",
       "175                                              NaN   \n",
       "324                                             None   \n",
       "12                                               NaN   \n",
       "13                                               NaN   \n",
       "169                                              NaN   \n",
       "168                                              NaN   \n",
       "\n",
       "       param_preprocessor__text__vectorizer__tokenizer  \\\n",
       "380  <__main__.StemmingTokenizer object at 0x2b87d8...   \n",
       "326  <__main__.StemmingTokenizer object at 0x2b87d8...   \n",
       "378                                               None   \n",
       "174                                                NaN   \n",
       "175                                                NaN   \n",
       "324                                               None   \n",
       "12                                                 NaN   \n",
       "13                                                 NaN   \n",
       "169                                                NaN   \n",
       "168                                                NaN   \n",
       "\n",
       "                                                params  split0_test_score  \\\n",
       "380  {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.521591   \n",
       "326  {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.521511   \n",
       "378  {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.520367   \n",
       "174  {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.520367   \n",
       "175  {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.520358   \n",
       "324  {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.520322   \n",
       "12   {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.520322   \n",
       "13   {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.520313   \n",
       "169  {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.519719   \n",
       "168  {'model': LinearSVC(C=0.1), 'model__C': 0.1, '...           0.519710   \n",
       "\n",
       "     split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "380           0.522346           0.520908         0.521615        0.000587   \n",
       "326           0.522363           0.520802         0.521559        0.000639   \n",
       "378           0.522523           0.521440         0.521443        0.000880   \n",
       "174           0.522523           0.521440         0.521443        0.000880   \n",
       "175           0.522514           0.521458         0.521443        0.000880   \n",
       "324           0.522559           0.521423         0.521435        0.000913   \n",
       "12            0.522559           0.521414         0.521432        0.000913   \n",
       "13            0.522550           0.521423         0.521429        0.000913   \n",
       "169           0.522843           0.521165         0.521242        0.001276   \n",
       "168           0.522843           0.521121         0.521225        0.001281   \n",
       "\n",
       "     rank_test_score  \n",
       "380                1  \n",
       "326                2  \n",
       "378                3  \n",
       "174                3  \n",
       "175                5  \n",
       "324                6  \n",
       "12                 7  \n",
       "13                 8  \n",
       "169                9  \n",
       "168               10  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the 10 best models\n",
    "cv_results.sort_values(\"rank_test_score\").head(10).drop([\"mean_fit_time\", \"std_fit_time\", \"mean_score_time\", \"std_score_time\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- The best model is the LinearSVC model with the TF-IDF Vectorizer and the following parameters:\n",
    "    - C = 0.1\n",
    "    - No class weights\n",
    "    - MinMaxScaler vs. StandardScaler does not matter\n",
    "    - max_df should be 1.0\n",
    "    - stopword removal does not help much\n",
    "    - strip_accents does matter much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about Kernels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Size of train set: 304246'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Size of validation set: 33806'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;kernel&#x27;,\n",
       "                 RBFSampler(gamma=0.1, n_components=1000, random_state=42)),\n",
       "                (&#x27;model&#x27;, LinearSVC(C=0.01))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-79\" type=\"checkbox\" ><label for=\"sk-estimator-id-79\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;kernel&#x27;,\n",
       "                 RBFSampler(gamma=0.1, n_components=1000, random_state=42)),\n",
       "                (&#x27;model&#x27;, LinearSVC(C=0.01))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-80\" type=\"checkbox\" ><label for=\"sk-estimator-id-80\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(n_jobs=-1,\n",
       "                  transformers=[(&#x27;ratio&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                  &#x27;author_count&#x27;]),\n",
       "                                (&#x27;date&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                (&#x27;text&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                  TfidfVectorizer())]),\n",
       "                                 &#x27;text&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-81\" type=\"checkbox\" ><label for=\"sk-estimator-id-81\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ratio</label><div class=\"sk-toggleable__content\"><pre>[&#x27;page_count&#x27;, &#x27;figure_count&#x27;, &#x27;author_count&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-82\" type=\"checkbox\" ><label for=\"sk-estimator-id-82\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-83\" type=\"checkbox\" ><label for=\"sk-estimator-id-83\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">date</label><div class=\"sk-toggleable__content\"><pre>[&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-84\" type=\"checkbox\" ><label for=\"sk-estimator-id-84\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-85\" type=\"checkbox\" ><label for=\"sk-estimator-id-85\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">text</label><div class=\"sk-toggleable__content\"><pre>text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-86\" type=\"checkbox\" ><label for=\"sk-estimator-id-86\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-87\" type=\"checkbox\" ><label for=\"sk-estimator-id-87\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RBFSampler</label><div class=\"sk-toggleable__content\"><pre>RBFSampler(gamma=0.1, n_components=1000, random_state=42)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-88\" type=\"checkbox\" ><label for=\"sk-estimator-id-88\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=0.01)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[('ratio',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['page_count', 'figure_count',\n",
       "                                                   'author_count']),\n",
       "                                                 ('date',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['year', 'month', 'day']),\n",
       "                                                 ('text',\n",
       "                                                  Pipeline(steps=[('vectorizer',\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  'text')])),\n",
       "                ('kernel',\n",
       "                 RBFSampler(gamma=0.1, n_components=1000, random_state=42)),\n",
       "                ('model', LinearSVC(C=0.01))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_pipeline = deepcopy(pipeline)\n",
    "kernel_pipeline.steps.insert(1, (\"kernel\", RBFSampler(gamma=0.1, random_state=42, n_components=1000)))\n",
    "kernel_pipeline.steps[-1] = (\"model\", LinearSVC(C=0.01))\n",
    "\n",
    "train_df_trunc, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"citation_bucket\"],\n",
    ")\n",
    "display(f\"Size of train set: {len(train_df_trunc)}\")\n",
    "display(f\"Size of validation set: {len(val_df)}\")\n",
    "kernel_pipeline.fit(train_df_trunc, train_df_trunc[\"citation_bucket\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train score: 0.5014396245143732'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Validation score: 0.49952671123469206'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f\"Train score: {kernel_pipeline.score(train_df_trunc, train_df_trunc['citation_bucket'])}\")\n",
    "display(f\"Validation score: {kernel_pipeline.score(val_df, val_df['citation_bucket'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Grid Search!\n",
    "\n",
    "We observe relative poor performance in the previous grid search.\n",
    "Since we only tried using a Linear model, we will try using a non-linear model (SVM with RBF kernel) to see if we can get better results.\n",
    "Regarding the preprocessing, we realize that the preprocessing operations have limited influence. \n",
    "Additionally, the last GridSearch took over 14 hours to run. We will try to reduce the number of parameters to search over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# First we construct a new pipeline and apply what we've learned\n",
    "\n",
    "data_prep_pipeline = pipeline.named_steps[\"preprocessor\"]\n",
    "# Transform the training data to save some resources!\n",
    "train_df_transformed = data_prep_pipeline.fit_transform(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df_transformed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit und Beruf/Uppsala/Data Mining/uu-data-mining-project/exploration_modeling.ipynb Cell 34\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m kernel_pipeline \u001b[39m=\u001b[39m Pipeline([\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mkernel\u001b[39m\u001b[39m\"\u001b[39m, RBFSampler(gamma\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, n_components\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, LinearSVC())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m ])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m display(train_df_transformed\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m display(kernel_pipeline)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m param_distributions \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mkernel__gamma\u001b[39m\u001b[39m\"\u001b[39m: uniform(\u001b[39m0.01\u001b[39m, \u001b[39m10\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mkernel__n_components\u001b[39m\u001b[39m\"\u001b[39m: randint(\u001b[39m100\u001b[39m, \u001b[39m1000\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmodel__C\u001b[39m\u001b[39m\"\u001b[39m: uniform(\u001b[39m0.01\u001b[39m, \u001b[39m10\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X45sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df_transformed' is not defined"
     ]
    }
   ],
   "source": [
    "kernel_pipeline = Pipeline([\n",
    "    (\"kernel\", RBFSampler(gamma=10, n_components=1000)),\n",
    "    (\"model\", LinearSVC())\n",
    "])\n",
    "\n",
    "display(train_df_transformed.shape)\n",
    "display(kernel_pipeline)\n",
    "\n",
    "param_distributions = {\n",
    "    \"kernel__gamma\": uniform(0.01, 10),\n",
    "    \"kernel__n_components\": randint(100, 1000),\n",
    "    \"model__C\": uniform(0.01, 10)\n",
    "}\n",
    "\n",
    "param_grid_kernel = {\n",
    "    \"kernel__gamma\": [0.01, 0.1, 1.0, 10.0],\n",
    "    \"kernel__n_components\": [100, 250, 1000],\n",
    "    \"model__C\": [0.01, 0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "# kernel_search = HalvingRandomSearchCV(kernel_pipeline, param_distributions=param_distributions, cv=3, verbose=10, n_jobs=2)\n",
    "kernel_search = HalvingGridSearchCV(kernel_pipeline, param_grid=param_grid_kernel, cv=3, verbose=10, n_jobs=6)\n",
    "kernel_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 4\n",
      "n_required_iterations: 4\n",
      "n_possible_iterations: 4\n",
      "min_resources_: 12520\n",
      "max_resources_: 338052\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 48\n",
      "n_resources: 12520\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "[CV 1/3; 1/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=0.01\n",
      "[CV 2/3; 1/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=0.01\n",
      "[CV 3/3; 1/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=0.01\n",
      "[CV 1/3; 2/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=0.1.\n",
      "[CV 2/3; 2/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=0.1.\n",
      "[CV 3/3; 2/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=0.1.\n",
      "[CV 1/3; 1/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=0.01;, score=(train=0.418, test=0.407) total time=   0.9s\n",
      "[CV 1/3; 3/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=1.0.\n",
      "[CV 2/3; 1/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=0.01;, score=(train=0.410, test=0.409) total time=   0.9s\n",
      "[CV 3/3; 1/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=0.01;, score=(train=0.411, test=0.393) total time=   0.9s\n",
      "[CV 2/3; 3/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=1.0.\n",
      "[CV 3/3; 3/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=1.0.\n",
      "[CV 1/3; 2/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=0.1;, score=(train=0.461, test=0.448) total time=   1.0s\n",
      "[CV 2/3; 2/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=0.1;, score=(train=0.452, test=0.445) total time=   1.0s\n",
      "[CV 1/3; 4/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=10.0\n",
      "[CV 3/3; 2/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=0.1;, score=(train=0.463, test=0.439) total time=   1.0s\n",
      "[CV 2/3; 4/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=10.0\n",
      "[CV 3/3; 4/48] START kernel__gamma=0.01, kernel__n_components=100, model__C=10.0\n",
      "[CV 1/3; 3/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=1.0;, score=(train=0.470, test=0.459) total time=   1.5s\n",
      "[CV 1/3; 5/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=0.01\n",
      "[CV 2/3; 3/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=1.0;, score=(train=0.479, test=0.454) total time=   1.5s\n",
      "[CV 2/3; 5/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=0.01\n",
      "[CV 3/3; 3/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=1.0;, score=(train=0.488, test=0.439) total time=   1.6s\n",
      "[CV 3/3; 5/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=0.01\n",
      "[CV 1/3; 5/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=0.01;, score=(train=0.418, test=0.407) total time=   2.5s\n",
      "[CV 1/3; 6/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=0.1.\n",
      "[CV 2/3; 5/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=0.01;, score=(train=0.410, test=0.409) total time=   2.5s\n",
      "[CV 2/3; 6/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=0.1.\n",
      "[CV 3/3; 5/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=0.01;, score=(train=0.411, test=0.393) total time=   2.5s\n",
      "[CV 3/3; 6/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=0.1.\n",
      "[CV 1/3; 6/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=0.1;, score=(train=0.472, test=0.453) total time=   2.4s\n",
      "[CV 1/3; 7/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=1.0.\n",
      "[CV 2/3; 6/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=0.1;, score=(train=0.472, test=0.464) total time=   2.4s\n",
      "[CV 2/3; 7/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=1.0.\n",
      "[CV 3/3; 6/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=0.1;, score=(train=0.489, test=0.454) total time=   2.5s\n",
      "[CV 3/3; 7/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=1.0.\n",
      "[CV 2/3; 4/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=10.0;, score=(train=0.483, test=0.450) total time=   7.1s\n",
      "[CV 1/3; 8/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=10.0\n",
      "[CV 1/3; 4/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=10.0;, score=(train=0.487, test=0.449) total time=   7.5s\n",
      "[CV 2/3; 8/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=10.0\n",
      "[CV 3/3; 4/48] END kernel__gamma=0.01, kernel__n_components=100, model__C=10.0;, score=(train=0.486, test=0.447) total time=   7.4s\n",
      "[CV 3/3; 8/48] START kernel__gamma=0.01, kernel__n_components=250, model__C=10.0\n",
      "[CV 1/3; 7/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=1.0;, score=(train=0.505, test=0.461) total time=   3.6s\n",
      "[CV 1/3; 9/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.01\n",
      "[CV 2/3; 7/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=1.0;, score=(train=0.508, test=0.468) total time=   4.0s\n",
      "[CV 2/3; 9/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.01\n",
      "[CV 3/3; 7/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=1.0;, score=(train=0.509, test=0.456) total time=   4.3s\n",
      "[CV 3/3; 9/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.01\n",
      "[CV 1/3; 8/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=10.0;, score=(train=0.515, test=0.450) total time=  19.8s\n",
      "[CV 2/3; 8/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=10.0;, score=(train=0.511, test=0.447) total time=  19.6s\n",
      "[CV 3/3; 8/48] END kernel__gamma=0.01, kernel__n_components=250, model__C=10.0;, score=(train=0.509, test=0.437) total time=  19.5s\n",
      "[CV 1/3; 10/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1\n",
      "[CV 2/3; 10/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1\n",
      "[CV 3/3; 10/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1\n",
      "[CV 1/3; 9/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.01;, score=(train=0.418, test=0.407) total time= 1.1min\n",
      "[CV 1/3; 11/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0\n",
      "[CV 2/3; 9/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.01;, score=(train=0.410, test=0.409) total time= 1.1min\n",
      "[CV 2/3; 11/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0\n",
      "[CV 3/3; 9/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.01;, score=(train=0.411, test=0.393) total time= 1.1min\n",
      "[CV 3/3; 11/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0\n",
      "[CV 1/3; 10/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1;, score=(train=0.475, test=0.453) total time= 1.1min\n",
      "[CV 1/3; 12/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0\n",
      "[CV 2/3; 10/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1;, score=(train=0.492, test=0.474) total time= 1.1min\n",
      "[CV 3/3; 10/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1;, score=(train=0.492, test=0.457) total time= 1.1min\n",
      "[CV 2/3; 12/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0\n",
      "[CV 3/3; 12/48] START kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0\n",
      "[CV 1/3; 11/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0;, score=(train=0.548, test=0.473) total time= 1.2min\n",
      "[CV 2/3; 11/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0;, score=(train=0.545, test=0.483) total time= 1.2min\n",
      "[CV 1/3; 13/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=0.01\n",
      "[CV 2/3; 13/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=0.01\n",
      "[CV 3/3; 11/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0;, score=(train=0.550, test=0.468) total time= 1.2min\n",
      "[CV 3/3; 13/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=0.01\n",
      "[CV 1/3; 13/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=0.01;, score=(train=0.456, test=0.438) total time=   1.0s\n",
      "[CV 1/3; 14/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=0.1.\n",
      "[CV 2/3; 13/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=0.01;, score=(train=0.454, test=0.444) total time=   1.0s\n",
      "[CV 2/3; 14/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=0.1.\n",
      "[CV 3/3; 13/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=0.01;, score=(train=0.457, test=0.432) total time=   1.0s\n",
      "[CV 3/3; 14/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=0.1.\n",
      "[CV 1/3; 14/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=0.1;, score=(train=0.483, test=0.454) total time=   1.0s\n",
      "[CV 1/3; 15/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=1.0.\n",
      "[CV 2/3; 14/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=0.1;, score=(train=0.479, test=0.457) total time=   1.0s\n",
      "[CV 2/3; 15/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=1.0.\n",
      "[CV 3/3; 14/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=0.1;, score=(train=0.475, test=0.434) total time=   1.0s\n",
      "[CV 3/3; 15/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=1.0.\n",
      "[CV 1/3; 15/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=1.0;, score=(train=0.466, test=0.437) total time=   1.6s\n",
      "[CV 1/3; 16/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=10.0\n",
      "[CV 2/3; 15/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=1.0;, score=(train=0.472, test=0.453) total time=   1.7s\n",
      "[CV 2/3; 16/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=10.0\n",
      "[CV 3/3; 15/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=1.0;, score=(train=0.482, test=0.436) total time=   1.7s\n",
      "[CV 3/3; 16/48] START kernel__gamma=0.1, kernel__n_components=100, model__C=10.0\n",
      "[CV 3/3; 16/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=10.0;, score=(train=0.490, test=0.449) total time=   8.3s\n",
      "[CV 1/3; 17/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.01\n",
      "[CV 1/3; 16/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=10.0;, score=(train=0.477, test=0.454) total time=   9.6s\n",
      "[CV 2/3; 17/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.01\n",
      "[CV 2/3; 16/48] END kernel__gamma=0.1, kernel__n_components=100, model__C=10.0;, score=(train=0.475, test=0.451) total time=   9.5s\n",
      "[CV 3/3; 17/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.01\n",
      "[CV 1/3; 17/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.01;, score=(train=0.469, test=0.451) total time=   2.5s\n",
      "[CV 1/3; 18/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.1.\n",
      "[CV 2/3; 17/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.01;, score=(train=0.476, test=0.466) total time=   2.5s\n",
      "[CV 2/3; 18/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.1.\n",
      "[CV 3/3; 17/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.01;, score=(train=0.475, test=0.446) total time=   2.5s\n",
      "[CV 3/3; 18/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.1.\n",
      "[CV 1/3; 18/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.1;, score=(train=0.512, test=0.466) total time=   2.4s\n",
      "[CV 1/3; 19/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=1.0.\n",
      "[CV 2/3; 18/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.1;, score=(train=0.502, test=0.474) total time=   2.4s\n",
      "[CV 2/3; 19/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=1.0.\n",
      "[CV 3/3; 18/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.1;, score=(train=0.510, test=0.454) total time=   2.5s\n",
      "[CV 3/3; 19/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=1.0.\n",
      "[CV 1/3; 19/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=1.0;, score=(train=0.513, test=0.455) total time=   3.3s\n",
      "[CV 1/3; 20/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=10.0\n",
      "[CV 2/3; 19/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=1.0;, score=(train=0.510, test=0.457) total time=   3.4s\n",
      "[CV 2/3; 20/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=10.0\n",
      "[CV 3/3; 19/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=1.0;, score=(train=0.512, test=0.443) total time=   3.4s\n",
      "[CV 3/3; 20/48] START kernel__gamma=0.1, kernel__n_components=250, model__C=10.0\n",
      "[CV 3/3; 12/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0;, score=(train=0.574, test=0.449) total time= 1.5min\n",
      "[CV 2/3; 12/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0;, score=(train=0.577, test=0.449) total time= 1.5min\n",
      "[CV 1/3; 21/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01\n",
      "[CV 2/3; 21/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01\n",
      "[CV 1/3; 12/48] END kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0;, score=(train=0.577, test=0.451) total time= 1.5min\n",
      "[CV 3/3; 21/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01\n",
      "[CV 1/3; 20/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=10.0;, score=(train=0.507, test=0.453) total time=  20.9s\n",
      "[CV 1/3; 22/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1\n",
      "[CV 3/3; 20/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=10.0;, score=(train=0.513, test=0.446) total time=  20.5s\n",
      "[CV 2/3; 22/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1\n",
      "[CV 2/3; 20/48] END kernel__gamma=0.1, kernel__n_components=250, model__C=10.0;, score=(train=0.505, test=0.448) total time=  21.6s\n",
      "[CV 3/3; 22/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1\n",
      "[CV 1/3; 21/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01;, score=(train=0.474, test=0.451) total time=  12.7s\n",
      "[CV 1/3; 23/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0\n",
      "[CV 2/3; 21/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01;, score=(train=0.479, test=0.474) total time=  44.5s\n",
      "[CV 2/3; 23/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0\n",
      "[CV 3/3; 21/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01;, score=(train=0.496, test=0.460) total time=  53.4s\n",
      "[CV 3/3; 23/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0\n",
      "[CV 1/3; 22/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1;, score=(train=0.547, test=0.475) total time= 1.1min\n",
      "[CV 1/3; 24/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=10.0\n",
      "[CV 2/3; 22/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1;, score=(train=0.539, test=0.485) total time= 1.1min\n",
      "[CV 3/3; 22/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1;, score=(train=0.547, test=0.472) total time= 1.2min\n",
      "[CV 2/3; 24/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=10.0\n",
      "[CV 3/3; 24/48] START kernel__gamma=0.1, kernel__n_components=1000, model__C=10.0\n",
      "[CV 1/3; 23/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0;, score=(train=0.592, test=0.452) total time= 1.3min\n",
      "[CV 1/3; 25/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=0.01\n",
      "[CV 1/3; 25/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=0.01;, score=(train=0.446, test=0.441) total time=   2.9s\n",
      "[CV 2/3; 25/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=0.01\n",
      "[CV 2/3; 25/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=0.01;, score=(train=0.441, test=0.421) total time=   1.6s\n",
      "[CV 3/3; 25/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=0.01\n",
      "[CV 3/3; 25/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=0.01;, score=(train=0.455, test=0.426) total time=   1.0s\n",
      "[CV 1/3; 26/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=0.1.\n",
      "[CV 1/3; 26/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=0.1;, score=(train=0.454, test=0.433) total time=   1.0s\n",
      "[CV 2/3; 26/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=0.1.\n",
      "[CV 2/3; 26/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=0.1;, score=(train=0.444, test=0.416) total time=   1.2s\n",
      "[CV 3/3; 26/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=0.1.\n",
      "[CV 3/3; 26/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=0.1;, score=(train=0.455, test=0.413) total time=   1.0s\n",
      "[CV 1/3; 27/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=1.0.\n",
      "[CV 1/3; 27/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=1.0;, score=(train=0.452, test=0.423) total time=   1.7s\n",
      "[CV 2/3; 27/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=1.0.\n",
      "[CV 2/3; 27/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=1.0;, score=(train=0.447, test=0.412) total time=   1.5s\n",
      "[CV 3/3; 27/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=1.0.\n",
      "[CV 3/3; 27/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=1.0;, score=(train=0.453, test=0.411) total time=   1.6s\n",
      "[CV 1/3; 28/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=10.0\n",
      "[CV 1/3; 28/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=10.0;, score=(train=0.449, test=0.412) total time=   8.8s\n",
      "[CV 2/3; 28/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=10.0\n",
      "[CV 2/3; 23/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0;, score=(train=0.581, test=0.463) total time= 1.1min\n",
      "[CV 3/3; 28/48] START kernel__gamma=1.0, kernel__n_components=100, model__C=10.0\n",
      "[CV 3/3; 23/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0;, score=(train=0.584, test=0.453) total time= 1.1min\n",
      "[CV 1/3; 29/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=0.01\n",
      "[CV 2/3; 28/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=10.0;, score=(train=0.446, test=0.416) total time=  10.8s\n",
      "[CV 2/3; 29/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=0.01\n",
      "[CV 3/3; 28/48] END kernel__gamma=1.0, kernel__n_components=100, model__C=10.0;, score=(train=0.457, test=0.393) total time=  10.7s\n",
      "[CV 3/3; 29/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=0.01\n",
      "[CV 2/3; 29/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=0.01;, score=(train=0.465, test=0.444) total time=   3.9s\n",
      "[CV 1/3; 30/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=0.1.\n",
      "[CV 1/3; 29/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=0.01;, score=(train=0.474, test=0.439) total time=   4.6s\n",
      "[CV 2/3; 30/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=0.1.\n",
      "[CV 3/3; 29/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=0.01;, score=(train=0.480, test=0.447) total time=   2.9s\n",
      "[CV 3/3; 30/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=0.1.\n",
      "[CV 1/3; 30/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=0.1;, score=(train=0.497, test=0.431) total time=   2.9s\n",
      "[CV 2/3; 30/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=0.1;, score=(train=0.489, test=0.433) total time=   2.8s\n",
      "[CV 1/3; 31/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=1.0.\n",
      "[CV 2/3; 31/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=1.0.\n",
      "[CV 3/3; 30/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=0.1;, score=(train=0.490, test=0.419) total time=   2.6s\n",
      "[CV 3/3; 31/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=1.0.\n",
      "[CV 1/3; 31/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=1.0;, score=(train=0.499, test=0.421) total time=   3.9s\n",
      "[CV 2/3; 31/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=1.0;, score=(train=0.488, test=0.427) total time=   3.9s\n",
      "[CV 1/3; 32/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=10.0\n",
      "[CV 2/3; 32/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=10.0\n",
      "[CV 3/3; 31/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=1.0;, score=(train=0.490, test=0.417) total time=   3.7s\n",
      "[CV 3/3; 32/48] START kernel__gamma=1.0, kernel__n_components=250, model__C=10.0\n",
      "[CV 1/3; 32/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=10.0;, score=(train=0.492, test=0.428) total time=  17.9s\n",
      "[CV 2/3; 32/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=10.0;, score=(train=0.484, test=0.429) total time=  17.9s\n",
      "[CV 1/3; 33/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01\n",
      "[CV 2/3; 33/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01\n",
      "[CV 3/3; 32/48] END kernel__gamma=1.0, kernel__n_components=250, model__C=10.0;, score=(train=0.491, test=0.427) total time=  17.0s\n",
      "[CV 3/3; 33/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01\n",
      "[CV 1/3; 24/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=10.0;, score=(train=0.611, test=0.453) total time= 1.2min\n",
      "[CV 1/3; 34/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=0.1\n",
      "[CV 2/3; 24/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=10.0;, score=(train=0.614, test=0.436) total time= 1.5min\n",
      "[CV 3/3; 24/48] END kernel__gamma=0.1, kernel__n_components=1000, model__C=10.0;, score=(train=0.607, test=0.435) total time= 1.5min\n",
      "[CV 2/3; 34/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=0.1\n",
      "[CV 3/3; 34/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=0.1\n",
      "[CV 2/3; 34/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=0.1;, score=(train=0.566, test=0.455) total time=  20.2s\n",
      "[CV 1/3; 35/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=1.0\n",
      "[CV 1/3; 33/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01;, score=(train=0.500, test=0.451) total time= 1.3min\n",
      "[CV 2/3; 35/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=1.0\n",
      "[CV 2/3; 33/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01;, score=(train=0.509, test=0.470) total time= 1.4min\n",
      "[CV 3/3; 35/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=1.0\n",
      "[CV 3/3; 33/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01;, score=(train=0.513, test=0.448) total time= 1.4min\n",
      "[CV 1/3; 34/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=0.1;, score=(train=0.568, test=0.458) total time= 1.2min\n",
      "[CV 1/3; 36/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=10.0\n",
      "[CV 2/3; 36/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=10.0\n",
      "[CV 3/3; 34/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=0.1;, score=(train=0.578, test=0.432) total time=  58.9s\n",
      "[CV 3/3; 36/48] START kernel__gamma=1.0, kernel__n_components=1000, model__C=10.0\n",
      "[CV 1/3; 35/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=1.0;, score=(train=0.597, test=0.423) total time=  56.8s\n",
      "[CV 1/3; 37/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=0.01\n",
      "[CV 1/3; 37/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=0.01;, score=(train=0.420, test=0.407) total time=   0.9s\n",
      "[CV 2/3; 37/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=0.01\n",
      "[CV 2/3; 37/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=0.01;, score=(train=0.410, test=0.409) total time=   0.9s\n",
      "[CV 3/3; 37/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=0.01\n",
      "[CV 3/3; 37/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=0.01;, score=(train=0.413, test=0.395) total time=   0.9s\n",
      "[CV 1/3; 38/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=0.1\n",
      "[CV 1/3; 38/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=0.1;, score=(train=0.427, test=0.395) total time=   0.9s\n",
      "[CV 2/3; 38/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=0.1\n",
      "[CV 2/3; 38/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=0.1;, score=(train=0.412, test=0.403) total time=   1.1s\n",
      "[CV 3/3; 38/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=0.1\n",
      "[CV 3/3; 38/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=0.1;, score=(train=0.413, test=0.386) total time=   1.0s\n",
      "[CV 1/3; 39/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=1.0\n",
      "[CV 1/3; 39/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=1.0;, score=(train=0.420, test=0.394) total time=   1.5s\n",
      "[CV 2/3; 39/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=1.0\n",
      "[CV 2/3; 39/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=1.0;, score=(train=0.411, test=0.398) total time=   1.4s\n",
      "[CV 3/3; 39/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=1.0\n",
      "[CV 3/3; 39/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=1.0;, score=(train=0.414, test=0.378) total time=   1.4s\n",
      "[CV 1/3; 40/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=10.0\n",
      "[CV 1/3; 40/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=10.0;, score=(train=0.424, test=0.399) total time=   6.9s\n",
      "[CV 2/3; 40/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=10.0\n",
      "[CV 2/3; 40/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=10.0;, score=(train=0.413, test=0.394) total time=   8.6s\n",
      "[CV 3/3; 40/48] START kernel__gamma=10.0, kernel__n_components=100, model__C=10.0\n",
      "[CV 2/3; 35/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=1.0;, score=(train=0.591, test=0.422) total time=  58.8s\n",
      "[CV 1/3; 41/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=0.01\n",
      "[CV 3/3; 35/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=1.0;, score=(train=0.595, test=0.417) total time=  59.8s\n",
      "[CV 2/3; 41/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=0.01\n",
      "[CV 1/3; 41/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=0.01;, score=(train=0.420, test=0.405) total time=   2.2s\n",
      "[CV 3/3; 41/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=0.01\n",
      "[CV 2/3; 41/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=0.01;, score=(train=0.415, test=0.406) total time=   2.3s\n",
      "[CV 1/3; 42/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=0.1\n",
      "[CV 3/3; 41/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=0.01;, score=(train=0.413, test=0.392) total time=   2.3s\n",
      "[CV 2/3; 42/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=0.1\n",
      "[CV 3/3; 40/48] END kernel__gamma=10.0, kernel__n_components=100, model__C=10.0;, score=(train=0.420, test=0.380) total time=   8.9s\n",
      "[CV 3/3; 42/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=0.1\n",
      "[CV 1/3; 42/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=0.1;, score=(train=0.439, test=0.392) total time=   2.4s\n",
      "[CV 1/3; 43/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=1.0\n",
      "[CV 2/3; 42/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=0.1;, score=(train=0.428, test=0.378) total time=   2.4s\n",
      "[CV 2/3; 43/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=1.0\n",
      "[CV 3/3; 42/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=0.1;, score=(train=0.433, test=0.381) total time=   2.6s\n",
      "[CV 3/3; 43/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=1.0\n",
      "[CV 1/3; 43/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=1.0;, score=(train=0.436, test=0.377) total time=   3.6s\n",
      "[CV 1/3; 44/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=10.0\n",
      "[CV 2/3; 43/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=1.0;, score=(train=0.432, test=0.376) total time=   3.5s\n",
      "[CV 2/3; 44/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=10.0\n",
      "[CV 3/3; 43/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=1.0;, score=(train=0.433, test=0.360) total time=   3.5s\n",
      "[CV 3/3; 44/48] START kernel__gamma=10.0, kernel__n_components=250, model__C=10.0\n",
      "[CV 1/3; 36/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=10.0;, score=(train=0.590, test=0.421) total time= 1.4min\n",
      "[CV 1/3; 45/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=0.01\n",
      "[CV 3/3; 36/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=10.0;, score=(train=0.603, test=0.423) total time= 1.4min\n",
      "[CV 2/3; 36/48] END kernel__gamma=1.0, kernel__n_components=1000, model__C=10.0;, score=(train=0.601, test=0.414) total time= 1.4min\n",
      "[CV 2/3; 45/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=0.01\n",
      "[CV 3/3; 45/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=0.01\n",
      "[CV 1/3; 44/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=10.0;, score=(train=0.442, test=0.378) total time=  16.2s\n",
      "[CV 1/3; 46/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=0.1\n",
      "[CV 2/3; 44/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=10.0;, score=(train=0.428, test=0.370) total time=  16.5s\n",
      "[CV 2/3; 46/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=0.1\n",
      "[CV 3/3; 44/48] END kernel__gamma=10.0, kernel__n_components=250, model__C=10.0;, score=(train=0.438, test=0.366) total time=  17.9s\n",
      "[CV 3/3; 46/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=0.1\n",
      "[CV 1/3; 45/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=0.01;, score=(train=0.419, test=0.407) total time=  19.7s\n",
      "[CV 1/3; 47/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=1.0\n",
      "[CV 1/3; 47/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=1.0;, score=(train=0.529, test=0.350) total time=  13.6s\n",
      "[CV 2/3; 47/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=1.0\n",
      "[CV 2/3; 45/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=0.01;, score=(train=0.410, test=0.408) total time= 1.1min\n",
      "[CV 3/3; 45/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=0.01;, score=(train=0.411, test=0.393) total time= 1.1min\n",
      "[CV 3/3; 47/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=1.0\n",
      "[CV 1/3; 48/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=10.0\n",
      "[CV 1/3; 46/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=0.1;, score=(train=0.508, test=0.372) total time= 1.1min\n",
      "[CV 2/3; 48/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=10.0\n",
      "[CV 2/3; 46/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=0.1;, score=(train=0.506, test=0.356) total time= 1.2min\n",
      "[CV 3/3; 48/48] START kernel__gamma=10.0, kernel__n_components=1000, model__C=10.0\n",
      "[CV 3/3; 46/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=0.1;, score=(train=0.511, test=0.352) total time= 1.3min\n",
      "[CV 2/3; 47/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=1.0;, score=(train=0.517, test=0.339) total time= 1.2min\n",
      "[CV 3/3; 47/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=1.0;, score=(train=0.526, test=0.345) total time=  45.8s\n",
      "[CV 2/3; 48/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=10.0;, score=(train=0.522, test=0.345) total time= 1.2min\n",
      "[CV 3/3; 48/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=10.0;, score=(train=0.532, test=0.351) total time= 1.2min\n",
      "[CV 1/3; 48/48] END kernel__gamma=10.0, kernel__n_components=1000, model__C=10.0;, score=(train=0.521, test=0.345) total time= 1.3min\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 16\n",
      "n_resources: 37560\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[CV 2/3; 1/16] START kernel__gamma=0.01, kernel__n_components=100, model__C=10.0\n",
      "[CV 1/3; 1/16] START kernel__gamma=0.01, kernel__n_components=100, model__C=10.0\n",
      "[CV 3/3; 1/16] START kernel__gamma=0.01, kernel__n_components=100, model__C=10.0\n",
      "[CV 1/3; 2/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=10.0.\n",
      "[CV 2/3; 2/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=10.0.\n",
      "[CV 3/3; 2/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=10.0.\n",
      "[CV 3/3; 1/16] END kernel__gamma=0.01, kernel__n_components=100, model__C=10.0;, score=(train=0.471, test=0.465) total time=  27.4s\n",
      "[CV 1/3; 3/16] START kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0\n",
      "[CV 1/3; 1/16] END kernel__gamma=0.01, kernel__n_components=100, model__C=10.0;, score=(train=0.474, test=0.465) total time=  32.2s\n",
      "[CV 2/3; 3/16] START kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0\n",
      "[CV 2/3; 1/16] END kernel__gamma=0.01, kernel__n_components=100, model__C=10.0;, score=(train=0.465, test=0.455) total time=  33.1s\n",
      "[CV 3/3; 3/16] START kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0\n",
      "[CV 2/3; 2/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=10.0;, score=(train=0.487, test=0.464) total time=  53.9s\n",
      "[CV 1/3; 4/16] START kernel__gamma=0.01, kernel__n_components=100, model__C=1.0.\n",
      "[CV 1/3; 4/16] END kernel__gamma=0.01, kernel__n_components=100, model__C=1.0;, score=(train=0.469, test=0.467) total time=   4.4s\n",
      "[CV 2/3; 4/16] START kernel__gamma=0.01, kernel__n_components=100, model__C=1.0.\n",
      "[CV 3/3; 2/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=10.0;, score=(train=0.488, test=0.465) total time= 1.0min\n",
      "[CV 3/3; 4/16] START kernel__gamma=0.01, kernel__n_components=100, model__C=1.0.\n",
      "[CV 1/3; 2/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=10.0;, score=(train=0.486, test=0.469) total time= 1.0min\n",
      "[CV 1/3; 5/16] START kernel__gamma=0.1, kernel__n_components=100, model__C=10.0.\n",
      "[CV 2/3; 4/16] END kernel__gamma=0.01, kernel__n_components=100, model__C=1.0;, score=(train=0.465, test=0.449) total time=   5.0s\n",
      "[CV 2/3; 5/16] START kernel__gamma=0.1, kernel__n_components=100, model__C=10.0.\n",
      "[CV 3/3; 4/16] END kernel__gamma=0.01, kernel__n_components=100, model__C=1.0;, score=(train=0.469, test=0.459) total time=   5.0s\n",
      "[CV 3/3; 5/16] START kernel__gamma=0.1, kernel__n_components=100, model__C=10.0.\n",
      "[CV 1/3; 5/16] END kernel__gamma=0.1, kernel__n_components=100, model__C=10.0;, score=(train=0.475, test=0.462) total time=  34.2s\n",
      "[CV 1/3; 6/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=1.0..\n",
      "[CV 3/3; 5/16] END kernel__gamma=0.1, kernel__n_components=100, model__C=10.0;, score=(train=0.462, test=0.455) total time=  40.4s\n",
      "[CV 2/3; 6/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=1.0..\n",
      "[CV 2/3; 5/16] END kernel__gamma=0.1, kernel__n_components=100, model__C=10.0;, score=(train=0.462, test=0.450) total time=  41.5s\n",
      "[CV 3/3; 6/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=1.0..\n",
      "[CV 1/3; 6/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=1.0;, score=(train=0.489, test=0.463) total time=   8.6s\n",
      "[CV 1/3; 7/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.01.\n",
      "[CV 1/3; 7/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.01;, score=(train=0.481, test=0.474) total time=   5.1s\n",
      "[CV 2/3; 7/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.01.\n",
      "[CV 2/3; 6/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=1.0;, score=(train=0.483, test=0.467) total time=   9.2s\n",
      "[CV 3/3; 6/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=1.0;, score=(train=0.491, test=0.476) total time=   9.1s\n",
      "[CV 3/3; 7/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.01.\n",
      "[CV 1/3; 8/16] START kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0.\n",
      "[CV 2/3; 7/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.01;, score=(train=0.483, test=0.476) total time=   4.0s\n",
      "[CV 2/3; 8/16] START kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0.\n",
      "[CV 3/3; 7/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.01;, score=(train=0.481, test=0.472) total time=   4.0s\n",
      "[CV 3/3; 8/16] START kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0.\n",
      "[CV 2/3; 8/16] END kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0;, score=(train=0.532, test=0.463) total time=  37.6s\n",
      "[CV 1/3; 8/16] END kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0;, score=(train=0.532, test=0.477) total time=  38.5s\n",
      "[CV 1/3; 9/16] START kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01\n",
      "[CV 2/3; 9/16] START kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01\n",
      "[CV 3/3; 8/16] END kernel__gamma=0.1, kernel__n_components=1000, model__C=1.0;, score=(train=0.533, test=0.474) total time=  40.8s\n",
      "[CV 3/3; 9/16] START kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01\n",
      "[CV 3/3; 3/16] END kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0;, score=(train=0.529, test=0.474) total time= 2.7min\n",
      "[CV 1/3; 3/16] END kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0;, score=(train=0.531, test=0.467) total time= 2.8min\n",
      "[CV 1/3; 10/16] START kernel__gamma=0.01, kernel__n_components=250, model__C=0.1\n",
      "[CV 2/3; 10/16] START kernel__gamma=0.01, kernel__n_components=250, model__C=0.1\n",
      "[CV 2/3; 10/16] END kernel__gamma=0.01, kernel__n_components=250, model__C=0.1;, score=(train=0.477, test=0.469) total time=   6.2s\n",
      "[CV 3/3; 10/16] START kernel__gamma=0.01, kernel__n_components=250, model__C=0.1\n",
      "[CV 1/3; 10/16] END kernel__gamma=0.01, kernel__n_components=250, model__C=0.1;, score=(train=0.488, test=0.481) total time=   6.8s\n",
      "[CV 1/3; 11/16] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1\n",
      "[CV 3/3; 10/16] END kernel__gamma=0.01, kernel__n_components=250, model__C=0.1;, score=(train=0.479, test=0.465) total time=   5.3s\n",
      "[CV 2/3; 11/16] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1\n",
      "[CV 2/3; 3/16] END kernel__gamma=0.01, kernel__n_components=1000, model__C=10.0;, score=(train=0.522, test=0.468) total time= 3.1min\n",
      "[CV 3/3; 11/16] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1\n",
      "[CV 2/3; 9/16] END kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01;, score=(train=0.503, test=0.468) total time= 2.2min\n",
      "[CV 3/3; 9/16] END kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01;, score=(train=0.506, test=0.470) total time= 2.0min\n",
      "[CV 1/3; 9/16] END kernel__gamma=1.0, kernel__n_components=1000, model__C=0.01;, score=(train=0.502, test=0.475) total time= 2.2min\n",
      "[CV 1/3; 12/16] START kernel__gamma=0.01, kernel__n_components=250, model__C=1.0\n",
      "[CV 2/3; 12/16] START kernel__gamma=0.01, kernel__n_components=250, model__C=1.0\n",
      "[CV 3/3; 12/16] START kernel__gamma=0.01, kernel__n_components=250, model__C=1.0\n",
      "[CV 1/3; 12/16] END kernel__gamma=0.01, kernel__n_components=250, model__C=1.0;, score=(train=0.485, test=0.480) total time=  14.1s\n",
      "[CV 2/3; 12/16] END kernel__gamma=0.01, kernel__n_components=250, model__C=1.0;, score=(train=0.481, test=0.464) total time=  14.0s\n",
      "[CV 1/3; 13/16] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01\n",
      "[CV 3/3; 12/16] END kernel__gamma=0.01, kernel__n_components=250, model__C=1.0;, score=(train=0.486, test=0.472) total time=  14.4s\n",
      "[CV 2/3; 13/16] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01\n",
      "[CV 3/3; 13/16] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01\n",
      "[CV 1/3; 11/16] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1;, score=(train=0.497, test=0.487) total time= 2.4min\n",
      "[CV 1/3; 14/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.1.\n",
      "[CV 2/3; 11/16] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1;, score=(train=0.493, test=0.482) total time= 2.4min\n",
      "[CV 2/3; 14/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.1.\n",
      "[CV 3/3; 11/16] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1;, score=(train=0.495, test=0.485) total time= 2.4min\n",
      "[CV 3/3; 14/16] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.1.\n",
      "[CV 2/3; 14/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.1;, score=(train=0.486, test=0.468) total time=  12.7s\n",
      "[CV 1/3; 15/16] START kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0\n",
      "[CV 1/3; 14/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.1;, score=(train=0.493, test=0.471) total time=  18.9s\n",
      "[CV 2/3; 15/16] START kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0\n",
      "[CV 3/3; 14/16] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.1;, score=(train=0.491, test=0.469) total time=  11.2s\n",
      "[CV 3/3; 15/16] START kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0\n",
      "[CV 3/3; 13/16] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01;, score=(train=0.496, test=0.487) total time= 2.5min\n",
      "[CV 2/3; 13/16] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01;, score=(train=0.490, test=0.480) total time= 2.5min\n",
      "[CV 1/3; 13/16] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01;, score=(train=0.496, test=0.488) total time= 2.5min\n",
      "[CV 1/3; 16/16] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1\n",
      "[CV 2/3; 16/16] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1\n",
      "[CV 3/3; 16/16] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1\n",
      "[CV 2/3; 15/16] END kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0;, score=(train=0.513, test=0.474) total time= 2.7min\n",
      "[CV 3/3; 15/16] END kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0;, score=(train=0.520, test=0.489) total time= 2.7min\n",
      "[CV 1/3; 15/16] END kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0;, score=(train=0.523, test=0.482) total time= 2.7min\n",
      "[CV 2/3; 16/16] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1;, score=(train=0.515, test=0.485) total time= 2.0min\n",
      "[CV 1/3; 16/16] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1;, score=(train=0.525, test=0.490) total time= 2.0min\n",
      "[CV 3/3; 16/16] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1;, score=(train=0.517, test=0.484) total time= 2.0min\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 6\n",
      "n_resources: 112680\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV 1/3; 1/6] START kernel__gamma=0.01, kernel__n_components=250, model__C=1.0..\n",
      "[CV 2/3; 1/6] START kernel__gamma=0.01, kernel__n_components=250, model__C=1.0..\n",
      "[CV 3/3; 1/6] START kernel__gamma=0.01, kernel__n_components=250, model__C=1.0..\n",
      "[CV 1/3; 2/6] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.01..\n",
      "[CV 2/3; 2/6] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.01..\n",
      "[CV 3/3; 2/6] START kernel__gamma=0.1, kernel__n_components=250, model__C=0.01..\n",
      "[CV 1/3; 2/6] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.01;, score=(train=0.482, test=0.474) total time=   9.5s\n",
      "[CV 1/3; 3/6] START kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0.\n",
      "[CV 2/3; 2/6] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.01;, score=(train=0.484, test=0.478) total time=  10.0s\n",
      "[CV 3/3; 2/6] END kernel__gamma=0.1, kernel__n_components=250, model__C=0.01;, score=(train=0.479, test=0.477) total time=   9.9s\n",
      "[CV 2/3; 3/6] START kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0.\n",
      "[CV 3/3; 3/6] START kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0.\n",
      "[CV 3/3; 1/6] END kernel__gamma=0.01, kernel__n_components=250, model__C=1.0;, score=(train=0.484, test=0.480) total time=  31.2s\n",
      "[CV 2/3; 1/6] END kernel__gamma=0.01, kernel__n_components=250, model__C=1.0;, score=(train=0.487, test=0.480) total time=  31.2s\n",
      "[CV 1/3; 1/6] END kernel__gamma=0.01, kernel__n_components=250, model__C=1.0;, score=(train=0.488, test=0.475) total time=  31.3s\n",
      "[CV 1/3; 4/6] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1.\n",
      "[CV 2/3; 4/6] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1.\n",
      "[CV 3/3; 4/6] START kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1.\n",
      "[CV 1/3; 3/6] END kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0;, score=(train=0.507, test=0.487) total time= 4.9min\n",
      "[CV 1/3; 5/6] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01.\n",
      "[CV 2/3; 3/6] END kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0;, score=(train=0.507, test=0.490) total time= 5.4min\n",
      "[CV 2/3; 5/6] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01.\n",
      "[CV 3/3; 3/6] END kernel__gamma=0.01, kernel__n_components=1000, model__C=1.0;, score=(train=0.507, test=0.487) total time= 5.9min\n",
      "[CV 3/3; 5/6] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01.\n",
      "[CV 3/3; 4/6] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1;, score=(train=0.497, test=0.491) total time= 6.4min\n",
      "[CV 2/3; 4/6] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1;, score=(train=0.499, test=0.491) total time= 6.4min\n",
      "[CV 1/3; 4/6] END kernel__gamma=0.01, kernel__n_components=1000, model__C=0.1;, score=(train=0.497, test=0.490) total time= 6.6min\n",
      "[CV 1/3; 6/6] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1..\n",
      "[CV 2/3; 6/6] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1..\n",
      "[CV 3/3; 6/6] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1..\n",
      "[CV 1/3; 5/6] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01;, score=(train=0.498, test=0.492) total time= 6.7min\n",
      "[CV 2/3; 5/6] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01;, score=(train=0.499, test=0.496) total time= 6.6min\n",
      "[CV 3/3; 5/6] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01;, score=(train=0.499, test=0.491) total time= 6.4min\n",
      "[CV 1/3; 6/6] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1;, score=(train=0.511, test=0.491) total time= 6.4min\n",
      "[CV 3/3; 6/6] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1;, score=(train=0.509, test=0.491) total time= 6.3min\n",
      "[CV 2/3; 6/6] END kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1;, score=(train=0.510, test=0.491) total time= 6.3min\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 2\n",
      "n_resources: 338040\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[CV 2/3; 1/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1..\n",
      "[CV 1/3; 1/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1..\n",
      "[CV 3/3; 1/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1..\n",
      "[CV 1/3; 2/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01.\n",
      "[CV 2/3; 2/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01.\n",
      "[CV 3/3; 2/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01.\n"
     ]
    }
   ],
   "source": [
    "kernel_search.fit(train_df_transformed, train_df[\"citation_bucket\"])\n",
    "save_model(kernel_search, \"models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```log\n",
    "----------\n",
    "iter: 3\n",
    "n_candidates: 2\n",
    "n_resources: 338040\n",
    "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
    "[CV 2/3; 1/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1..\n",
    "[CV 1/3; 1/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1..\n",
    "[CV 3/3; 1/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.1..\n",
    "[CV 1/3; 2/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01.\n",
    "[CV 2/3; 2/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01.\n",
    "[CV 3/3; 2/2] START kernel__gamma=0.1, kernel__n_components=1000, model__C=0.01.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about SGD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Size of train set: 304246'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Size of validation set: 33806'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;kernel&#x27;,\n",
       "                 RBFSampler(gamma=0.1, n_components=1000, random_state=42)),\n",
       "                (&#x27;model&#x27;, SGDClassifier(n_jobs=-1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-99\" type=\"checkbox\" ><label for=\"sk-estimator-id-99\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;ratio&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                                   &#x27;author_count&#x27;]),\n",
       "                                                 (&#x27;date&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                                 (&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;kernel&#x27;,\n",
       "                 RBFSampler(gamma=0.1, n_components=1000, random_state=42)),\n",
       "                (&#x27;model&#x27;, SGDClassifier(n_jobs=-1))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-100\" type=\"checkbox\" ><label for=\"sk-estimator-id-100\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(n_jobs=-1,\n",
       "                  transformers=[(&#x27;ratio&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;page_count&#x27;, &#x27;figure_count&#x27;,\n",
       "                                  &#x27;author_count&#x27;]),\n",
       "                                (&#x27;date&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]),\n",
       "                                (&#x27;text&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                  TfidfVectorizer())]),\n",
       "                                 &#x27;text&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-101\" type=\"checkbox\" ><label for=\"sk-estimator-id-101\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ratio</label><div class=\"sk-toggleable__content\"><pre>[&#x27;page_count&#x27;, &#x27;figure_count&#x27;, &#x27;author_count&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-102\" type=\"checkbox\" ><label for=\"sk-estimator-id-102\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-103\" type=\"checkbox\" ><label for=\"sk-estimator-id-103\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">date</label><div class=\"sk-toggleable__content\"><pre>[&#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-104\" type=\"checkbox\" ><label for=\"sk-estimator-id-104\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-105\" type=\"checkbox\" ><label for=\"sk-estimator-id-105\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">text</label><div class=\"sk-toggleable__content\"><pre>text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-106\" type=\"checkbox\" ><label for=\"sk-estimator-id-106\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-107\" type=\"checkbox\" ><label for=\"sk-estimator-id-107\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RBFSampler</label><div class=\"sk-toggleable__content\"><pre>RBFSampler(gamma=0.1, n_components=1000, random_state=42)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-108\" type=\"checkbox\" ><label for=\"sk-estimator-id-108\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(n_jobs=-1)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[('ratio',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['page_count', 'figure_count',\n",
       "                                                   'author_count']),\n",
       "                                                 ('date',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['year', 'month', 'day']),\n",
       "                                                 ('text',\n",
       "                                                  Pipeline(steps=[('vectorizer',\n",
       "                                                                   TfidfVectorizer())]),\n",
       "                                                  'text')])),\n",
       "                ('kernel',\n",
       "                 RBFSampler(gamma=0.1, n_components=1000, random_state=42)),\n",
       "                ('model', SGDClassifier(n_jobs=-1))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_pipeline = deepcopy(pipeline)\n",
    "sgd_pipeline.steps.insert(1, (\"kernel\", RBFSampler(gamma=0.1, random_state=42, n_components=1000)))\n",
    "sgd_pipeline.steps[-1] = (\"model\", SGDClassifier(loss=\"hinge\", n_jobs=-1))\n",
    "train_df_trunc, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"citation_bucket\"],\n",
    ")\n",
    "display(f\"Size of train set: {len(train_df_trunc)}\")\n",
    "display(f\"Size of validation set: {len(val_df)}\")\n",
    "sgd_pipeline.fit(train_df_trunc, train_df_trunc[\"citation_bucket\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4475753173418878"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.4454830503460924"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sgd_pipeline.score(train_df_trunc, train_df_trunc[\"citation_bucket\"]))\n",
    "display(sgd_pipeline.score(val_df, val_df[\"citation_bucket\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about Dim Reduction before applying the Kernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transf_ds_std_vec\n",
      "Number of features: 389873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sample from the vocabulary: ['052507', '1gb1', 'maskrcnn', '10uk', '00186', 'zhifeng', 'removal', 'avagyan', 'sampurn', 'cures', '03450', 'apbc', 'corbeil', '07752', '915317', 'ruggero', 'homage', '125031', 'oauj', 'nrhc', 'centerpoint', 'bvr_ci_c', 'hasten', 'nanofuels', 'minorization']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transf_ds_max_df_vec\n",
      "Number of features: 389858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sample from the vocabulary: ['biophotovoltaics', 'toxicants', 'rokushima', 'bhattacharyay', 'bijari', 'm_dla', 'quantumenabled', 'garrod', 'sics', 'loginov', 'bertens', 'eufemio', 'cider', 'eyvaz', '04011', 'qlsca', 'sukhman', 'srikantha', '120008', 'hyperkagome', 'clua', 'cabss', 'emrl', 'testament', 'hxt']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transf_ds_lemma_vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['could', 'doe', 'ha', 'might', 'must', 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 875692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sample from the vocabulary: [\\'j.phys.conf.ser.150:042191,2009\\', \\'2442\\', \\'voulis\\', \\'ms2137.3\\', \\'aiai\\', \\'rebbapragada\\', \\'randomization\\', \\'coadsorption\\', \\'210.\\', \\'e_g/\\\\\\\\hbar\\', \\'phys.17:855-863,2008\\', \\'teff-color\\', \"\\'fukaya\", \\'13541.\\', \\'spoelstra\\', \\'j1653.6-0158\\', \\'g.c.anupama\\', \\'arxiv:1009.5553\\', \\'4400a\\', \\'la0.7ca0.3mn16o3\\', \\'\\\\\\\\say\\', \\'pptp\\', \\'085014\\', \\'male-bias\\', \\'nemykin\\']'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transf_ds_stem_vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit und Beruf/Uppsala/Data Mining/uu-data-mining-project/exploration_modeling.ipynb Cell 41\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X61sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtransf_ds_stem_vec\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X61sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m transf_ds_stem_vec \u001b[39m=\u001b[39m TfidfVectorizer(tokenizer\u001b[39m=\u001b[39mStemmingTokenizer(), stop_words\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(stop_words), strip_accents\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39municode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X61sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m transf_ds_stem \u001b[39m=\u001b[39m transf_ds_stem_vec\u001b[39m.\u001b[39mfit_transform(train_df[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X61sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of features: \u001b[39m\u001b[39m{\u001b[39;00mtransf_ds_stem\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X61sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m display(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSample from the vocabulary: \u001b[39m\u001b[39m{\u001b[39;00mrandom\u001b[39m.\u001b[39msample(\u001b[39mlist\u001b[39m(transf_ds_stem_vec\u001b[39m.\u001b[39mvocabulary_\u001b[39m.\u001b[39mkeys()),\u001b[39m \u001b[39m\u001b[39m25\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2133\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[1;32m   2127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2128\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[1;32m   2129\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[1;32m   2130\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[1;32m   2131\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[1;32m   2132\u001b[0m )\n\u001b[0;32m-> 2133\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfit_transform(raw_documents)\n\u001b[1;32m   2134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[1;32m   2135\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2136\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m             )\n\u001b[1;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_count_vocab(raw_documents, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfixed_vocabulary_)\n\u001b[1;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1274\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1275\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1276\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    111\u001b[0m     doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     doc \u001b[39m=\u001b[39m tokenizer(doc)\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m ngrams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit und Beruf/Uppsala/Data Mining/uu-data-mining-project/exploration_modeling.ipynb Cell 41\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X61sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, doc):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X61sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X61sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwnl\u001b[39m.\u001b[39mstem(t)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X61sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m         \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m nltk\u001b[39m.\u001b[39mword_tokenize(doc)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X61sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m         \u001b[39mif\u001b[39;00m t \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_tokens\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tillgrutschus/Library/CloudStorage/OneDrive-Personal/Documents/Arbeit%20und%20Beruf/Uppsala/Data%20Mining/uu-data-mining-project/exploration_modeling.ipynb#X61sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/tokenize/__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtokenizers/punkt/\u001b[39m\u001b[39m{\u001b[39;00mlanguage\u001b[39m}\u001b[39;00m\u001b[39m.pickle\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   1278\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[39m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1281\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\n\u001b[1;32m   1333\u001b[0m     \u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   1335\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1341\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\n\u001b[1;32m   1333\u001b[0m     \u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   1335\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1341\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[39mif\u001b[39;00m realign_boundaries:\n\u001b[1;32m   1328\u001b[0m     slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[0;32m-> 1329\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m slices:\n\u001b[1;32m   1330\u001b[0m     \u001b[39myield\u001b[39;00m (sentence\u001b[39m.\u001b[39mstart, sentence\u001b[39m.\u001b[39mstop)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \u001b[39mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m \u001b[39mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[39m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m realign \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1459\u001b[0m \u001b[39mfor\u001b[39;00m sentence1, sentence2 \u001b[39min\u001b[39;00m _pair_iter(slices):\n\u001b[1;32m   1460\u001b[0m     sentence1 \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(sentence1\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m realign, sentence1\u001b[39m.\u001b[39mstop)\n\u001b[1;32m   1461\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sentence2:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/tokenize/punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    319\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(iterator)\n\u001b[1;32m    320\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     prev \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[1;32m    322\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/uu-data-mining/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1433\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[39mfor\u001b[39;00m match, context \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[1;32m   1432\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m-> 1433\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mslice\u001b[39m(last_break, match\u001b[39m.\u001b[39mend())\n\u001b[1;32m   1434\u001b[0m         \u001b[39mif\u001b[39;00m match\u001b[39m.\u001b[39mgroup(\u001b[39m\"\u001b[39m\u001b[39mnext_tok\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1435\u001b[0m             \u001b[39m# next sentence starts after whitespace\u001b[39;00m\n\u001b[1;32m   1436\u001b[0m             last_break \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mstart(\u001b[39m\"\u001b[39m\u001b[39mnext_tok\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's compare dim reduction models\n",
    "print(\"transf_ds_std_vec\")\n",
    "transf_ds_std_vec = TfidfVectorizer(strip_accents=\"unicode\")\n",
    "transf_ds_std = transf_ds_std_vec.fit_transform(train_df[\"text\"])\n",
    "print(f\"Number of features: {transf_ds_std.shape[1]}\")\n",
    "display(f\"Sample from the vocabulary: {random.sample(list(transf_ds_std_vec.vocabulary_.keys()), 25)}\")\n",
    "\n",
    "print(\"transf_ds_max_df_vec\")\n",
    "transf_ds_max_df_vec = TfidfVectorizer(max_df=0.5, strip_accents=\"unicode\")\n",
    "transf_ds_max_df = transf_ds_max_df_vec.fit_transform(train_df[\"text\"])\n",
    "print(f\"Number of features: {transf_ds_max_df.shape[1]}\")\n",
    "display(f\"Sample from the vocabulary: {random.sample(list(transf_ds_max_df_vec.vocabulary_.keys()), 25)}\")\n",
    "\n",
    "print(\"transf_ds_lemma_vec\")\n",
    "transf_ds_lemma_vec = TfidfVectorizer(tokenizer=LemmaTokenizer(), strip_accents=\"unicode\", stop_words=list(stop_words))\n",
    "transf_ds_lemma = transf_ds_lemma_vec.fit_transform(train_df[\"text\"])\n",
    "print(f\"Number of features: {transf_ds_lemma.shape[1]}\")\n",
    "display(f\"Sample from the vocabulary: {random.sample(list(transf_ds_lemma_vec.vocabulary_.keys()), 25)}\")\n",
    "\n",
    "print(\"transf_ds_stem_vec\")\n",
    "transf_ds_stem_vec = TfidfVectorizer(tokenizer=StemmingTokenizer(), stop_words=list(stop_words), strip_accents=\"unicode\")\n",
    "transf_ds_stem = transf_ds_stem_vec.fit_transform(train_df[\"text\"])\n",
    "print(f\"Number of features: {transf_ds_stem.shape[1]}\")\n",
    "display(f\"Sample from the vocabulary: {random.sample(list(transf_ds_stem_vec.vocabulary_.keys()), 25)}\")\n",
    "\n",
    "print(\"transf_ds_max_feat_vec\")\n",
    "transf_ds_max_feat_vec = TfidfVectorizer(max_features=10000, strip_accents=\"unicode\")\n",
    "transf_ds_max_feat = transf_ds_max_feat_vec.fit_transform(train_df[\"text\"])\n",
    "print(f\"Number of features: {transf_ds_max_feat.shape[1]}\")\n",
    "display(f\"Sample from the vocabulary: {random.sample(list(transf_ds_max_feat_vec.vocabulary_.keys()), 25)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_red_text_pipeline = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(max_df=))\n",
    "])\n",
    "\n",
    "dim_red_pipeline = Pipeline([\n",
    "\n",
    "])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uu-data-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
